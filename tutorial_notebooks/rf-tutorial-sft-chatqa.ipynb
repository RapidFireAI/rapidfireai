{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RapidFire AI Tutorial Use Case: SFT for Customer Support Q&A Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-16 08:01:31 [__init__.py:241] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Specify Train and Eval Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "\n",
    "# Select a subset of the dataset for demo purposes\n",
    "train_dataset=dataset[\"train\"].select(range(5000))\n",
    "eval_dataset=dataset[\"train\"].select(range(5000,5200))\n",
    "train_dataset=train_dataset.shuffle(seed=42)\n",
    "eval_dataset=eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_formatting_function(row):\n",
    "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
    "    # Special tokens for formatting\n",
    "    SYSTEM_PROMPT = \"You are a helpful and friendly customer support assistant. Please answer the user's query to the best of your ability.\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "            \n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/16 08:01:33 INFO mlflow.tracking.fluent: Experiment with name 'exp1-chatqa_7' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An experiment with the same name already exists. Created a new experiment with name 'exp1-chatqa_7' with Experiment ID: 12 and MLFlow Experiment ID: 12 saved at /home/palebluedot/test/rapidfireai/tutorial_notebooks/rapidfire_experiments/exp1-chatqa_7\n"
     ]
    }
   ],
   "source": [
    "# Every experiment instance must be uniquely named\n",
    "experiment = Experiment(experiment_name=\"exp1-chatqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Eval Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_compute_metrics(eval_preds):  \n",
    "    \"\"\"Optional function to compute eval metrics based on predictions and labels\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Standard text-based eval metrics: Rouge and BLEU\n",
    "    import evaluate\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
    "    rouge_l = rouge_output[\"rougeL\"]\n",
    "    bleu_output = bleu.compute(predictions=predictions, references=labels)\n",
    "    bleu_score = bleu_output[\"bleu\"]\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_l, 4),\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multi-Config Knobs for Model, LoRA, and SFT Trainer using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 LoRA PEFT configs with different adapter capacities\n",
    "peft_configs = List([\n",
    "    RFLoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    RFLoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2 base models x 2 peft configs = 4 combinations in total\n",
    "config_set = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"rapidfire-ai-inc/Llama-3.1-8B-bnb-4bit\",\n",
    "        peft_config=peft_configs,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            fp16=True,\n",
    "            save_strategy=\"epoch\"\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\"load_in_4bit\": True, \"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        formatting_func = sample_formatting_function,\n",
    "        compute_metrics = sample_compute_metrics,\n",
    "        generation_config = { # This is for text based evaluation/prediction for causal_lm models\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.18,\n",
    "        }\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"rapidfire-ai-inc/Mistral-7B-Instruct-v0.3-bnb-4bit\",\n",
    "        peft_config=peft_configs,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            fp16=True,\n",
    "            save_strategy=\"epoch\"\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\"load_in_4bit\": True, \"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        formatting_func = sample_formatting_function,\n",
    "        compute_metrics = sample_compute_metrics,\n",
    "        generation_config = { # This is for text based evaluation/prediction for causal_lm models\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.18,\n",
    "        }\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Creation Function for All Model Types Across Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_create_model(model_config): \n",
    "     \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
    "     from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM\n",
    "\n",
    "     model_name = model_config[\"model_name\"]\n",
    "     model_type = model_config[\"model_type\"]\n",
    "     model_kwargs = model_config[\"model_kwargs\"]\n",
    " \n",
    "     if model_type == \"causal_lm\":\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"seq2seq_lm\":\n",
    "          model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"masked_lm\":\n",
    "          model = AutoModelForMaskedLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"custom\":\n",
    "          # Handle custom model loading logic, e.g., loading your own checkpoints\n",
    "          # model = ... \n",
    "          pass\n",
    "     else:\n",
    "          # Default to causal LM\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "      \n",
    "     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      \n",
    "     return (model,tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-Config Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 4 worker processes successfully\n",
      "Created workers\n",
      "INFO 09-16 08:01:39 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-16 08:01:39 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-16 08:01:39 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-16 08:01:39 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-16 08:01:49 [__init__.py:241] Automatically detected platform cuda.\n",
      "Run 3 has failed: element 0 of tensors does not require grad and does not have a grad_fnTraceback (most recent call last):\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 240, in serve_forever\n",
      "    self.run_fit(run_id, chunk_id, create_model_fn)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 159, in run_fit\n",
      "    trainer_instance.train()\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2328, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py\", line 904, in training_step\n",
      "    return super().training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 4060, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2730, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "Run 4 has failed: element 0 of tensors does not require grad and does not have a grad_fnTraceback (most recent call last):\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 240, in serve_forever\n",
      "    self.run_fit(run_id, chunk_id, create_model_fn)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 159, in run_fit\n",
      "    trainer_instance.train()\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2328, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py\", line 904, in training_step\n",
      "    return super().training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 4060, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2730, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "Run 1 has failed: element 0 of tensors does not require grad and does not have a grad_fnTraceback (most recent call last):\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 240, in serve_forever\n",
      "    self.run_fit(run_id, chunk_id, create_model_fn)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 159, in run_fit\n",
      "    trainer_instance.train()\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2328, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py\", line 904, in training_step\n",
      "    return super().training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 4060, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2730, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "Run 2 has failed: element 0 of tensors does not require grad and does not have a grad_fnTraceback (most recent call last):\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 240, in serve_forever\n",
      "    self.run_fit(run_id, chunk_id, create_model_fn)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/rapidfireai/backend/worker.py\", line 159, in run_fit\n",
      "    trainer_instance.train()\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2328, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py\", line 904, in training_step\n",
      "    return super().training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/transformers/trainer.py\", line 4060, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2730, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/palebluedot/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
    "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=4, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Current Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active MLflow run to clear\n",
      "Experiment exp1-chatqa_7 ended\n",
      "Workers stopped\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
