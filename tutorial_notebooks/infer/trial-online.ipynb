{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from rapidfireai.infer.experiment import Experiment\n",
    "from rapidfireai.infer.rag.context_generator import ContextGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bbf90",
   "metadata": {},
   "source": [
    "### Model config and Sampling Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623742da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai.infer.utils.config import VLLMModelConfig\n",
    "\n",
    "pipeline = VLLMModelConfig(\n",
    "    model_config = {\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.9,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 2048,\n",
    "        \"disable_log_stats\": True,  # Disable VLLM progress logging\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    "    context_generator=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fa27a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa87f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test split for evaluation (not train)\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "print(f\"Loaded {len(dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding id column to the dataset for testing online aggregation\n",
    "dataset = dataset.add_column(\"id\", list(range(1, len(dataset) + 1)))\n",
    "dataset = dataset.shuffle(seed=1337) # Shuffling the dataset to ensure randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Utility, Preprocessor, Postprocessor, Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(answer):\n",
    "    solution = re.search(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", answer)\n",
    "    if solution is None:\n",
    "        return \"0\"\n",
    "    final_solution = solution.group(0)\n",
    "    final_solution = final_solution.split(\"#### \")[1].replace(\",\", \"\")\n",
    "    return final_solution\n",
    "\n",
    "def preprocess_fn(batch: Dict[str, List], context_generator: ContextGenerator) -> Dict[str, List]:\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": 'Let\\'s think step by step and output the final answer after \"####\".'},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            for question in batch[\"question\"]\n",
    "        ],\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "def postprocess_fn(batch: Dict[str, List]) -> Dict[str, List]:\n",
    "    batch[\"model_answer\"] = [extract_solution(answer) for answer in batch[\"generated_text\"]]\n",
    "    batch[\"ground_truth\"] = [extract_solution(answer) for answer in batch[\"answer\"]]\n",
    "    return batch\n",
    "\n",
    "def compute_metrics_fn(batch: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    correct = sum(1 for pred, gt in zip(batch[\"model_answer\"], batch[\"ground_truth\"])\n",
    "                  if pred == gt)\n",
    "    total = len(batch[\"model_answer\"])\n",
    "    sum_n = sum(id for id in batch[\"id\"])\n",
    "    return {\n",
    "        \"Correct\": {\"value\": correct},\n",
    "        \"Total\": {\"value\": total},\n",
    "        \"SumN\": {\"value\": sum_n}\n",
    "    }\n",
    "\n",
    "def accumulate_metrics_fn(aggregated_metrics: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    # aggregated_metrics is a dict of lists: {\"Correct\": [5, 3, 7], \"Total\": [10, 8, 12]}\n",
    "    correct = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Correct\", [{}]))\n",
    "    total = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Total\", [{}]))\n",
    "    sum_n = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"SumN\", [{}]))\n",
    "    avg_n = float(sum_n) / total if total > 0 else 0\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return {\n",
    "        \"Total\": {\"value\": total},\n",
    "        \"Correct\": {\"value\": correct, \"is_distributive\": True, \"value_range\": (0, 1)}, # 0 (min) if not correct, 1 if correct (max)\n",
    "        \"SumN\": {\"value\": sum_n, \"is_distributive\": True, \"value_range\": (0, 7472)}, # each sample can a have value ranging from 0 to 7472\n",
    "        \"AvgN\": {\"value\": avg_n, \"is_algebraic\": True, \"value_range\": (0, 7472)},\n",
    "        \"Accuracy\": {\"value\": accuracy, \"is_algebraic\": True, \"value_range\": (0, 1)} # Algebraic metric for online aggregation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4699f9",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(experiment_name=\"trial-online\", num_actors=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results, metrics = experiment.run_evals(\n",
    "    pipeline,\n",
    "    dataset,\n",
    "    batch_size=128,  # Per actor batch size\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    postprocess_fn=postprocess_fn,\n",
    "    compute_metrics_fn=compute_metrics_fn,\n",
    "    accumulate_metrics_fn=accumulate_metrics_fn,\n",
    "    online_strategy_kwargs={\"strategy_name\": \"normal\", \"confidence_level\": 0.95, \"use_fpc\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f686476",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results, metrics = experiment.run_evals(\n",
    "    pipeline,\n",
    "    dataset,\n",
    "    batch_size=128,  # Per actor batch size\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    postprocess_fn=postprocess_fn,\n",
    "    compute_metrics_fn=compute_metrics_fn,\n",
    "    accumulate_metrics_fn=accumulate_metrics_fn,\n",
    "    online_strategy_kwargs={\"strategy_name\": \"wilson\", \"confidence_level\": 0.95, \"use_fpc\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results, metrics = experiment.run_evals(\n",
    "    pipeline,\n",
    "    dataset,\n",
    "    batch_size=128,  # Per actor batch size\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    postprocess_fn=postprocess_fn,\n",
    "    compute_metrics_fn=compute_metrics_fn,\n",
    "    accumulate_metrics_fn=accumulate_metrics_fn,\n",
    "    online_strategy_kwargs={\"strategy_name\": \"hoeffding\", \"confidence_level\": 0.95, \"use_fpc\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756344e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nResults:\")\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFirst few examples:\")\n",
    "for i in range(min(3, metrics['Samples Processed']['value'])):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {aggregated_results['question'][i]}\")\n",
    "    print(f\"Ground truth: {aggregated_results['ground_truth'][i]}\")\n",
    "    print(f\"Model answer: {aggregated_results['model_answer'][i]}\")\n",
    "    print(f\"Generated text: {aggregated_results['generated_text'][i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
