{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-08 23:12:35 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.5.1+cu124 available.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from rapidfireai.infer.experiment import Experiment\n",
    "from rapidfireai.infer.rag.context_generator import ContextGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bbf90",
   "metadata": {},
   "source": [
    "### Model config and Sampling Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623742da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai.infer.utils.config import VLLMModelConfig\n",
    "\n",
    "pipeline = VLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 2048,\n",
    "        \"disable_log_stats\": True,  # Disable VLLM progress logging\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    "    context_generator=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fa27a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa87f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7473 test samples\n"
     ]
    }
   ],
   "source": [
    "# Use test split for evaluation (not train)\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "print(f\"Loaded {len(dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Utility, Preprocessor, Postprocessor, Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(answer):\n",
    "    solution = re.search(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", answer)\n",
    "    if solution is None:\n",
    "        return \"0\"\n",
    "    final_solution = solution.group(0)\n",
    "    final_solution = final_solution.split(\"#### \")[1].replace(\",\", \"\")\n",
    "    return final_solution\n",
    "\n",
    "def preprocess_fn(batch: Dict[str, List], context_generator: ContextGenerator) -> Dict[str, List]:\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": 'Let\\'s think step by step and output the final answer after \"####\".'},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            for question in batch[\"question\"]\n",
    "        ],\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "def postprocess_fn(batch: Dict[str, List]) -> Dict[str, List]:\n",
    "    batch[\"model_answer\"] = [extract_solution(answer) for answer in batch[\"generated_text\"]]\n",
    "    batch[\"ground_truth\"] = [extract_solution(answer) for answer in batch[\"answer\"]]\n",
    "    return batch\n",
    "\n",
    "def compute_metrics_fn(batch: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    correct = sum(1 for pred, gt in zip(batch[\"model_answer\"], batch[\"ground_truth\"])\n",
    "                  if pred == gt)\n",
    "    total = len(batch[\"model_answer\"])\n",
    "    return {\n",
    "        \"Correct\": {\"value\": correct},\n",
    "        \"Total\": {\"value\": total},\n",
    "    }\n",
    "\n",
    "def accumulate_metrics_fn(aggregated_metrics: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    # aggregated_metrics is a dict of lists: {\"Correct\": [5, 3, 7], \"Total\": [10, 8, 12]}\n",
    "    correct = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Correct\", [{}]))\n",
    "    total = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Total\", [{}]))\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return {\n",
    "        \"Total\": {\"value\": total},\n",
    "        \"Correct\": {\"value\": correct, \"is_distributive\": True, \"value_range\": (0, 1)}, # 0 (min) if not correct, 1 if correct (max)\n",
    "        \"Accuracy\": {\"value\": accuracy, \"is_algebraic\": True, \"value_range\": (0, 1)} # Algebraic metric for online aggregation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4699f9",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0ec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster resources: {\n",
      "    \"object_store_memory\": 114952562688.0,\n",
      "    \"memory\": 268222646272.0,\n",
      "    \"node:172.31.6.151\": 1.0,\n",
      "    \"node:__internal_head__\": 1.0,\n",
      "    \"GPU\": 8.0,\n",
      "    \"accelerator_type:T4\": 1.0,\n",
      "    \"CPU\": 96.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(experiment_name=\"trial-infer\", num_actors=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 8 model actors with 1 GPU and 8 CPUs each\n",
      "Split 7473 samples into 59 batches of size 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completing batches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59/59 [05:28<00:00,  5.57s/batch], Live Metrics: Total=7473, Correct=293.0000\u00b10.0000, Accuracy=0.0392\u00b10.0000  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches completed.\n",
      "Accumulating batch-level metrics offline\n",
      "\n",
      "Shutting down actors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aggregated_results, metrics = experiment.run_evals(\n",
    "    pipeline,\n",
    "    dataset,\n",
    "    batch_size=128,  # Per actor batch size\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    postprocess_fn=postprocess_fn,\n",
    "    compute_metrics_fn=compute_metrics_fn,\n",
    "    accumulate_metrics_fn=accumulate_metrics_fn,\n",
    "    online_strategy_kwargs={\"strategy_name\": \"normal\", \"confidence_level\": 0.95, \"use_fpc\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All actors shut down\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756344e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "{\n",
      "    \"Samples Processed\": {\n",
      "        \"value\": 7473,\n",
      "        \"is_algebraic\": false\n",
      "    },\n",
      "    \"Processing Time\": {\n",
      "        \"value\": \"328.94 seconds\",\n",
      "        \"is_algebraic\": false\n",
      "    },\n",
      "    \"Samples Per Second\": {\n",
      "        \"value\": \"22.72\",\n",
      "        \"is_algebraic\": false\n",
      "    },\n",
      "    \"Total\": {\n",
      "        \"value\": 7473\n",
      "    },\n",
      "    \"Correct\": {\n",
      "        \"value\": 293,\n",
      "        \"is_distributive\": true,\n",
      "        \"value_range\": [\n",
      "            0,\n",
      "            1\n",
      "        ]\n",
      "    },\n",
      "    \"Accuracy\": {\n",
      "        \"value\": 0.03920781479994647,\n",
      "        \"is_algebraic\": true,\n",
      "        \"value_range\": [\n",
      "            0,\n",
      "            1\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nResults:\")\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70272fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few examples:\n",
      "\n",
      "Example 1:\n",
      "Question: Elysse can carry 3 bags of groceries into her home with each trip from the car. Her brother can carry the same amount. How many trips will it take them to carry 30 bags of groceries?\n",
      "Ground truth: 5\n",
      "Model answer: 0\n",
      "Generated text: To find out how many trips Elysse and her brother need to carry 30 bags of groceries, we can follow these steps:\n",
      "\n",
      "1. Determine how many bags they can carry together in one trip:\n",
      "   Elysse can carry 3 bags per trip, and her brother can carry 3 bags per trip.\n",
      "   Together, they can carry \\( 3 + 3 = 6 \\) bags per trip.\n",
      "\n",
      "2. Calculate how many trips they need to carry 30 bags:\n",
      "   Since they can carry 6 bags per trip, we divide the total number of bags by the number of bags they can carry per trip:\n",
      "   \\[\n",
      "   \\frac{30 \\text{ bags}}{6 \\text{ bags per trip}} = 5 \\text{ trips}\n",
      "   \\]\n",
      "\n",
      "Therefore, it will take them 5 trips to carry 30 bags of groceries.\n",
      "\n",
      "Example 2:\n",
      "Question: Jennifer has ten pears, 20 oranges, and twice as many apples as pears. If she gives her sister two of each fruit, how many fruits does she have left?\n",
      "Ground truth: 44\n",
      "Model answer: 0\n",
      "Generated text: First, let's calculate the initial number of each fruit Jennifer has:\n",
      "\n",
      "- Jennifer has 10 pears.\n",
      "- Jennifer has 20 oranges.\n",
      "- She has twice as many apples as pears, so she has 2 * 10 = 20 apples.\n",
      "\n",
      "Next, we calculate the number of fruits she gives to her sister:\n",
      "\n",
      "- Jennifer gives 2 pears to her sister.\n",
      "- Jennifer gives 2 oranges to her sister.\n",
      "- Jennifer gives 2 apples to her sister.\n",
      "\n",
      "Now, let's calculate how many fruits Jennifer has left:\n",
      "\n",
      "- Remaining pears: 10 - 2 = 8 pears.\n",
      "- Remaining oranges: 20 - 2 = 18 oranges.\n",
      "- Remaining apples: 20 - 2 = 18 apples.\n",
      "\n",
      "Finally, we sum up the remaining fruits:\n",
      "\n",
      "8 + 18 + 18 = 44 fruits.\n",
      "\n",
      "So, Jennifer has 44 fruits left.\n",
      "\n",
      "Example 3:\n",
      "Question: Clara brings a package of 100 stickers to school. She gives 10 stickers to a boy she likes. She gives half of the stickers which she has left to her best friends. How many stickers does Clara have left?\n",
      "Ground truth: 45\n",
      "Model answer: 0\n",
      "Generated text: First, we start with the initial number of stickers Clara has, which is 100.\n",
      "\n",
      "Clara gives 10 stickers to a boy she likes. Therefore, the number of stickers left with Clara is:\n",
      "100 - 10 = 90\n",
      "\n",
      "Clara then gives half of the remaining stickers to her best friends. Half of 90 is:\n",
      "90 / 2 = 45\n",
      "\n",
      "So, Clara gives 45 stickers to her best friends and the number of stickers she has left is:\n",
      "90 - 45 = 45\n",
      "\n",
      "Therefore, Clara has 45 stickers left.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFirst few examples:\")\n",
    "for i in range(min(3, metrics['Samples Processed']['value'])):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {aggregated_results['question'][i]}\")\n",
    "    print(f\"Ground truth: {aggregated_results['ground_truth'][i]}\")\n",
    "    print(f\"Model answer: {aggregated_results['model_answer'][i]}\")\n",
    "    print(f\"Generated text: {aggregated_results['generated_text'][i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}