{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-29 08:00:17 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from rapidfireai.infer.experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a605a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee571098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2048 test samples\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\").select(range(512))\n",
    "print(f\"Loaded {len(dataset)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bbf90",
   "metadata": {},
   "source": [
    "### Inference Pipeline Config using `VLLMModelConfig`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecf9d6",
   "metadata": {},
   "source": [
    "##### We will now build a fast inference pipeline using vLLM with a full suite of context engineering capabilities including retrieval augmented generation (RAG), in-context learning with fewshot examples, with evaluation metrics powered by online aggregation.\n",
    "\n",
    "You will be able to experiment with and tune configurations:\n",
    "- Document splitting and chunking\n",
    "- Embeddings\n",
    "- Document and vector stores\n",
    "- Retrieval techniques\n",
    "- Reranking techniques\n",
    "- Prompt engineering and in-context learning\n",
    "- Dynamically selecting fewshot examples\n",
    "- Model generation settings\n",
    "- Post-processing\n",
    "- Evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77c516",
   "metadata": {},
   "source": [
    "##### We will use locally hosted models for both embedding and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a21ee",
   "metadata": {},
   "source": [
    "### RAG Implementation using `LangChainRagSpec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from rapidfireai.infer.rag.rag_pipeline import LangChainRagSpec\n",
    "\n",
    "batch_size = 32  # Reduced from 128 for T4\n",
    "\n",
    "# Shared document loader and text splitter\n",
    "document_loader = DirectoryLoader(\n",
    "    path=\"../data/gsm8k\",\n",
    "    glob=\"*.txt\",\n",
    "    recursive=True,\n",
    "    sample_seed=1337\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"gpt2\", chunk_size=128, chunk_overlap=32\n",
    ")\n",
    "\n",
    "# GPU-based RAG - Configuration 1 (retrieve 2 documents - reduced from 3)\n",
    "rag_gpu_k2 = LangChainRagSpec(\n",
    "    document_loader=document_loader,\n",
    "    text_splitter=text_splitter,\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    retriever=None,\n",
    "    vector_store=None,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},  # Retrieve 2 documents\n",
    "    reranker=None,\n",
    "    enable_gpu_search=True\n",
    ")\n",
    "\n",
    "# GPU-based RAG - Configuration 2 (retrieve 3 documents)\n",
    "rag_gpu_k3 = LangChainRagSpec(\n",
    "    document_loader=document_loader,\n",
    "    text_splitter=text_splitter,\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    retriever=None,\n",
    "    vector_store=None,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},  # Retrieve 3 documents\n",
    "    reranker=None,\n",
    "    enable_gpu_search=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776902e",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Manager using `PromptManager`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from rapidfireai.infer.rag.prompt_manager import PromptManager\n",
    "\n",
    "INSTRUCTIONS = \"You are a helpful assistant that is good at solving math problems. You think step by step and ALWAYS output the final answer after '####'.\"\n",
    "\n",
    "# Keep same examples but reduce k values\n",
    "\n",
    "# Prompt Manager Configuration 1 (1 fewshot example - reduced from 2)\n",
    "fewshot_prompt_manager_k1 = PromptManager(\n",
    "    instructions=INSTRUCTIONS,\n",
    "    examples=examples,  # Same examples as before\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    example_selector_cls=SemanticSimilarityExampleSelector,\n",
    "    example_prompt_template=PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=\"Question: {question}\\nAnswer: {answer}\",\n",
    "    ),\n",
    "    k=1,  # 1 fewshot example\n",
    ")\n",
    "\n",
    "# Prompt Manager Configuration 2 (2 fewshot examples - reduced from 3)\n",
    "fewshot_prompt_manager_k2 = PromptManager(\n",
    "    instructions=INSTRUCTIONS,\n",
    "    examples=examples,\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    example_selector_cls=SemanticSimilarityExampleSelector,\n",
    "    example_prompt_template=PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=\"Question: {question}\\nAnswer: {answer}\",\n",
    "    ),\n",
    "    k=2,  # 2 fewshot examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8050d",
   "metadata": {},
   "source": [
    "### Context engineering using `ContextGenerator` = RAG + Prompt Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai.infer.rag.context_generator import ContextGenerator\n",
    "\n",
    "# Context Generator 1: k=2 RAG docs + k=1 fewshot example (lightweight)\n",
    "context_generator_1 = ContextGenerator(\n",
    "    rag_spec=rag_gpu_k2,\n",
    "    prompt_manager=fewshot_prompt_manager_k1\n",
    ")\n",
    "\n",
    "# Context Generator 2: k=3 RAG docs + k=2 fewshot examples (richer context)\n",
    "context_generator_2 = ContextGenerator(\n",
    "    rag_spec=rag_gpu_k3,\n",
    "    prompt_manager=fewshot_prompt_manager_k2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546221",
   "metadata": {},
   "source": [
    "### Model config using `VLLMModelConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956201bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 3 pipelines:\n",
      "  - Pipeline 1 & 2: Same context (3 RAG docs, 2 fewshot), different models (3B vs 1.5B)\n",
      "  - Pipeline 1 & 3: Same model (3B), different contexts (Context1 vs Context2)\n"
     ]
    }
   ],
   "source": [
    "from rapidfireai.infer.utils.config import VLLMModelConfig\n",
    "\n",
    "# Pipeline 1: Qwen 0.5B + Context 1 (ultra-light, fits easily in T4)\n",
    "pipeline_1 = VLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",  # 0.5B params\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.5,  # Conservative for T4\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 1536,  # Reduced from 2048\n",
    "        \"disable_log_stats\": True,\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 384,  # Reduced from 512\n",
    "    },\n",
    "    context_generator=context_generator_1\n",
    ")\n",
    "\n",
    "# Pipeline 2: Qwen 1.5B + Context 2 (still fits in T4 with room to spare)\n",
    "pipeline_2 = VLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",  # 1.5B params\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.5,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 1536,\n",
    "        \"disable_log_stats\": True,\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 384,\n",
    "    },\n",
    "    context_generator=context_generator_2\n",
    ")\n",
    "\n",
    "# Create list of (name, config) tuples - ONLY 2 pipelines\n",
    "pipelines = [\n",
    "    (\"0.5B_LightContext\", pipeline_1),\n",
    "    (\"1.5B_RichContext\", pipeline_2),\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(pipelines)} pipelines (T4-optimized):\")\n",
    "print(f\"  - Pipeline 1: Qwen 0.5B + light context (2 RAG docs, 1 fewshot)\")\n",
    "print(f\"  - Pipeline 2: Qwen 1.5B + rich context (3 RAG docs, 2 fewshot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Utility, Preprocessor, Postprocessor, Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(answer):\n",
    "    solution = re.search(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", answer)\n",
    "    if solution is None:\n",
    "        return \"0\"\n",
    "    final_solution = solution.group(0)\n",
    "    final_solution = final_solution.split(\"#### \")[1].replace(\",\", \"\")\n",
    "    return final_solution\n",
    "\n",
    "def preprocess_fn(batch: Dict[str, List], context_generator: ContextGenerator) -> Dict[str, List]:\n",
    "\n",
    "    INSTRUCTIONS = context_generator.get_instructions()\n",
    "\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": INSTRUCTIONS\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f'Here are some examples: \\n{examples}. \\nHere is some additional context:\\n{context}. \\nNow answer the following question:\\n{question}'\n",
    "                }\n",
    "            ]\n",
    "            for question, examples,context in zip(\n",
    "                batch[\"question\"],\n",
    "                context_generator.get_fewshot_examples(user_queries=batch[\"question\"]),\n",
    "                context_generator.get_context(batch_queries=batch[\"question\"])\n",
    "            )\n",
    "        ],\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "def postprocess_fn(batch: Dict[str, List]) -> Dict[str, List]:\n",
    "    batch[\"model_answer\"] = [extract_solution(answer) for answer in batch[\"generated_text\"]]\n",
    "    batch[\"ground_truth\"] = [extract_solution(answer) for answer in batch[\"answer\"]]\n",
    "    return batch\n",
    "\n",
    "def compute_metrics_fn(batch: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    correct = sum(1 for pred, gt in zip(batch[\"model_answer\"], batch[\"ground_truth\"])\n",
    "                  if pred == gt)\n",
    "    total = len(batch[\"model_answer\"])\n",
    "    return {\n",
    "        \"Correct\": {\"value\": correct},\n",
    "        \"Total\": {\"value\": total},\n",
    "    }\n",
    "\n",
    "def accumulate_metrics_fn(aggregated_metrics: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    # aggregated_metrics is a dict of lists: {\"Correct\": [5, 3, 7], \"Total\": [10, 8, 12]}\n",
    "    correct = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Correct\", [{}]))\n",
    "    total = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Total\", [{}]))\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return {\n",
    "        \"Total\": {\"value\": total},\n",
    "        \"Correct\": {\"value\": correct, \"is_distributive\": True, \"value_range\": (0, 1)}, # 0 (min) if not correct, 1 if correct (max)\n",
    "        \"Accuracy\": {\"value\": accuracy, \"is_algebraic\": True, \"value_range\": (0, 1)} # Algebraic metric for online aggregation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4699f9",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0ec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 08:00:21,051\tINFO worker.py:2004 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "/home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'rapidfireai.infer.dispatcher.dispatcher'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.06it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:17,  1.97it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:00<00:15,  2.07it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:15,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:01<00:14,  2.11it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:14,  2.13it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:02<00:13,  2.15it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:12,  2.16it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:03<00:12,  2.16it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:11,  2.17it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:04<00:11,  2.18it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:10,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:05<00:10,  2.21it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:09,  2.21it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:06<00:10,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:09,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:07<00:09,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:07<00:08,  2.13it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:08<00:07,  2.17it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:08<00:07,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:09<00:06,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:09<00:06,  2.14it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:10<00:06,  2.16it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:10<00:05,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:11<00:04,  2.20it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:11<00:04,  2.20it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:12<00:04,  2.21it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:12<00:03,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:12<00:03,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:13<00:02,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:13<00:02,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:14<00:01,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:14<00:01,  2.23it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:15<00:00,  2.22it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:16<00:00,  2.17it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m /home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/langchain_core/globals.py:148: UserWarning: Importing debug from langchain root module is no longer supported. Please use langchain.globals.set_debug() / langchain.globals.get_debug() instead.\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m   old_debug = langchain.debug\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m /home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/langchain_core/globals.py:148: UserWarning: Importing debug from langchain root module is no longer supported. Please use langchain.globals.set_debug() / langchain.globals.get_debug() instead.\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m   old_debug = langchain.debug\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.78it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.78it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:17,  1.95it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:00<00:15,  2.13it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:14,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:01<00:13,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:13,  2.18it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:02<00:13,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:12,  2.19it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:03<00:12,  2.20it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:11,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:04<00:11,  2.23it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:04<00:10,  2.24it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:05<00:10,  2.23it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:05<00:09,  2.26it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:06<00:09,  2.26it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:06<00:08,  2.27it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:07<00:08,  2.27it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:07<00:08,  2.22it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:08<00:07,  2.23it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:08<00:07,  2.25it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:08<00:06,  2.26it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:09<00:06,  2.23it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:09<00:05,  2.23it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:10<00:05,  2.24it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:10<00:04,  2.26it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:11<00:04,  2.25it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:11<00:04,  2.24it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:12<00:03,  2.18it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:12<00:03,  2.13it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:13<00:02,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:13<00:02,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:14<00:01,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:14<00:01,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:15<00:00,  2.02it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:16<00:00,  2.16it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m /home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/langchain_core/globals.py:148: UserWarning: Importing debug from langchain root module is no longer supported. Please use langchain.globals.set_debug() / langchain.globals.get_debug() instead.\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m   old_debug = langchain.debug\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m /home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/langchain_core/globals.py:148: UserWarning: Importing debug from langchain root module is no longer supported. Please use langchain.globals.set_debug() / langchain.globals.get_debug() instead.\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m   old_debug = langchain.debug\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m /home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/langchain_core/globals.py:148: UserWarning: Importing debug from langchain root module is no longer supported. Please use langchain.globals.set_debug() / langchain.globals.get_debug() instead.\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m   old_debug = langchain.debug\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m /home/palebluedot/miniconda3/envs/infer/lib/python3.10/site-packages/langchain_core/globals.py:148: UserWarning: Importing debug from langchain root module is no longer supported. Please use langchain.globals.set_debug() / langchain.globals.get_debug() instead.\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m   old_debug = langchain.debug\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.87it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.03it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:17,  1.93it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:16,  1.99it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:16,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:16,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.97it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:14,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:13,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:12,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:11,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:11,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:10,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:06<00:10,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:09,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:07<00:09,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:08<00:08,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:08<00:08,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:09<00:07,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:09<00:07,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:10<00:06,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:10<00:06,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:11<00:05,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:11<00:05,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:12<00:04,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:12<00:04,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:13<00:03,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:13<00:03,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:14<00:02,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:14<00:02,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:01,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:15<00:01,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:16<00:00,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:16<00:00,  2.06it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:17<00:00,  2.03it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.05it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:18,  1.84it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:17,  1.91it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:16,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:16,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:15,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:13,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:13,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:12,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:11,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:10,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:07<00:10,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:09,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:08<00:09,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:08<00:09,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:09<00:08,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:09<00:07,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:10<00:07,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:10<00:06,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:11<00:06,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:11<00:05,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:12<00:05,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:12<00:05,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:13<00:04,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:13<00:03,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:14<00:03,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:14<00:03,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:15<00:02,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:02,  1.97it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:16<00:01,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:16<00:01,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:17<00:00,  1.96it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:17<00:00,  1.98it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:17,  1.99it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:17,  1.91it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:16,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:15,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:14,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:02<00:14,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:13,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:03<00:13,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:12,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:04<00:12,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:11,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:05<00:11,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:10,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:06<00:10,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:09,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:07<00:09,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:08<00:08,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:08<00:08,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:09<00:07,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:09<00:07,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:10<00:06,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:10<00:06,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:11<00:05,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:11<00:05,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:12<00:04,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:12<00:04,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:13<00:03,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:13<00:03,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:14<00:02,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:14<00:02,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:01,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:15<00:01,  2.08it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:16<00:00,  2.09it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:16<00:00,  2.10it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:17<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.59it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.76it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.74it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:21,  1.56it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:18,  1.76it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:17,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:16,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:14,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:13,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:13,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:12,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:12,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:11,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:07<00:11,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:10,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:08<00:10,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:08<00:09,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:09<00:08,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:09<00:08,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:10<00:07,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:10<00:07,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:11<00:07,  1.86it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:12<00:06,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:12<00:05,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:13<00:05,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:13<00:04,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:14<00:04,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:14<00:03,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:03,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:15<00:02,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:16<00:02,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:16<00:01,  1.87it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:17<00:01,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:17<00:00,  1.90it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:18<00:00,  1.90it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:31,  1.08it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:31,  1.05it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:02<00:29,  1.08it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:03<00:25,  1.21it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:04<00:23,  1.26it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:04<00:22,  1.32it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:05<00:20,  1.39it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:06<00:18,  1.45it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:06<00:17,  1.49it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:07<00:16,  1.53it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:08<00:15,  1.56it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:08<00:14,  1.60it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:09<00:13,  1.66it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:09<00:12,  1.66it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:10<00:11,  1.70it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:10<00:11,  1.72it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:11<00:10,  1.73it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:11<00:09,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:12<00:08,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:13<00:08,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:13<00:07,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:14<00:07,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:14<00:06,  1.79it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:15<00:06,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:05,  1.81it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:16<00:04,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:16<00:04,  1.85it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:17<00:03,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:18<00:03,  1.78it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:18<00:02,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:19<00:02,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:19<00:01,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:20<00:01,  1.84it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:20<00:00,  1.84it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:21<00:00,  1.64it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:19,  1.77it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:17,  1.85it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:17,  1.88it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:16,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:15,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:14,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:13,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:12,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:12,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:11,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:07<00:10,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:10,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:08<00:09,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:08<00:09,  1.92it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:09<00:09,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:09<00:08,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:10<00:07,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:11<00:07,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:11<00:06,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:12<00:06,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:12<00:05,  1.91it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:13<00:05,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:13<00:04,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:14<00:04,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:14<00:03,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:03,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:15<00:02,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:16<00:02,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:16<00:01,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:17<00:01,  1.97it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:17<00:00,  1.96it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:18<00:00,  1.92it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.65it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.92it/s]\n",
      "\u001b[36m(QueryProcessingActor pid=1459581)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:17,  1.92it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:16,  1.98it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:16,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:15,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:14,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:13,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:13,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:12,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:11,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:11,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:07<00:10,  1.97it/s]\n",
      "Capturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:10,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:08<00:09,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:08<00:09,  1.97it/s]\n",
      "Capturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:09<00:08,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:09<00:08,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:10<00:07,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:10<00:06,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:11<00:06,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:11<00:06,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:12<00:05,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:12<00:04,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:13<00:04,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:13<00:03,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:14<00:03,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:14<00:02,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:15<00:02,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:01,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:15<00:01,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:16<00:00,  2.07it/s]\n",
      "Capturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:16<00:00,  2.06it/s]\n",
      "Capturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:17<00:00,  2.01it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.13s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]\n",
      "\u001b[36m(QueryProcessingActor pid=1459574)\u001b[0m \n",
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
      "Capturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:19,  1.78it/s]\n",
      "Capturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:18,  1.82it/s]\n",
      "Capturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:17,  1.82it/s]\n",
      "Capturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:16,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.90it/s]\n",
      "Capturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:14,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:13,  1.93it/s]\n",
      "Capturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:13,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:12,  1.94it/s]\n",
      "Capturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:11,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:11,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:07<00:10,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# T4 is a single GPU, so use num_actors=1\n",
    "experiment = Experiment(\n",
    "    experiment_name=\"trial-context-generator-colab-t4\",\n",
    "    num_actors=1  # Changed from 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Context Generation ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5184f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5184f_level0_col0\" class=\"col_heading level0 col0\" >Context ID</th>\n",
       "      <th id=\"T_5184f_level0_col1\" class=\"col_heading level0 col1\" >Context Hash</th>\n",
       "      <th id=\"T_5184f_level0_col2\" class=\"col_heading level0 col2\" >Status</th>\n",
       "      <th id=\"T_5184f_level0_col3\" class=\"col_heading level0 col3\" >Duration</th>\n",
       "      <th id=\"T_5184f_level0_col4\" class=\"col_heading level0 col4\" >Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5184f_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_5184f_row0_col1\" class=\"data row0 col1\" >0867fad4...</td>\n",
       "      <td id=\"T_5184f_row0_col2\" class=\"data row0 col2\" >Complete</td>\n",
       "      <td id=\"T_5184f_row0_col3\" class=\"data row0 col3\" >38.8s</td>\n",
       "      <td id=\"T_5184f_row0_col4\" class=\"data row0 col4\" >FAISS, GPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5184f_row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "      <td id=\"T_5184f_row1_col1\" class=\"data row1 col1\" >1ef40f9a...</td>\n",
       "      <td id=\"T_5184f_row1_col2\" class=\"data row1 col2\" >Complete</td>\n",
       "      <td id=\"T_5184f_row1_col3\" class=\"data row1 col3\" >39.9s</td>\n",
       "      <td id=\"T_5184f_row1_col4\" class=\"data row1 col4\" >FAISS, GPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7eaa4d9d7e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multi-Pipeline Experiment Progress ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_263e6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_263e6_level0_col0\" class=\"col_heading level0 col0\" >Pipeline ID</th>\n",
       "      <th id=\"T_263e6_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_263e6_level0_col2\" class=\"col_heading level0 col2\" >Model</th>\n",
       "      <th id=\"T_263e6_level0_col3\" class=\"col_heading level0 col3\" >Progress</th>\n",
       "      <th id=\"T_263e6_level0_col4\" class=\"col_heading level0 col4\" >Confidence</th>\n",
       "      <th id=\"T_263e6_level0_col5\" class=\"col_heading level0 col5\" >Accuracy</th>\n",
       "      <th id=\"T_263e6_level0_col6\" class=\"col_heading level0 col6\" >Throughput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_263e6_row0_col0\" class=\"data row0 col0\" >102</td>\n",
       "      <td id=\"T_263e6_row0_col1\" class=\"data row0 col1\" >3B_Context1</td>\n",
       "      <td id=\"T_263e6_row0_col2\" class=\"data row0 col2\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_263e6_row0_col3\" class=\"data row0 col3\" >3/4</td>\n",
       "      <td id=\"T_263e6_row0_col4\" class=\"data row0 col4\" >0.013</td>\n",
       "      <td id=\"T_263e6_row0_col5\" class=\"data row0 col5\" >49.22%</td>\n",
       "      <td id=\"T_263e6_row0_col6\" class=\"data row0 col6\" >3.5/s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_263e6_row1_col0\" class=\"data row1 col0\" >103</td>\n",
       "      <td id=\"T_263e6_row1_col1\" class=\"data row1 col1\" >1.5B_Context1</td>\n",
       "      <td id=\"T_263e6_row1_col2\" class=\"data row1 col2\" >Qwen/Qwen2.5-1.5B-Instruct</td>\n",
       "      <td id=\"T_263e6_row1_col3\" class=\"data row1 col3\" >3/4</td>\n",
       "      <td id=\"T_263e6_row1_col4\" class=\"data row1 col4\" >0.011</td>\n",
       "      <td id=\"T_263e6_row1_col5\" class=\"data row1 col5\" >28.26%</td>\n",
       "      <td id=\"T_263e6_row1_col6\" class=\"data row1 col6\" >3.6/s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_263e6_row2_col0\" class=\"data row2 col0\" >104</td>\n",
       "      <td id=\"T_263e6_row2_col1\" class=\"data row2 col1\" >3B_Context2</td>\n",
       "      <td id=\"T_263e6_row2_col2\" class=\"data row2 col2\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_263e6_row2_col3\" class=\"data row2 col3\" >0/4</td>\n",
       "      <td id=\"T_263e6_row2_col4\" class=\"data row2 col4\" >-</td>\n",
       "      <td id=\"T_263e6_row2_col5\" class=\"data row2 col5\" >-</td>\n",
       "      <td id=\"T_263e6_row2_col6\" class=\"data row2 col6\" >-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7eaa4d048af0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_by_run = experiment.run_evals(\n",
    "    configs=pipelines,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,  # 32\n",
    "    num_shards=4,\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    postprocess_fn=postprocess_fn,\n",
    "    compute_metrics_fn=compute_metrics_fn,\n",
    "    accumulate_metrics_fn=accumulate_metrics_fn,\n",
    "    online_strategy_kwargs={\"strategy_name\": \"normal\", \"confidence_level\": 0.95, \"use_fpc\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756344e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS FOR ALL PIPELINES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for pipeline_id, (aggregated_results, metrics) in results_by_pipeline.items():\n",
    "    # Get pipeline name from the original configs\n",
    "    pipeline_name = [name for name, _ in pipelines][pipeline_id - 1] if pipeline_id <= len(pipelines) else f\"Pipeline {pipeline_id}\"\n",
    "\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Pipeline: {pipeline_name.upper()} (ID: {pipeline_id})\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(json.dumps(metrics, indent=4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE OUTPUTS FROM EACH PIPELINE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "num_examples_to_show = 2  # Show 2 examples per pipeline\n",
    "\n",
    "for pipeline_id, (aggregated_results, metrics) in results_by_pipeline.items():\n",
    "    # Get pipeline name from the original configs\n",
    "    pipeline_name = [name for name, _ in pipelines][pipeline_id - 1] if pipeline_id <= len(pipelines) else f\"Pipeline {pipeline_id}\"\n",
    "\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Pipeline: {pipeline_name.upper()} (ID: {pipeline_id})\")\n",
    "    print(f\"{'-'*80}\\n\")\n",
    "\n",
    "    samples_available = min(num_examples_to_show, metrics['Samples Processed']['value'])\n",
    "\n",
    "    for i in range(samples_available):\n",
    "        print(f\"\\n{'~'*40}\")\n",
    "        print(f\"Example {i+1}/{samples_available}\")\n",
    "        print(f\"{'~'*40}\")\n",
    "\n",
    "        prompt = aggregated_results['prompts'][i]\n",
    "        print(f\"\\nSystem Instructions:\")\n",
    "        print(f\"{prompt[0]['content'][:200]}...\")  # Truncate long instructions\n",
    "\n",
    "        print(f\"\\nUser Query + Context:\")\n",
    "        print(f\"{prompt[1]['content'][:300]}...\")  # Truncate long context\n",
    "\n",
    "        print(f\"\\nModel Output:\")\n",
    "        print(f\"{aggregated_results['generated_text'][i]}\")\n",
    "\n",
    "        print(f\"\\nGround Truth:\")\n",
    "        print(f\"{aggregated_results['ground_truth'][i]}\")\n",
    "\n",
    "        print(f\"\\nExtracted Answer:\")\n",
    "        print(f\"Model: {aggregated_results['model_answer'][i]} | Truth: {aggregated_results['ground_truth'][i]}\")\n",
    "\n",
    "        is_correct = aggregated_results['model_answer'][i] == aggregated_results['ground_truth'][i]\n",
    "        print(f\"\u2713 CORRECT\" if is_correct else \"\u2717 INCORRECT\")\n",
    "\n",
    "    print(f\"\\n{'-'*80}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}