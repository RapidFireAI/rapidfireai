{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e06d0c6d",
      "metadata": {
        "id": "e06d0c6d"
      },
      "source": [
        "<div align=\"center\">\n",
        "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
        "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
        "<br/>\n",
        "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
        "<br/>\n",
        "üëâ <b>Note:</b> This Colab notebook illustrates simplified usage of <code>rapidfireai</code>. For the full RapidFire AI experience with advanced experiment manager, UI, and production features, see our <a href=\\\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\\\">Install and Get Started</a> guide.\n",
        "<br/>\n",
        "üé¨ Watch our <a href=\\\"https://youtu.be/vVXorey0ANk\\\">intro video</a> to get started!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "001983a2",
      "metadata": {
        "id": "001983a2"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RapidFireAI/rapidfireai/blob/main/tutorial_notebooks/rag-contexteng/rf-colab-rag-fiqa-tutorial.ipynb)\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT:** Do not let the Colab notebook tab stay idle for more than 5min; Colab will disconnect otherwise. Interact with the cells to avoid disconnection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644fc36b",
      "metadata": {
        "id": "644fc36b"
      },
      "source": [
        "# Optimizing RAG Pipelines with RapidFire AI\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a practical way to make an AI assistant **answer using your documents**:\n",
        "\n",
        "- **Retrieve**: find the most relevant passages for a question.\n",
        "- **Generate**: give those passages to a language model so it can answer *grounded in evidence*.\n",
        "\n",
        "In this beginner-friendly Colab, we‚Äôll build and evaluate a RAG pipeline for a **financial opinion Q&A** assistant using the [FiQA dataset](https://huggingface.co/datasets/explodinggradients/fiqa).\n",
        "\n",
        "Examples of the kind of questions we‚Äôre targeting:\n",
        "\n",
        "- ‚ÄúShould I invest in index funds or individual stocks?‚Äù\n",
        "- ‚ÄúWhat‚Äôs a good way to save for retirement in my 30s?‚Äù\n",
        "- ‚ÄúIs it worth refinancing my mortgage right now?‚Äù\n",
        "\n",
        "## What We‚Äôre Building\n",
        "\n",
        "A concrete RAG pipeline that looks like this:\n",
        "\n",
        "1. **Load a financial corpus** (documents + posts).\n",
        "2. **Split documents into chunks** (so we can search smaller, more relevant pieces).\n",
        "3. **Embed the chunks** (turn text into vectors) and store them in a vector index (FAISS).\n",
        "4. **Retrieve top‚ÄëK chunks** for each question using similarity search.\n",
        "5. *(Optional)* **Rerank** the retrieved chunks with a stronger model to keep only the best evidence.\n",
        "6. **Build a prompt** that includes the question + retrieved context.\n",
        "7. **Generate an answer** with a small vLLM model.\n",
        "8. **Evaluate retrieval quality** (Precision, Recall, NDCG@5, MRR) so we can tell which settings find better evidence.\n",
        "\n",
        "## Our Approach\n",
        "\n",
        "RAG has a lot of ‚Äúknobs‚Äù, and it‚Äôs easy to lose track of what helped. In this notebook we‚Äôll focus on **retrieval quality** by keeping the generator (the vLLM model) fixed and only varying retrieval settings.\n",
        "\n",
        "We‚Äôll use [RapidFireAI](https://github.com/RapidFireAI/rapidfireai) to:\n",
        "\n",
        "- **Define a small retrieval grid**: 2 chunking strategies √ó 2 reranking `top_n` values = **4 retrieval configs**.\n",
        "- **Run all configs the same way** on the same dataset.\n",
        "- **Compare retrieval metrics side-by-side** as they update (Precision/Recall/NDCG/MRR) to pick the best evidence-finding setup."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c769cc",
      "metadata": {
        "id": "e4c769cc"
      },
      "source": [
        "## Install RapidFire AI Package and Setup\n",
        "### Option 1: Install Locally (or on a VM)\n",
        "For the full RapidFire AI experience‚Äîadvanced experiment management, UI, and production features‚Äîwe recommend installing the package on a machine you control (for example, a VM or your local machine) rather than Google Colab. See our [Install and Get Started](https://oss-docs.rapidfire.ai/en/latest/walkthrough.html) guide.\n",
        "\n",
        "### Option 2: Install in Google Colab\n",
        "For simplicity, you can run this notebook on Google Colab. This notebook is configured to run end-to-end on Colab with no local installation required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d0e23e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0e23e77",
        "outputId": "0b80c55b-1cad-493d-de71-33e7a924c407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/RapidFireAI/rapidfireai.git@bugfix/DBCommitForMetricLogger\n",
            "  Cloning https://github.com/RapidFireAI/rapidfireai.git (to revision bugfix/DBCommitForMetricLogger) to /tmp/pip-req-build-pq_ba87d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/RapidFireAI/rapidfireai.git /tmp/pip-req-build-pq_ba87d\n",
            "  Running command git checkout -b bugfix/DBCommitForMetricLogger --track origin/bugfix/DBCommitForMetricLogger\n",
            "  Switched to a new branch 'bugfix/DBCommitForMetricLogger'\n",
            "  Branch 'bugfix/DBCommitForMetricLogger' set up to track remote branch 'bugfix/DBCommitForMetricLogger' from 'origin'.\n",
            "  Resolved https://github.com/RapidFireAI/rapidfireai.git to commit 929e31aec205b07c6bacbc59bff0bba191fca3e4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from rapidfireai==0.12.8) (3.1.2)\n",
            "Collecting flask-cors (from rapidfireai==0.12.8)\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting waitress (from rapidfireai==0.12.8)\n",
            "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jq (from rapidfireai==0.12.8)\n",
            "  Downloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from rapidfireai==0.12.8) (0.3.8)\n",
            "Collecting jedi (from rapidfireai==0.12.8)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting uv (from rapidfireai==0.12.8)\n",
            "  Downloading uv-0.9.24-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting trackio (from rapidfireai==0.12.8)\n",
            "  Downloading trackio-0.15.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting mlflow (from rapidfireai==0.12.8)\n",
            "  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: fsspec<=2025.10.0 in /usr/local/lib/python3.12/dist-packages (from rapidfireai==0.12.8) (2025.3.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->rapidfireai==0.12.8) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->rapidfireai==0.12.8) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->rapidfireai==0.12.8) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask->rapidfireai==0.12.8) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->rapidfireai==0.12.8) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->rapidfireai==0.12.8) (3.1.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi->rapidfireai==0.12.8) (0.8.5)\n",
            "Collecting mlflow-skinny==3.8.1 (from mlflow->rapidfireai==0.12.8)\n",
            "  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.8.1 (from mlflow->rapidfireai==0.12.8)\n",
            "  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow->rapidfireai==0.12.8)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow->rapidfireai==0.12.8)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow->rapidfireai==0.12.8)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow->rapidfireai==0.12.8)\n",
            "  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->rapidfireai==0.12.8) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (6.2.4)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8)\n",
            "  Downloading databricks_sdk-0.77.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (3.1.46)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (8.7.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.5.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.40.0)\n",
            "Collecting gradio<7.0.0,>=6.3.0 (from gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8)\n",
            "  Downloading gradio-6.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0.0 in /usr/local/lib/python3.12/dist-packages (from trackio->rapidfireai==0.12.8) (0.36.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from trackio->rapidfireai==0.12.8) (3.11.5)\n",
            "Requirement already satisfied: pillow<12.0.0 in /usr/local/lib/python3.12/dist-packages (from trackio->rapidfireai==0.12.8) (11.3.0)\n",
            "Collecting plotly<7.0.0,>=6.0.0 (from trackio->rapidfireai==0.12.8)\n",
            "  Downloading plotly-6.5.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydub<1.0.0 in /usr/local/lib/python3.12/dist-packages (from trackio->rapidfireai==0.12.8) (0.25.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow->rapidfireai==0.12.8) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow->rapidfireai==0.12.8) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow->rapidfireai==0.12.8) (2.5.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (1.0.0)\n",
            "Collecting gradio-client==2.0.3 (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8)\n",
            "  Downloading gradio_client-2.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.0.21)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.21.1)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.12/dist-packages (from gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (1.6.6)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow->rapidfireai==0.12.8)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow->rapidfireai==0.12.8)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow->rapidfireai==0.12.8) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0.0->trackio->rapidfireai==0.12.8) (3.20.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0.0->trackio->rapidfireai==0.12.8) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0.0->trackio->rapidfireai==0.12.8) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow->rapidfireai==0.12.8) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow->rapidfireai==0.12.8) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow->rapidfireai==0.12.8) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow->rapidfireai==0.12.8) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow->rapidfireai==0.12.8) (3.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow->rapidfireai==0.12.8) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow->rapidfireai==0.12.8) (2025.3)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from plotly<7.0.0,>=6.0.0->trackio->rapidfireai==0.12.8) (2.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow->rapidfireai==0.12.8) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow->rapidfireai==0.12.8) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->rapidfireai==0.12.8) (3.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (3.11)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow->rapidfireai==0.12.8) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (2.43.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow->rapidfireai==0.12.8) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (3.4.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (13.9.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (4.9.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<7.0.0,>=6.3.0->gradio[oauth]<7.0.0,>=6.3.0->trackio->rapidfireai==0.12.8) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->rapidfireai==0.12.8) (0.6.1)\n",
            "Downloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trackio-0.15.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uv-0.9.24-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.2/56.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-6.3.0-py3-none-any.whl (23.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-2.0.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly-6.5.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.77.0-py3-none-any.whl (779 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m779.2/779.2 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: rapidfireai\n",
            "  Building wheel for rapidfireai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rapidfireai: filename=rapidfireai-0.12.8-py3-none-any.whl size=21987118 sha256=54b3396fa79adce9dce61081f14f145306c4b38ac59e3516cc430aef3fdbc502\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lnby4f4t/wheels/c1/a5/8d/6c8a84fad3a064b6eb9ded28f84ab3608aa57c4ab808ff0b35\n",
            "Successfully built rapidfireai\n",
            "Installing collected packages: huey, waitress, uv, plotly, jq, jedi, gunicorn, graphql-core, graphql-relay, docker, graphene, gradio-client, flask-cors, databricks-sdk, gradio, mlflow-tracing, mlflow-skinny, trackio, mlflow, rapidfireai\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "Successfully installed databricks-sdk-0.77.0 docker-7.1.0 flask-cors-6.0.2 gradio-6.3.0 gradio-client-2.0.3 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.6.0 jedi-0.19.2 jq-1.10.0 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1 plotly-6.5.1 rapidfireai-0.12.8 trackio-0.15.0 uv-0.9.24 waitress-3.0.2\n",
            "Created directory: /content/rapidfireai/logs\n",
            "üîß Initializing RapidFire AI project...\n",
            "------------------------------\n",
            "Initializing project...\n",
            "Colab environment detected, installing evals packages\n",
            "Installing packages from /usr/local/lib/python3.12/dist-packages/setup/evals/requirements-colab.txt...\n",
            "‚úÖ Successfully installed packages from /usr/local/lib/python3.12/dist-packages/setup/evals/requirements-colab.txt\n",
            "Getting tutorial notebooks...\n",
            "Copying tutorial notebooks from /usr/local/lib/python3.12/dist-packages/tutorial_notebooks to ./tutorial_notebooks...\n",
            "‚úÖ Successfully copied notebooks to ./tutorial_notebooks\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import rapidfireai\n",
        "    print(\"‚úÖ rapidfireai already installed\")\n",
        "except ImportError:\n",
        "    %pip install git+https://github.com/RapidFireAI/rapidfireai.git@bugfix/DBCommitForMetricLogger  # Takes 1 min\n",
        "    !rapidfireai init --evals # Takes 1 min"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b703a70",
      "metadata": {
        "id": "3b703a70"
      },
      "source": [
        "### Import RapidFire Components\n",
        "\n",
        "Import RapidFire‚Äôs core classes for defining the RAG pipeline and running a small retrieval grid search (plus a Colab-friendly protobuf setting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3f8598e1",
      "metadata": {
        "id": "3f8598e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['RF_TRACKIO_ENABLED'] = 'true'\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "\n",
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
        "import re, json\n",
        "from typing import List as listtype, Dict, Any\n",
        "\n",
        "# If you get \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\" from Colab, just rerun this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e16327b",
      "metadata": {
        "id": "9e16327b"
      },
      "source": [
        "### Loading the Data\n",
        "\n",
        "We load the FiQA **queries** and **relevance labels (qrels)**, then downsample to keep this Colab run fast.\n",
        "Next we filter the corpus to only documents relevant to the sampled queries and write a smaller `corpus_sampled.jsonl`.\n",
        "Finally, we update `qrels` to match the sampled subset so evaluation stays consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee571098",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "0e20789803bc435a85b9c94b8f335453",
            "4090404abb644b7497a6d15afd3e46b5",
            "3ea846268d004d7184bb911db67a2708",
            "872cdc12775c463ba2ba3b3864e42c8d",
            "92594a327bb14b4da0e7b6f8d6c30576",
            "9dd5fcdf2c2c485a86626997081e62be",
            "489946a99b2d45828a3c7b9b4def319e",
            "73fc6e1fb17845998b05828786f537f3",
            "f2c37210bbd945cebbc24dde8a2eb151",
            "7a1fdc3feeaa4088a8df21e9735c820e",
            "618038b1576d4a7e84781a4c0e8ba017"
          ]
        },
        "id": "ee571098",
        "outputId": "4eae6ff9-ce81-40f6-fd61-ec3f282b5721"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e20789803bc435a85b9c94b8f335453"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 6 queries\n",
            "Found 16 relevant documents for these queries\n",
            "Sampled 16 documents from 57638 total\n",
            "Saved to: /content/tutorial_notebooks/rag-contexteng/datasets/fiqa/corpus_sampled.jsonl\n",
            "Filtered qrels to 16 relevance judgments\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "# Dataset directory\n",
        "dataset_dir = Path(\"/content/tutorial_notebooks/rag-contexteng/datasets\")\n",
        "\n",
        "# Load full dataset\n",
        "fiqa_dataset = load_dataset(\"json\", data_files=str(dataset_dir / \"fiqa\" / \"queries.jsonl\"), split=\"train\")\n",
        "fiqa_dataset = fiqa_dataset.rename_columns({\"text\": \"query\", \"_id\": \"query_id\"})\n",
        "qrels = pd.read_csv(str(dataset_dir / \"fiqa\" / \"qrels.tsv\"), sep=\"\\t\")\n",
        "qrels = qrels.rename(\n",
        "    columns={\"query-id\": \"query_id\", \"corpus-id\": \"corpus_id\", \"score\": \"relevance\"}\n",
        ")\n",
        "\n",
        "# Downsample queries and corpus JOINTLY\n",
        "sample_fraction = 0.001  # low sample_fraction makes demo faster but degrades metrics; set to 1.0 for full evals if running on a local machine.\n",
        "rseed = 1\n",
        "random.seed(rseed)\n",
        "\n",
        "# Step 1: Sample queries\n",
        "sample_size = int(len(fiqa_dataset) * sample_fraction)\n",
        "fiqa_dataset = fiqa_dataset.shuffle(seed=rseed).select(range(sample_size))\n",
        "\n",
        "# Convert query_ids to integers for matching\n",
        "query_ids = set([int(qid) for qid in fiqa_dataset[\"query_id\"]])\n",
        "\n",
        "# Step 2: Get all corpus docs relevant to sampled queries\n",
        "qrels_filtered = qrels[qrels[\"query_id\"].isin(query_ids)]\n",
        "relevant_corpus_ids = set(qrels_filtered[\"corpus_id\"].tolist())\n",
        "\n",
        "print(f\"Using {len(fiqa_dataset)} queries\")\n",
        "print(f\"Found {len(relevant_corpus_ids)} relevant documents for these queries\")\n",
        "\n",
        "# Step 3: Load corpus and filter to relevant docs\n",
        "input_file = dataset_dir / \"fiqa\" / \"corpus.jsonl\"\n",
        "output_file = dataset_dir / \"fiqa\" / \"corpus_sampled.jsonl\"\n",
        "\n",
        "with open(input_file, 'r') as f:\n",
        "    all_corpus = [json.loads(line) for line in f]\n",
        "\n",
        "# Keep only relevant documents (convert _id to int for matching)\n",
        "sampled_corpus = [doc for doc in all_corpus if int(doc[\"_id\"]) in relevant_corpus_ids]\n",
        "\n",
        "# Write sampled corpus\n",
        "with open(output_file, 'w') as f:\n",
        "    for doc in sampled_corpus:\n",
        "        f.write(json.dumps(doc) + '\\n')\n",
        "\n",
        "print(f\"Sampled {len(sampled_corpus)} documents from {len(all_corpus)} total\")\n",
        "print(f\"Saved to: {output_file}\")\n",
        "print(f\"Filtered qrels to {len(qrels_filtered)} relevance judgments\")\n",
        "\n",
        "# Update qrels to match\n",
        "qrels = qrels_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73a21ee",
      "metadata": {
        "id": "a73a21ee"
      },
      "source": [
        "### Defining the RAG Search Space\n",
        "This is where RapidFireAI shines. Instead of hardcoding a single RAG configuration, we define a search space using RFLangChainRagSpec.\n",
        "\n",
        "We will test:\n",
        "\n",
        "* **2 Chunking Strategies**: Different chunk sizes (256 vs 128).\n",
        "* **2 Reranking Strategies**: Different `top_n` values (2 vs 5).\n",
        "\n",
        "This gives us 4 combinations to evaluate for the retrieval part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02b73586",
      "metadata": {
        "id": "02b73586"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "# Per-Actor batch size for hardware efficiency\n",
        "batch_size = 50\n",
        "\n",
        "# 2 chunk sizes x 2 reranking top-n = 4 combinations in total\n",
        "rag_gpu = RFLangChainRagSpec(\n",
        "    document_loader=DirectoryLoader(\n",
        "        path=str(dataset_dir / \"fiqa\"),\n",
        "        glob=\"corpus_sampled.jsonl\",\n",
        "        loader_cls=JSONLoader,\n",
        "        loader_kwargs={\n",
        "            \"jq_schema\": \".\",\n",
        "            \"content_key\": \"text\",\n",
        "            \"metadata_func\": lambda record, metadata: {\n",
        "                \"corpus_id\": int(record.get(\"_id\"))\n",
        "            },  # store the document id\n",
        "            \"json_lines\": True,\n",
        "            \"text_content\": False,\n",
        "        },\n",
        "        sample_seed=42,\n",
        "    ),\n",
        "    # 2 chunking strategies with different chunk sizes\n",
        "    text_splitter=List([\n",
        "            RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                encoding_name=\"gpt2\", chunk_size=256, chunk_overlap=32\n",
        "            ),\n",
        "            RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                encoding_name=\"gpt2\", chunk_size=128, chunk_overlap=32\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    embedding_cls=HuggingFaceEmbeddings,\n",
        "    embedding_kwargs={\n",
        "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "        \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
        "    },\n",
        "    vector_store=None,  # uses FAISS by default\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 8},\n",
        "    # 2 reranking strategies with different top-n values\n",
        "    reranker_cls=CrossEncoderReranker,\n",
        "    reranker_kwargs={\n",
        "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cpu\"},\n",
        "        \"top_n\": List([2, 5]),\n",
        "    },\n",
        "    enable_gpu_search=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6fb0a8",
      "metadata": {
        "id": "dd6fb0a8"
      },
      "source": [
        "### Define Data Processing and Postprocessing Functions\n",
        "\n",
        "We retrieve context for each question and turn it into LLM-ready prompts.\n",
        "Then we attach the ‚Äúground truth‚Äù relevant documents from FiQA (`qrels`) so we can score retrieval quality later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ecc17276",
      "metadata": {
        "id": "ecc17276"
      },
      "outputs": [],
      "source": [
        "def sample_preprocess_fn(\n",
        "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
        ") -> Dict[str, listtype]:\n",
        "    \"\"\"Function to prepare the final inputs given to the generator model\"\"\"\n",
        "\n",
        "    INSTRUCTIONS = \"Utilize your financial knowledge, give your answer or opinion to the input question or subject matter.\"\n",
        "\n",
        "    # Perform batched retrieval over all queries; returns a list of lists of k documents per query\n",
        "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
        "\n",
        "    # Extract the retrieved document ids from the context\n",
        "    retrieved_documents = [\n",
        "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
        "    ]\n",
        "\n",
        "    # Serialize the retrieved documents into a single string per query using the default template\n",
        "    serialized_context = rag.serialize_documents(all_context)\n",
        "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
        "\n",
        "    # Each batch to contain conversational prompt, retrieved documents, and original 'query_id', 'query', 'metadata'\n",
        "    return {\n",
        "        \"prompts\": [\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Here is some relevant context:\\n{context}. \\nNow answer the following question using the context provided earlier:\\n{question}\",\n",
        "                },\n",
        "            ]\n",
        "            for question, context in zip(batch[\"query\"], serialized_context)\n",
        "        ],\n",
        "        \"retrieved_documents\": retrieved_documents,\n",
        "        **batch,\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
        "    \"\"\"Function to postprocess outputs produced by generator model\"\"\"\n",
        "    # Get ground truth documents for each query; can be done in preprocess_fn too but done here for clarity\n",
        "    batch[\"ground_truth_documents\"] = [\n",
        "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
        "        for query_id in batch[\"query_id\"]\n",
        "    ]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39eb16c3",
      "metadata": {
        "id": "39eb16c3"
      },
      "source": [
        "### Define Custom Eval Metrics Functions\n",
        "\n",
        "The following helper methods compute standard retrieval metrics (Precision, Recall, F1, NDCG@5, MRR) from the retrieved vs. ground-truth document IDs.\n",
        "We compute metrics per batch and then combine them across batches so each config gets one consistent score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "22773d53",
      "metadata": {
        "id": "22773d53"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=5):\n",
        "    \"\"\"Utility function to compute NDCG@k\"\"\"\n",
        "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
        "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
        "\n",
        "    # IDCG: perfect ranking limited by min(k, len(expected_docs))\n",
        "    ideal_length = min(k, len(expected_docs))\n",
        "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
        "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "\n",
        "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
        "    \"\"\"Utility function to compute Reciprocal Rank (RR) for a single query\"\"\"\n",
        "    rr = 0\n",
        "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
        "        if retrieved_doc in expected_docs:\n",
        "            rr = 1 / (i + 1)\n",
        "            break\n",
        "    return rr\n",
        "\n",
        "\n",
        "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Function to compute all eval metrics based on retrievals and/or generations\"\"\"\n",
        "\n",
        "    true_positives, precisions, recalls, f1_scores, ndcgs, rrs = 0, [], [], [], [], []\n",
        "    total_queries = len(batch[\"query\"])\n",
        "\n",
        "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
        "        expected_set = set(gt)\n",
        "        retrieved_set = set(pred)\n",
        "\n",
        "        true_positives = len(expected_set.intersection(retrieved_set))\n",
        "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
        "        f1 = (\n",
        "            2 * precision * recall / (precision + recall)\n",
        "            if (precision + recall) > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=5))\n",
        "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
        "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
        "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
        "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
        "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_accumulate_metrics_fn(\n",
        "    aggregated_metrics: Dict[str, listtype],\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Function to accumulate eval metrics across all batches\"\"\"\n",
        "\n",
        "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
        "    total_queries = sum(num_queries_per_batch)\n",
        "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        **{\n",
        "            metric: {\n",
        "                \"value\": sum(\n",
        "                    m[\"value\"] * queries\n",
        "                    for m, queries in zip(\n",
        "                        aggregated_metrics[metric], num_queries_per_batch\n",
        "                    )\n",
        "                )\n",
        "                / total_queries,\n",
        "                \"is_algebraic\": True,\n",
        "                \"value_range\": (0, 1),\n",
        "            }\n",
        "            for metric in algebraic_metrics\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57887bc",
      "metadata": {
        "id": "c57887bc"
      },
      "source": [
        "### Define Partial Multi-Config Knobs for vLLM Generator part of RAG Pipeline using RapidFire AI Wrapper APIs\n",
        "\n",
        "We pick a lightweight vLLM model and sampling settings that fit in Colab GPU memory.\n",
        "Then we bundle the generator + our preprocessing/metrics functions into `config_set`, which RapidFire will run across the 4 retrieval configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8f5d0824",
      "metadata": {
        "id": "8f5d0824"
      },
      "outputs": [],
      "source": [
        "vllm_config1 = RFvLLMModelConfig(\n",
        "    model_config={\n",
        "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"dtype\": \"half\",\n",
        "        \"gpu_memory_utilization\": 0.25,\n",
        "        \"tensor_parallel_size\": 1,\n",
        "        \"distributed_executor_backend\": \"mp\",\n",
        "        \"enable_chunked_prefill\": False,\n",
        "        \"enable_prefix_caching\": False,\n",
        "        \"max_model_len\": 3000,\n",
        "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
        "        \"enforce_eager\": True,\n",
        "        \"disable_custom_all_reduce\": True,\n",
        "    },\n",
        "    sampling_params={\n",
        "        \"temperature\": 0.8,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 128,\n",
        "    },\n",
        "    rag=rag_gpu,\n",
        "    prompt_manager=None,\n",
        ")\n",
        "\n",
        "batch_size = 3 # Smaller batch size for generation\n",
        "config_set = {\n",
        "    \"vllm_config\": vllm_config1,  # Only 1 generator, but it represents 4 full configs\n",
        "    \"batch_size\": batch_size,\n",
        "    \"preprocess_fn\": sample_preprocess_fn,\n",
        "    \"postprocess_fn\": sample_postprocess_fn,\n",
        "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
        "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
        "    \"online_strategy_kwargs\": {\n",
        "        \"strategy_name\": \"normal\",\n",
        "        \"confidence_level\": 0.95,\n",
        "        \"use_fpc\": True,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7dd280",
      "metadata": {
        "id": "3a7dd280"
      },
      "source": [
        "### Create Config Group\n",
        "\n",
        "We create an `RFGridSearch` over `config_set`, producing **4 retrieval configs** (2 chunkers √ó 2 rerankers) to run and compare.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "67f26d9d",
      "metadata": {
        "id": "67f26d9d"
      },
      "outputs": [],
      "source": [
        "# Simple grid search across all config combinations: 4 total (2 chunkers √ó 2 rerankers)\n",
        "config_group = RFGridSearch(config_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b4b0d6",
      "metadata": {
        "id": "77b4b0d6"
      },
      "source": [
        "### Create Experiment\n",
        "\n",
        "An `Experiment` is RapidFire‚Äôs top-level container for this notebook run: it groups configs/runs, saves artifacts, and tracks metrics under a unique name.\n",
        "We set `mode=\"evals\"` because we‚Äôre running evaluation (not training). See the docs: https://oss-docs.rapidfire.ai/en/latest/experiment.html#api-experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ae4b5729",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ae4b5729",
        "outputId": "badcf8a7-a881-455f-d388-32c1d2639348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory for database at /content/rapidfireai/db\n",
            "Experiment exp1-fiqa-rag-colab created with Experiment ID: 1 at /content/rapidfireai/rapidfire_experiments/exp1-fiqa-rag-colab\n",
            "Created directory: /content/rapidfireai/logs/exp1-fiqa-rag-colab\n",
            "üåê Google Colab detected. Ray dashboard URL: https://8855-gpu-t4-s-2nqnb69r177ws-b.us-west1-0.prod.colab.dev\n",
            "üåê Google Colab detected. Dispatcher URL: https://8851-gpu-t4-s-2nqnb69r177ws-b.us-west1-0.prod.colab.dev\n"
          ]
        }
      ],
      "source": [
        "experiment = Experiment(experiment_name=\"exp1-fiqa-rag-colab\", mode=\"evals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8a0e92",
      "metadata": {
        "id": "6e8a0e92"
      },
      "source": [
        "### Display Ray Dashboard (Optional)\n",
        "\n",
        "Ray is the system RapidFire uses under the hood to run work in parallel; this cell simply embeds Ray‚Äôs dashboard below so we can monitor what‚Äôs running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c7447480",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "c7447480",
        "outputId": "982f79e1-f1cb-45fe-94a9-0ccae6b41c1a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8855, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display the Ray dashboard in the Colab notebook\n",
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(8855)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa186134",
      "metadata": {
        "id": "fa186134"
      },
      "source": [
        "### Run Multi-Config Evals + Launch Interactive Run Controller\n",
        "\n",
        "Now we get to the main function for running multi-config evals. Two tables will appear below the run_evals cell:\n",
        "- The first table will appear immediately. It lists all preprocessing/RAG sources.\n",
        "- After a short while, the second table will appear. It lists all individual runs with their knobs and metrics that are updated in real-time via online aggregation showing both estimates and confidence intervals.\n",
        "\n",
        "RapidFire AI also provides an Interactive Controller panel UI for Colab that lets you manage executing runs dynamically in real-time from the notebook:\n",
        "\n",
        "- ‚èπÔ∏è **Stop**: Gracefully stop a running config\n",
        "- ‚ñ∂Ô∏è **Resume**: Resume a stopped run\n",
        "- üóëÔ∏è **Delete**: Remove a run from this experiment\n",
        "- üìã **Clone**: Create a new run by editing the config dictionary of a parent run to try new knob values; optional warm start of parameters\n",
        "- üîÑ **Refresh**: Update run status and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8e07274a",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8851/dispatcher/list-all-pipeline-ids": {
              "data": "W10K",
              "ok": true,
              "headers": [
                [
                  "content-length",
                  "3"
                ],
                [
                  "content-type",
                  "application/json"
                ]
              ],
              "status": 200,
              "status_text": ""
            },
            "http://localhost:8851/dispatcher/get-pipeline-config-json/1": {
              "data": "eyJjb250ZXh0X2lkIjoxLCJwaXBlbGluZV9jb25maWdfanNvbiI6eyJiYXRjaF9zaXplIjozLCJtb2RlbF9jb25maWciOnsiZGlzYWJsZV9jdXN0b21fYWxsX3JlZHVjZSI6dHJ1ZSwiZGlzYWJsZV9sb2dfc3RhdHMiOnRydWUsImRpc3RyaWJ1dGVkX2V4ZWN1dG9yX2JhY2tlbmQiOiJtcCIsImR0eXBlIjoiaGFsZiIsImVuYWJsZV9jaHVua2VkX3ByZWZpbGwiOmZhbHNlLCJlbmFibGVfcHJlZml4X2NhY2hpbmciOmZhbHNlLCJlbmZvcmNlX2VhZ2VyIjp0cnVlLCJncHVfbWVtb3J5X3V0aWxpemF0aW9uIjowLjI1LCJtYXhfbW9kZWxfbGVuIjozMDAwLCJtb2RlbCI6IlF3ZW4vUXdlbjIuNS0wLjVCLUluc3RydWN0IiwidGVuc29yX3BhcmFsbGVsX3NpemUiOjF9LCJvbmxpbmVfc3RyYXRlZ3lfa3dhcmdzIjp7ImNvbmZpZGVuY2VfbGV2ZWwiOjAuOTUsInN0cmF0ZWd5X25hbWUiOiJub3JtYWwiLCJ1c2VfZnBjIjp0cnVlfSwicGlwZWxpbmVfdHlwZSI6InZsbG0iLCJyYWdfY29uZmlnIjp7ImNodW5rX292ZXJsYXAiOjMyLCJjaHVua19zaXplIjoyNTYsImsiOjgsInNlYXJjaF90eXBlIjoic2ltaWxhcml0eSIsInRvcF9uIjoyfSwic2FtcGxpbmdfcGFyYW1zIjp7Im1heF90b2tlbnMiOjEyOCwidGVtcGVyYXR1cmUiOjAuOCwidG9wX3AiOjAuOTV9fX0K",
              "ok": true,
              "headers": [
                [
                  "content-length",
                  "654"
                ],
                [
                  "content-type",
                  "application/json"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8e07274a",
        "outputId": "071dd6e6-1c98-4c61-a9dc-30b88081b8a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div id=\"controller_8b869975\" style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto;\">\n",
              "            <style>\n",
              "                #controller_8b869975 h3 { margin: 10px 0; font-size: 1.2em; font-weight: 600; }\n",
              "                #controller_8b869975 .header-info { display: flex; gap: 20px; margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 4px; font-size: 13px; }\n",
              "                #controller_8b869975 .section { margin: 15px 0; }\n",
              "                #controller_8b869975 .section-label { font-weight: 600; margin-bottom: 8px; font-size: 14px; }\n",
              "                #controller_8b869975 .button-row { display: flex; gap: 8px; flex-wrap: wrap; margin: 10px 0; }\n",
              "                #controller_8b869975 select { padding: 6px 12px; border: 1px solid #ccc; border-radius: 4px; font-size: 13px; background: white; min-width: 300px; cursor: pointer; }\n",
              "                #controller_8b869975 button { padding: 6px 16px; border: none; border-radius: 4px; font-size: 13px; font-weight: 500; cursor: pointer; }\n",
              "                #controller_8b869975 button:disabled { opacity: 0.5; cursor: not-allowed; }\n",
              "                #controller_8b869975 .btn-success { background: #28a745; color: white; }\n",
              "                #controller_8b869975 .btn-danger { background: #dc3545; color: white; }\n",
              "                #controller_8b869975 .btn-info { background: #17a2b8; color: white; }\n",
              "                #controller_8b869975 .btn-default { background: #6c757d; color: white; }\n",
              "                #controller_8b869975 textarea { width: 100%; min-height: 200px; padding: 10px; border: 1px solid #ccc; border-radius: 4px; font-family: 'Courier New', monospace; font-size: 12px; box-sizing: border-box; }\n",
              "                #controller_8b869975 .status-message { padding: 10px; margin: 10px 0; border-radius: 4px; display: none; }\n",
              "                #controller_8b869975 .msg-success { background: #d4edda; color: #155724; }\n",
              "                #controller_8b869975 .msg-error { background: #f8d7da; color: #721c24; }\n",
              "                #controller_8b869975 .msg-info { background: #d1ecf1; color: #0c5460; }\n",
              "            </style>\n",
              "\n",
              "            <div>\n",
              "                <h3>Interactive Run Controller</h3>\n",
              "                <div class=\"header-info\">\n",
              "                    <div><b>Run ID:</b> <span id=\"pipeline-id-value\">N/A</span></div>\n",
              "                    <div><b>Status:</b> <span id=\"status-value\">Not loaded</span></div>\n",
              "                    <div><b>Last Update:</b> <span id=\"last-update\">Never</span></div>\n",
              "                </div>\n",
              "\n",
              "                <div id=\"status-message\" class=\"status-message\"></div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"section-label\">Select a Config ID:</div>\n",
              "                    <select id=\"pipeline-selector\">\n",
              "                        <option value=\"\">Waiting for data...</option>\n",
              "                    </select>\n",
              "                </div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"button-row\">\n",
              "                        <button class=\"btn-success\" id=\"resume-btn\">‚ñ∂ Resume</button>\n",
              "                        <button class=\"btn-danger\" id=\"stop-btn\">‚ñ† Stop</button>\n",
              "                        <button class=\"btn-danger\" id=\"delete-btn\">üóë Delete</button>\n",
              "                    </div>\n",
              "                </div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"section-label\">Configuration: <span id=\"config-name\">N/A</span></div>\n",
              "                    <textarea id=\"config-text\" readonly>{}</textarea>\n",
              "                    <div class=\"button-row\">\n",
              "                        <button class=\"btn-info\" id=\"clone-btn\">Clone Run</button>\n",
              "                        <button class=\"btn-success\" id=\"submit-clone-btn\" disabled>‚úì Submit Clone</button>\n",
              "                        <button class=\"btn-danger\" id=\"cancel-clone-btn\" disabled>‚úó Cancel</button>\n",
              "                    </div>\n",
              "                </div>\n",
              "            </div>\n",
              "\n",
              "            <script>\n",
              "                (function() {\n",
              "                    const WIDGET_ID = 'controller_8b869975';\n",
              "                    const DISPATCHER_URL = 'https://localhost:8851';\n",
              "                    let currentPipelineId = null;\n",
              "                    let currentConfig = null;\n",
              "                    let currentContextId = null;\n",
              "                    let isCloneMode = false;\n",
              "                    let pollingInterval = null;\n",
              "\n",
              "                    // Elements\n",
              "                    const el = {\n",
              "                        pipelineIdValue: document.getElementById('pipeline-id-value'),\n",
              "                        statusValue: document.getElementById('status-value'),\n",
              "                        lastUpdate: document.getElementById('last-update'),\n",
              "                        statusMessage: document.getElementById('status-message'),\n",
              "                        pipelineSelector: document.getElementById('pipeline-selector'),\n",
              "                        resumeBtn: document.getElementById('resume-btn'),\n",
              "                        stopBtn: document.getElementById('stop-btn'),\n",
              "                        deleteBtn: document.getElementById('delete-btn'),\n",
              "                        configName: document.getElementById('config-name'),\n",
              "                        configText: document.getElementById('config-text'),\n",
              "                        cloneBtn: document.getElementById('clone-btn'),\n",
              "                        submitCloneBtn: document.getElementById('submit-clone-btn'),\n",
              "                        cancelCloneBtn: document.getElementById('cancel-clone-btn')\n",
              "                    };\n",
              "\n",
              "                    // Use fetch API with explicit CORS mode and optional auth token\n",
              "                    async function xhrRequest(url, method = 'GET', body = null) {\n",
              "                        const options = {\n",
              "                            method: method,\n",
              "                            headers: {\n",
              "                                'Content-Type': 'application/json'\n",
              "                            },\n",
              "                            mode: 'cors',\n",
              "                            credentials: 'include'  // Include cookies for Colab proxy auth\n",
              "                        };\n",
              "\n",
              "                        if (body) {\n",
              "                            options.body = JSON.stringify(body);\n",
              "                        }\n",
              "\n",
              "                        const response = await fetch(url, options);\n",
              "                        if (!response.ok) {\n",
              "                            throw new Error('HTTP ' + response.status);\n",
              "                        }\n",
              "                        return await response.json();\n",
              "                    }\n",
              "\n",
              "                    async function fetchPipelines() {\n",
              "                        try {\n",
              "                            console.log('Fetching pipelines...');\n",
              "                            const pipelines = await xhrRequest(DISPATCHER_URL + '/dispatcher/list-all-pipeline-ids');\n",
              "                            console.log('Got pipelines:', pipelines.length);\n",
              "\n",
              "                            updatePipelinesDropdown(pipelines);\n",
              "                            el.lastUpdate.textContent = new Date().toLocaleTimeString();\n",
              "\n",
              "                        } catch (error) {\n",
              "                            console.error('Failed to fetch pipelines:', error);\n",
              "                            showMessage('Connection error: ' + error.message, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    async function fetchPipelineConfig(pipelineId) {\n",
              "                        try {\n",
              "                            const data = await xhrRequest(DISPATCHER_URL + `/dispatcher/get-pipeline-config-json/${pipelineId}`);\n",
              "                            const config = data.pipeline_config_json || {};\n",
              "\n",
              "                            currentConfig = config;\n",
              "                            currentContextId = data.context_id;\n",
              "\n",
              "                            el.configName.textContent = config.pipeline_name || 'N/A';\n",
              "\n",
              "                            if (!isCloneMode) {\n",
              "                                el.configText.value = JSON.stringify(config, null, 2);\n",
              "                            }\n",
              "\n",
              "                        } catch (error) {\n",
              "                            console.error('Failed to fetch config:', error);\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function updatePipelinesDropdown(pipelines) {\n",
              "                        const selector = el.pipelineSelector;\n",
              "                        const currentSelection = selector.value;\n",
              "\n",
              "                        selector.innerHTML = '';\n",
              "\n",
              "                        if (pipelines && pipelines.length > 0) {\n",
              "                            pipelines.forEach(p => {\n",
              "                                const option = document.createElement('option');\n",
              "                                option.value = p.pipeline_id;\n",
              "                                option.textContent = `Config ID: ${p.pipeline_id} (${p.status || 'unknown'})`;\n",
              "                                selector.appendChild(option);\n",
              "                            });\n",
              "\n",
              "                            if (currentSelection && pipelines.some(p => p.pipeline_id == currentSelection)) {\n",
              "                                selector.value = currentSelection;\n",
              "                                currentPipelineId = currentSelection;\n",
              "                            } else {\n",
              "                                selector.value = pipelines[0].pipeline_id;\n",
              "                                currentPipelineId = pipelines[0].pipeline_id;\n",
              "                                fetchPipelineConfig(currentPipelineId);\n",
              "                            }\n",
              "\n",
              "                            // Update status display\n",
              "                            const currentPipeline = pipelines.find(p => p.pipeline_id == currentPipelineId);\n",
              "                            if (currentPipeline) {\n",
              "                                el.pipelineIdValue.textContent = currentPipeline.pipeline_id;\n",
              "                                el.statusValue.textContent = currentPipeline.status || 'unknown';\n",
              "\n",
              "                                const isCompleted = currentPipeline.status?.toLowerCase() === 'completed';\n",
              "                                el.resumeBtn.disabled = isCompleted;\n",
              "                                el.stopBtn.disabled = isCompleted;\n",
              "                                el.deleteBtn.disabled = isCompleted;\n",
              "                                el.cloneBtn.disabled = isCompleted || !currentContextId;\n",
              "                            }\n",
              "                        } else {\n",
              "                            selector.innerHTML = '<option value=\"\">No pipelines found</option>';\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function showMessage(message, type) {\n",
              "                        el.statusMessage.className = 'status-message msg-' + type;\n",
              "                        el.statusMessage.textContent = message;\n",
              "                        el.statusMessage.style.display = 'block';\n",
              "                        setTimeout(() => el.statusMessage.style.display = 'none', 5000);\n",
              "                    }\n",
              "\n",
              "                    async function handleAction(action) {\n",
              "                        if (!currentPipelineId) {\n",
              "                            showMessage('No pipeline selected', 'error');\n",
              "                            return;\n",
              "                        }\n",
              "\n",
              "                        try {\n",
              "                            const endpoint = DISPATCHER_URL + `/dispatcher/${action}-pipeline`;\n",
              "                            const result = await xhrRequest(endpoint, 'POST', { pipeline_id: currentPipelineId });\n",
              "\n",
              "                            showMessage(`‚úì ${action} completed for pipeline ${currentPipelineId}`, 'success');\n",
              "\n",
              "                            // Refresh after a short delay\n",
              "                            setTimeout(async () => {\n",
              "                                await fetchPipelines();\n",
              "                            }, 500);\n",
              "\n",
              "                        } catch (error) {\n",
              "                            showMessage(`Error: ${error.message}`, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function enableCloneMode() {\n",
              "                        isCloneMode = true;\n",
              "                        el.configText.readOnly = false;\n",
              "                        el.submitCloneBtn.disabled = false;\n",
              "                        el.cancelCloneBtn.disabled = false;\n",
              "                        el.cloneBtn.disabled = true;\n",
              "                        showMessage('Edit config and click Submit to clone', 'info');\n",
              "                    }\n",
              "\n",
              "                    function disableCloneMode() {\n",
              "                        isCloneMode = false;\n",
              "                        el.configText.readOnly = true;\n",
              "                        el.configText.value = JSON.stringify(currentConfig || {}, null, 2);\n",
              "                        el.submitCloneBtn.disabled = true;\n",
              "                        el.cancelCloneBtn.disabled = true;\n",
              "                        el.cloneBtn.disabled = false;\n",
              "                    }\n",
              "\n",
              "                    async function handleClone() {\n",
              "                        if (!currentPipelineId) {\n",
              "                            showMessage('No pipeline selected', 'error');\n",
              "                            return;\n",
              "                        }\n",
              "\n",
              "                        try {\n",
              "                            // Parse edited config\n",
              "                            let editedConfig;\n",
              "                            try {\n",
              "                                editedConfig = JSON.parse(el.configText.value);\n",
              "                            } catch (e) {\n",
              "                                showMessage('Invalid JSON: ' + e.message, 'error');\n",
              "                                return;\n",
              "                            }\n",
              "\n",
              "                            // Validate required fields\n",
              "                            if (!editedConfig.pipeline_type) {\n",
              "                                showMessage('config_json must include pipeline_type', 'error');\n",
              "                                return;\n",
              "                            }\n",
              "\n",
              "                            // Send clone request\n",
              "                            const cloneRequest = {\n",
              "                                parent_pipeline_id: currentPipelineId,\n",
              "                                config_json: editedConfig\n",
              "                            };\n",
              "\n",
              "                            const result = await xhrRequest(\n",
              "                                DISPATCHER_URL + '/dispatcher/clone-pipeline',\n",
              "                                'POST',\n",
              "                                cloneRequest\n",
              "                            );\n",
              "\n",
              "                            showMessage(`‚úì Cloned from Config ID ${currentPipelineId} successfully!`, 'success');\n",
              "                            disableCloneMode();\n",
              "\n",
              "                            // Refresh after delay\n",
              "                            setTimeout(async () => {\n",
              "                                await fetchPipelines();\n",
              "                            }, 1000);\n",
              "\n",
              "                        } catch (error) {\n",
              "                            showMessage(`Error cloning: ${error.message}`, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    // Event listeners\n",
              "                    el.pipelineSelector.addEventListener('change', (e) => {\n",
              "                        if (e.target.value) {\n",
              "                            currentPipelineId = parseInt(e.target.value);\n",
              "                            fetchPipelineConfig(currentPipelineId);\n",
              "                        }\n",
              "                    });\n",
              "\n",
              "                    el.resumeBtn.addEventListener('click', () => handleAction('resume'));\n",
              "                    el.stopBtn.addEventListener('click', () => handleAction('stop'));\n",
              "                    el.deleteBtn.addEventListener('click', () => handleAction('delete'));\n",
              "\n",
              "                    el.cloneBtn.addEventListener('click', enableCloneMode);\n",
              "                    el.submitCloneBtn.addEventListener('click', handleClone);\n",
              "                    el.cancelCloneBtn.addEventListener('click', () => {\n",
              "                        disableCloneMode();\n",
              "                        showMessage('Cancelled clone', 'info');\n",
              "                    });\n",
              "\n",
              "                    // Initial fetch\n",
              "                    console.log('UI initialized, fetching initial data...');\n",
              "                    setTimeout(async () => {\n",
              "                        await fetchPipelines();\n",
              "\n",
              "                        // Start polling - use HTTP fetch (works even when kernel is busy)\n",
              "                        pollingInterval = setInterval(async () => {\n",
              "                            await fetchPipelines();\n",
              "                        }, 3000.0);\n",
              "                        console.log('Polling started: every 3.0s');\n",
              "                    }, 1000);\n",
              "\n",
              "                    // Cleanup on unload\n",
              "                    window.addEventListener('beforeunload', () => {\n",
              "                        if (pollingInterval) clearInterval(pollingInterval);\n",
              "                    });\n",
              "                })();\n",
              "            </script>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Preprocessing RAG Sources ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7d8c57ab7dd0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_dceb0\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_dceb0_level0_col0\" class=\"col_heading level0 col0\" >RAG Source ID</th>\n",
              "      <th id=\"T_dceb0_level0_col1\" class=\"col_heading level0 col1\" >Status</th>\n",
              "      <th id=\"T_dceb0_level0_col2\" class=\"col_heading level0 col2\" >Duration</th>\n",
              "      <th id=\"T_dceb0_level0_col3\" class=\"col_heading level0 col3\" >Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_dceb0_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_dceb0_row0_col1\" class=\"data row0 col1\" >Complete</td>\n",
              "      <td id=\"T_dceb0_row0_col2\" class=\"data row0 col2\" >45.4s</td>\n",
              "      <td id=\"T_dceb0_row0_col3\" class=\"data row0 col3\" >FAISS, GPU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_dceb0_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_dceb0_row1_col1\" class=\"data row1 col1\" >Complete</td>\n",
              "      <td id=\"T_dceb0_row1_col2\" class=\"data row1 col2\" >45.5s</td>\n",
              "      <td id=\"T_dceb0_row1_col3\" class=\"data row1 col3\" >FAISS, GPU</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on public URL: https://46c1c92496e259fd4b.gradio.live\n",
            "* Trackio project initialized: exp1-fiqa-rag-colab\n",
            "* Trackio metrics logged to: /root/.cache/huggingface/trackio\n",
            "* View dashboard by running in your terminal:\n",
            "\u001b[1m\u001b[38;5;208mtrackio show --project \"exp1-fiqa-rag-colab\"\u001b[0m\n",
            "* or by running in Python: trackio.show(project=\"exp1-fiqa-rag-colab\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* GPU detected, enabling automatic GPU metrics logging\n",
            "* Created new run: dainty-sunset-0\n",
            "* Run finished. Uploading logs to Trackio (please wait...)\n",
            "* GPU detected, enabling automatic GPU metrics logging\n",
            "* Created new run: Pipeline 1_1\n",
            "* Run finished. Uploading logs to Trackio (please wait...)\n",
            "* GPU detected, enabling automatic GPU metrics logging\n",
            "* Created new run: Pipeline 2_2\n",
            "* Run finished. Uploading logs to Trackio (please wait...)\n",
            "* GPU detected, enabling automatic GPU metrics logging\n",
            "* Created new run: Pipeline 3_3\n",
            "* Run finished. Uploading logs to Trackio (please wait...)\n",
            "* GPU detected, enabling automatic GPU metrics logging\n",
            "* Created new run: Pipeline 4_4\n",
            "\n",
            "=== Multi-Config Experiment Progress ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7d8b4e1f0bf0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_e1e22\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_e1e22_level0_col0\" class=\"col_heading level0 col0\" >Run ID</th>\n",
              "      <th id=\"T_e1e22_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
              "      <th id=\"T_e1e22_level0_col2\" class=\"col_heading level0 col2\" >Status</th>\n",
              "      <th id=\"T_e1e22_level0_col3\" class=\"col_heading level0 col3\" >Progress</th>\n",
              "      <th id=\"T_e1e22_level0_col4\" class=\"col_heading level0 col4\" >Conf. Interval</th>\n",
              "      <th id=\"T_e1e22_level0_col5\" class=\"col_heading level0 col5\" >search_type</th>\n",
              "      <th id=\"T_e1e22_level0_col6\" class=\"col_heading level0 col6\" >rag_k</th>\n",
              "      <th id=\"T_e1e22_level0_col7\" class=\"col_heading level0 col7\" >top_n</th>\n",
              "      <th id=\"T_e1e22_level0_col8\" class=\"col_heading level0 col8\" >chunk_size</th>\n",
              "      <th id=\"T_e1e22_level0_col9\" class=\"col_heading level0 col9\" >chunk_overlap</th>\n",
              "      <th id=\"T_e1e22_level0_col10\" class=\"col_heading level0 col10\" >sampling_params</th>\n",
              "      <th id=\"T_e1e22_level0_col11\" class=\"col_heading level0 col11\" >model_config</th>\n",
              "      <th id=\"T_e1e22_level0_col12\" class=\"col_heading level0 col12\" >Precision</th>\n",
              "      <th id=\"T_e1e22_level0_col13\" class=\"col_heading level0 col13\" >Recall</th>\n",
              "      <th id=\"T_e1e22_level0_col14\" class=\"col_heading level0 col14\" >F1 Score</th>\n",
              "      <th id=\"T_e1e22_level0_col15\" class=\"col_heading level0 col15\" >NDCG@5</th>\n",
              "      <th id=\"T_e1e22_level0_col16\" class=\"col_heading level0 col16\" >MRR</th>\n",
              "      <th id=\"T_e1e22_level0_col17\" class=\"col_heading level0 col17\" >Throughput</th>\n",
              "      <th id=\"T_e1e22_level0_col18\" class=\"col_heading level0 col18\" >Total</th>\n",
              "      <th id=\"T_e1e22_level0_col19\" class=\"col_heading level0 col19\" >Samples Processed</th>\n",
              "      <th id=\"T_e1e22_level0_col20\" class=\"col_heading level0 col20\" >Processing Time</th>\n",
              "      <th id=\"T_e1e22_level0_col21\" class=\"col_heading level0 col21\" >Samples Per Second</th>\n",
              "      <th id=\"T_e1e22_level0_col22\" class=\"col_heading level0 col22\" >model_name</th>\n",
              "      <th id=\"T_e1e22_level0_col23\" class=\"col_heading level0 col23\" >run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_e1e22_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_e1e22_row0_col1\" class=\"data row0 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row0_col2\" class=\"data row0 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_e1e22_row0_col3\" class=\"data row0 col3\" >4/4</td>\n",
              "      <td id=\"T_e1e22_row0_col4\" class=\"data row0 col4\" >0.000</td>\n",
              "      <td id=\"T_e1e22_row0_col5\" class=\"data row0 col5\" >similarity</td>\n",
              "      <td id=\"T_e1e22_row0_col6\" class=\"data row0 col6\" >8.00</td>\n",
              "      <td id=\"T_e1e22_row0_col7\" class=\"data row0 col7\" >2.00</td>\n",
              "      <td id=\"T_e1e22_row0_col8\" class=\"data row0 col8\" >256.00</td>\n",
              "      <td id=\"T_e1e22_row0_col9\" class=\"data row0 col9\" >32.00</td>\n",
              "      <td id=\"T_e1e22_row0_col10\" class=\"data row0 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_e1e22_row0_col11\" class=\"data row0 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 3000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_e1e22_row0_col12\" class=\"data row0 col12\" >43.95% [43.95%, 43.95%]</td>\n",
              "      <td id=\"T_e1e22_row0_col13\" class=\"data row0 col13\" >88.33% [88.33%, 88.33%]</td>\n",
              "      <td id=\"T_e1e22_row0_col14\" class=\"data row0 col14\" >53.26% [53.26%, 53.26%]</td>\n",
              "      <td id=\"T_e1e22_row0_col15\" class=\"data row0 col15\" >20.07% [20.07%, 20.07%]</td>\n",
              "      <td id=\"T_e1e22_row0_col16\" class=\"data row0 col16\" >68.06% [68.06%, 68.06%]</td>\n",
              "      <td id=\"T_e1e22_row0_col17\" class=\"data row0 col17\" >0.0/s</td>\n",
              "      <td id=\"T_e1e22_row0_col18\" class=\"data row0 col18\" >6</td>\n",
              "      <td id=\"T_e1e22_row0_col19\" class=\"data row0 col19\" >6</td>\n",
              "      <td id=\"T_e1e22_row0_col20\" class=\"data row0 col20\" >604.04 seconds</td>\n",
              "      <td id=\"T_e1e22_row0_col21\" class=\"data row0 col21\" >0.01</td>\n",
              "      <td id=\"T_e1e22_row0_col22\" class=\"data row0 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row0_col23\" class=\"data row0 col23\" >1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_e1e22_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_e1e22_row1_col1\" class=\"data row1 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row1_col2\" class=\"data row1 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_e1e22_row1_col3\" class=\"data row1 col3\" >4/4</td>\n",
              "      <td id=\"T_e1e22_row1_col4\" class=\"data row1 col4\" >0.000</td>\n",
              "      <td id=\"T_e1e22_row1_col5\" class=\"data row1 col5\" >similarity</td>\n",
              "      <td id=\"T_e1e22_row1_col6\" class=\"data row1 col6\" >8.00</td>\n",
              "      <td id=\"T_e1e22_row1_col7\" class=\"data row1 col7\" >5.00</td>\n",
              "      <td id=\"T_e1e22_row1_col8\" class=\"data row1 col8\" >256.00</td>\n",
              "      <td id=\"T_e1e22_row1_col9\" class=\"data row1 col9\" >32.00</td>\n",
              "      <td id=\"T_e1e22_row1_col10\" class=\"data row1 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_e1e22_row1_col11\" class=\"data row1 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 3000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_e1e22_row1_col12\" class=\"data row1 col12\" >43.95% [43.95%, 43.95%]</td>\n",
              "      <td id=\"T_e1e22_row1_col13\" class=\"data row1 col13\" >88.33% [88.33%, 88.33%]</td>\n",
              "      <td id=\"T_e1e22_row1_col14\" class=\"data row1 col14\" >53.26% [53.26%, 53.26%]</td>\n",
              "      <td id=\"T_e1e22_row1_col15\" class=\"data row1 col15\" >20.07% [20.07%, 20.07%]</td>\n",
              "      <td id=\"T_e1e22_row1_col16\" class=\"data row1 col16\" >68.06% [68.06%, 68.06%]</td>\n",
              "      <td id=\"T_e1e22_row1_col17\" class=\"data row1 col17\" >0.1/s</td>\n",
              "      <td id=\"T_e1e22_row1_col18\" class=\"data row1 col18\" >6</td>\n",
              "      <td id=\"T_e1e22_row1_col19\" class=\"data row1 col19\" >6</td>\n",
              "      <td id=\"T_e1e22_row1_col20\" class=\"data row1 col20\" >127.98 seconds</td>\n",
              "      <td id=\"T_e1e22_row1_col21\" class=\"data row1 col21\" >0.05</td>\n",
              "      <td id=\"T_e1e22_row1_col22\" class=\"data row1 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row1_col23\" class=\"data row1 col23\" >2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_e1e22_row2_col0\" class=\"data row2 col0\" >3</td>\n",
              "      <td id=\"T_e1e22_row2_col1\" class=\"data row2 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row2_col2\" class=\"data row2 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_e1e22_row2_col3\" class=\"data row2 col3\" >4/4</td>\n",
              "      <td id=\"T_e1e22_row2_col4\" class=\"data row2 col4\" >0.000</td>\n",
              "      <td id=\"T_e1e22_row2_col5\" class=\"data row2 col5\" >similarity</td>\n",
              "      <td id=\"T_e1e22_row2_col6\" class=\"data row2 col6\" >8.00</td>\n",
              "      <td id=\"T_e1e22_row2_col7\" class=\"data row2 col7\" >2.00</td>\n",
              "      <td id=\"T_e1e22_row2_col8\" class=\"data row2 col8\" >128.00</td>\n",
              "      <td id=\"T_e1e22_row2_col9\" class=\"data row2 col9\" >32.00</td>\n",
              "      <td id=\"T_e1e22_row2_col10\" class=\"data row2 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_e1e22_row2_col11\" class=\"data row2 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 3000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_e1e22_row2_col12\" class=\"data row2 col12\" >45.83% [45.83%, 45.83%]</td>\n",
              "      <td id=\"T_e1e22_row2_col13\" class=\"data row2 col13\" >80.00% [80.00%, 80.00%]</td>\n",
              "      <td id=\"T_e1e22_row2_col14\" class=\"data row2 col14\" >53.31% [53.31%, 53.31%]</td>\n",
              "      <td id=\"T_e1e22_row2_col15\" class=\"data row2 col15\" >20.06% [20.06%, 20.06%]</td>\n",
              "      <td id=\"T_e1e22_row2_col16\" class=\"data row2 col16\" >61.11% [61.11%, 61.11%]</td>\n",
              "      <td id=\"T_e1e22_row2_col17\" class=\"data row2 col17\" >0.1/s</td>\n",
              "      <td id=\"T_e1e22_row2_col18\" class=\"data row2 col18\" >6</td>\n",
              "      <td id=\"T_e1e22_row2_col19\" class=\"data row2 col19\" >6</td>\n",
              "      <td id=\"T_e1e22_row2_col20\" class=\"data row2 col20\" >120.68 seconds</td>\n",
              "      <td id=\"T_e1e22_row2_col21\" class=\"data row2 col21\" >0.05</td>\n",
              "      <td id=\"T_e1e22_row2_col22\" class=\"data row2 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row2_col23\" class=\"data row2 col23\" >3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_e1e22_row3_col0\" class=\"data row3 col0\" >4</td>\n",
              "      <td id=\"T_e1e22_row3_col1\" class=\"data row3 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row3_col2\" class=\"data row3 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_e1e22_row3_col3\" class=\"data row3 col3\" >4/4</td>\n",
              "      <td id=\"T_e1e22_row3_col4\" class=\"data row3 col4\" >0.000</td>\n",
              "      <td id=\"T_e1e22_row3_col5\" class=\"data row3 col5\" >similarity</td>\n",
              "      <td id=\"T_e1e22_row3_col6\" class=\"data row3 col6\" >8.00</td>\n",
              "      <td id=\"T_e1e22_row3_col7\" class=\"data row3 col7\" >5.00</td>\n",
              "      <td id=\"T_e1e22_row3_col8\" class=\"data row3 col8\" >128.00</td>\n",
              "      <td id=\"T_e1e22_row3_col9\" class=\"data row3 col9\" >32.00</td>\n",
              "      <td id=\"T_e1e22_row3_col10\" class=\"data row3 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_e1e22_row3_col11\" class=\"data row3 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 3000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_e1e22_row3_col12\" class=\"data row3 col12\" >45.83% [45.83%, 45.83%]</td>\n",
              "      <td id=\"T_e1e22_row3_col13\" class=\"data row3 col13\" >80.00% [80.00%, 80.00%]</td>\n",
              "      <td id=\"T_e1e22_row3_col14\" class=\"data row3 col14\" >53.31% [53.31%, 53.31%]</td>\n",
              "      <td id=\"T_e1e22_row3_col15\" class=\"data row3 col15\" >20.06% [20.06%, 20.06%]</td>\n",
              "      <td id=\"T_e1e22_row3_col16\" class=\"data row3 col16\" >61.11% [61.11%, 61.11%]</td>\n",
              "      <td id=\"T_e1e22_row3_col17\" class=\"data row3 col17\" >0.1/s</td>\n",
              "      <td id=\"T_e1e22_row3_col18\" class=\"data row3 col18\" >6</td>\n",
              "      <td id=\"T_e1e22_row3_col19\" class=\"data row3 col19\" >6</td>\n",
              "      <td id=\"T_e1e22_row3_col20\" class=\"data row3 col20\" >101.20 seconds</td>\n",
              "      <td id=\"T_e1e22_row3_col21\" class=\"data row3 col21\" >0.06</td>\n",
              "      <td id=\"T_e1e22_row3_col22\" class=\"data row3 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_e1e22_row3_col23\" class=\"data row3 col23\" >4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n",
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Run finished. Uploading logs to Trackio (please wait...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trackio/run.py:223: UserWarning: Reserved keys renamed: ['step'] ‚Üí '__{key}'\n",
            "  warnings.warn(f\"Reserved keys renamed: {renamed_keys} ‚Üí '__{{key}}'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Run finished. Uploading logs to Trackio (please wait...)\n",
            "* Run finished. Uploading logs to Trackio (please wait...)\n",
            "* Run finished. Uploading logs to Trackio (please wait...)\n"
          ]
        }
      ],
      "source": [
        "# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\n",
        "# NB: If your machine has more than 1 GPU, set num_actors to that number\n",
        "results = experiment.run_evals(\n",
        "    config_group=config_group,\n",
        "    dataset=fiqa_dataset,\n",
        "    num_actors=1,\n",
        "    num_shards=4,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Trackio Dashboard"
      ],
      "metadata": {
        "id": "suDZAZmHGSWg"
      },
      "id": "suDZAZmHGSWg"
    },
    {
      "cell_type": "code",
      "source": [
        "output.serve_kernel_port_as_iframe(7860)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "HRZVTxuZGPvQ",
        "outputId": "31e8cbbc-9a29-4c8e-80c9-561f1f797810"
      },
      "id": "HRZVTxuZGPvQ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(7860, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "656ce33b",
      "metadata": {
        "id": "656ce33b"
      },
      "source": [
        "### View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "127e7b4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "127e7b4a",
        "outputId": "40d70fdb-edf2-4b50-d458-effae1ce775a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   run_id                  model_name search_type  rag_k  top_n  chunk_size  \\\n",
              "0       1  Qwen/Qwen2.5-0.5B-Instruct  similarity      8      2         256   \n",
              "1       2  Qwen/Qwen2.5-0.5B-Instruct  similarity      8      5         256   \n",
              "2       3  Qwen/Qwen2.5-0.5B-Instruct  similarity      8      2         128   \n",
              "3       4  Qwen/Qwen2.5-0.5B-Instruct  similarity      8      5         128   \n",
              "\n",
              "   chunk_overlap                                    sampling_params  \\\n",
              "0             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "1             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "2             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "3             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "\n",
              "                                        model_config  Samples Processed  \\\n",
              "0  {'dtype': 'half', 'gpu_memory_utilization': 0....                  6   \n",
              "1  {'dtype': 'half', 'gpu_memory_utilization': 0....                  6   \n",
              "2  {'dtype': 'half', 'gpu_memory_utilization': 0....                  6   \n",
              "3  {'dtype': 'half', 'gpu_memory_utilization': 0....                  6   \n",
              "\n",
              "  Processing Time Samples Per Second  Total  Precision    Recall  F1 Score  \\\n",
              "0  604.04 seconds               0.01      6   0.439484  0.883333  0.532576   \n",
              "1  127.98 seconds               0.05      6   0.439484  0.883333  0.532576   \n",
              "2  120.68 seconds               0.05      6   0.458333  0.800000  0.533069   \n",
              "3  101.20 seconds               0.06      6   0.458333  0.800000  0.533069   \n",
              "\n",
              "     NDCG@5       MRR  \n",
              "0  0.200650  0.680556  \n",
              "1  0.200650  0.680556  \n",
              "2  0.200601  0.611111  \n",
              "3  0.200601  0.611111  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7762c38-fe12-4ab7-b597-e6b2b0dc29da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run_id</th>\n",
              "      <th>model_name</th>\n",
              "      <th>search_type</th>\n",
              "      <th>rag_k</th>\n",
              "      <th>top_n</th>\n",
              "      <th>chunk_size</th>\n",
              "      <th>chunk_overlap</th>\n",
              "      <th>sampling_params</th>\n",
              "      <th>model_config</th>\n",
              "      <th>Samples Processed</th>\n",
              "      <th>Processing Time</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Total</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>NDCG@5</th>\n",
              "      <th>MRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>256</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>6</td>\n",
              "      <td>604.04 seconds</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6</td>\n",
              "      <td>0.439484</td>\n",
              "      <td>0.883333</td>\n",
              "      <td>0.532576</td>\n",
              "      <td>0.200650</td>\n",
              "      <td>0.680556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>256</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>6</td>\n",
              "      <td>127.98 seconds</td>\n",
              "      <td>0.05</td>\n",
              "      <td>6</td>\n",
              "      <td>0.439484</td>\n",
              "      <td>0.883333</td>\n",
              "      <td>0.532576</td>\n",
              "      <td>0.200650</td>\n",
              "      <td>0.680556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>6</td>\n",
              "      <td>120.68 seconds</td>\n",
              "      <td>0.05</td>\n",
              "      <td>6</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.533069</td>\n",
              "      <td>0.200601</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>6</td>\n",
              "      <td>101.20 seconds</td>\n",
              "      <td>0.06</td>\n",
              "      <td>6</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.533069</td>\n",
              "      <td>0.200601</td>\n",
              "      <td>0.611111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7762c38-fe12-4ab7-b597-e6b2b0dc29da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7762c38-fe12-4ab7-b597-e6b2b0dc29da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7762c38-fe12-4ab7-b597-e6b2b0dc29da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_2e7d3056-3d67-4d82-928f-25eefc9a1591\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2e7d3056-3d67-4d82-928f-25eefc9a1591 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"run_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Qwen/Qwen2.5-0.5B-Instruct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"similarity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 8,\n        \"max\": 8,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73,\n        \"min\": 128,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_overlap\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sampling_params\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_config\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Samples Processed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Processing Time\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"127.98 seconds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Samples Per Second\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"0.01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010882594359725082,\n        \"min\": 0.439484126984127,\n        \"max\": 0.4583333333333333,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.4583333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.048112522432468836,\n        \"min\": 0.7999999999999999,\n        \"max\": 0.8833333333333333,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7999999999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002846484011156464,\n        \"min\": 0.5325757575757576,\n        \"max\": 0.5330687830687831,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5330687830687831\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NDCG@5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.828696784121853e-05,\n        \"min\": 0.20060126188701569,\n        \"max\": 0.20065025635250874,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.20060126188701569\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MRR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.040093768693724,\n        \"min\": 0.611111111111111,\n        \"max\": 0.6805555555555555,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.611111111111111\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Convert results dict to DataFrame\n",
        "results_df = pd.DataFrame([\n",
        "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
        "    for run_id, (_, metrics_dict) in results.items()\n",
        "])\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9135d951",
      "metadata": {
        "id": "9135d951"
      },
      "source": [
        "### End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "94ab038d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "94ab038d",
        "outputId": "0371318f-3ac9-4ca3-880a-c16d030f9dad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment exp1-fiqa-rag-colab ended\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import output\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML('''\n",
        "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n",
        "'''))\n",
        "\n",
        "# eval_js blocks until the Promise resolves\n",
        "output.eval_js('''\n",
        "new Promise((resolve) => {\n",
        "    document.getElementById(\"continue-btn\").onclick = () => {\n",
        "        document.getElementById(\"continue-btn\").disabled = true;\n",
        "        document.getElementById(\"continue-btn\").innerText = \"Continuing...\";\n",
        "        resolve(\"clicked\");\n",
        "    };\n",
        "})\n",
        "''')\n",
        "\n",
        "# Actually end the experiment after the button is clicked\n",
        "experiment.end()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09265e66",
      "metadata": {
        "id": "09265e66"
      },
      "source": [
        "### View RapidFire AI Log Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "05379a93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05379a93",
        "outputId": "45a6ea0f-0920-44d3-cd6d-1c05ce7efb80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Log File: /content/rapidfireai/logs/exp1-fiqa-rag-colab/rapidfire.log\n",
            "\n",
            "================================================================================\n",
            "Last 30 lines of rapidfire.log:\n",
            "================================================================================\n",
            "2026-01-13 20:20:52 | Controller | INFO | controller.py:1251 | [exp1-fiqa-rag-colab:Controller] Scheduling pipeline 4 (Pipeline 4) on actor 0 for shard 3 (1 batches)\n",
            "2026-01-13 20:20:52 | QueryProcessingActor-0 | INFO | query_actor.py:143 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Reusing existing inference engine (config hash: f895be3c)\n",
            "2026-01-13 20:20:52 | QueryProcessingActor-0 | INFO | query_actor.py:162 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Using CPU-based FAISS for retrieval (avoids GPU memory conflicts)\n",
            "2026-01-13 20:20:52 | QueryProcessingActor-0 | INFO | query_actor.py:169 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Deserializing FAISS index for this actor...\n",
            "2026-01-13 20:20:52 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2026-01-13 20:20:54 | QueryProcessingActor-0 | INFO | query_actor.py:178 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated embedding function: HuggingFaceEmbeddings\n",
            "2026-01-13 20:20:54 | QueryProcessingActor-0 | INFO | query_actor.py:187 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Created independent FAISS vector store for this actor\n",
            "2026-01-13 20:20:54 | QueryProcessingActor-0 | INFO | query_actor.py:196 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated retriever with search_type=similarity\n",
            "2026-01-13 20:20:54 | QueryProcessingActor-0 | INFO | query_actor.py:218 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated RAG spec with retriever and template\n",
            "2026-01-13 20:21:01 | Controller | INFO | controller.py:1044 | [exp1-fiqa-rag-colab:Controller] Pipeline 4 completed all 4 shards\n",
            "2026-01-13 20:21:01 | Controller | INFO | controller.py:1149 | [exp1-fiqa-rag-colab:Controller] Pipeline 4 completed shard 3 (1 batches, 7.51s)\n",
            "2026-01-13 20:21:01 | Controller | INFO | controller.py:1215 | [exp1-fiqa-rag-colab:Controller] All pipelines completed all shards!\n",
            "2026-01-13 20:21:01 | Controller | INFO | controller.py:571 | [exp1-fiqa-rag-colab:Controller] Computing final metrics for all pipelines...\n",
            "2026-01-13 20:21:01 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 1_1 in rf_tensorboard\n",
            "2026-01-13 20:21:01 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 1_1 in rf_trackio\n",
            "2026-01-13 20:21:02 | Controller | INFO | controller.py:731 | [exp1-fiqa-rag-colab:Controller] Pipeline 1 (Pipeline 1) completed successfully\n",
            "2026-01-13 20:21:02 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 2_2 in rf_tensorboard\n",
            "2026-01-13 20:21:02 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 2_2 in rf_trackio\n",
            "2026-01-13 20:21:03 | Controller | INFO | controller.py:731 | [exp1-fiqa-rag-colab:Controller] Pipeline 2 (Pipeline 2) completed successfully\n",
            "2026-01-13 20:21:03 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 3_3 in rf_tensorboard\n",
            "2026-01-13 20:21:03 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 3_3 in rf_trackio\n",
            "2026-01-13 20:21:04 | Controller | INFO | controller.py:731 | [exp1-fiqa-rag-colab:Controller] Pipeline 3 (Pipeline 3) completed successfully\n",
            "2026-01-13 20:21:04 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 4_4 in rf_tensorboard\n",
            "2026-01-13 20:21:04 | Experiment | INFO | metric_rfmetric_manager.py:137 | [exp1-fiqa-rag-colab:Experiment] Ending run: Pipeline 4_4 in rf_trackio\n",
            "2026-01-13 20:21:05 | Controller | INFO | controller.py:731 | [exp1-fiqa-rag-colab:Controller] Pipeline 4 (Pipeline 4) completed successfully\n",
            "2026-01-13 20:45:35 | ExperimentUtils | INFO | experiment_utils.py:180 | [exp1-fiqa-rag-colab:ExperimentUtils] Reset experiment states - marked ongoing pipelines, contexts, and tasks as failed\n",
            "2026-01-13 20:45:35 | ExperimentUtils | INFO | experiment_utils.py:188 | [exp1-fiqa-rag-colab:ExperimentUtils] Experiment marked as cancelled. Ongoing pipelines, contexts, and tasks have been marked as failed.\n",
            "2026-01-13 20:45:35 | ExperimentUtils | INFO | experiment_utils.py:154 | [exp1-fiqa-rag-colab:ExperimentUtils] Experiment exp1-fiqa-rag-colab ended\n",
            "2026-01-13 20:45:36 | Experiment | INFO | experiment.py:564 | [exp1-fiqa-rag-colab:Experiment] All actors shut down\n",
            "2026-01-13 20:45:36 | Experiment | INFO | experiment.py:565 | [exp1-fiqa-rag-colab:Experiment] Dispatcher will automatically shut down (daemon thread)\n"
          ]
        }
      ],
      "source": [
        "# Get the experiment-specific log file\n",
        "log_file = experiment.get_log_file_path()\n",
        "\n",
        "print(f\"üìÑ Log File: {log_file}\")\n",
        "print()\n",
        "\n",
        "if log_file.exists():\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Last 30 lines of {log_file.name}:\")\n",
        "    print(\"=\" * 80)\n",
        "    with open(log_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-30:]:\n",
        "            print(line.rstrip())\n",
        "else:\n",
        "    print(f\"‚ùå Log file not found: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e420d4",
      "metadata": {
        "id": "20e420d4"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "We built a simple Financial Q&A RAG pipeline and compared **4 retrieval configurations** (chunking √ó reranking) using standard retrieval metrics.\n",
        "\n",
        "Optional ideas to explore later:\n",
        "- Increase `sample_fraction` (or run locally) for more reliable results.\n",
        "- Try additional retrieval knobs (e.g., embedding model, `k`, chunk overlap) and re-run the same evaluation loop.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e20789803bc435a85b9c94b8f335453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4090404abb644b7497a6d15afd3e46b5",
              "IPY_MODEL_3ea846268d004d7184bb911db67a2708",
              "IPY_MODEL_872cdc12775c463ba2ba3b3864e42c8d"
            ],
            "layout": "IPY_MODEL_92594a327bb14b4da0e7b6f8d6c30576"
          }
        },
        "4090404abb644b7497a6d15afd3e46b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dd5fcdf2c2c485a86626997081e62be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_489946a99b2d45828a3c7b9b4def319e",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá"
          }
        },
        "3ea846268d004d7184bb911db67a2708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73fc6e1fb17845998b05828786f537f3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2c37210bbd945cebbc24dde8a2eb151",
            "value": 1
          }
        },
        "872cdc12775c463ba2ba3b3864e42c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a1fdc3feeaa4088a8df21e9735c820e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_618038b1576d4a7e84781a4c0e8ba017",
            "value": "‚Äá6648/0‚Äá[00:00&lt;00:00,‚Äá139268.95‚Äáexamples/s]"
          }
        },
        "92594a327bb14b4da0e7b6f8d6c30576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd5fcdf2c2c485a86626997081e62be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489946a99b2d45828a3c7b9b4def319e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73fc6e1fb17845998b05828786f537f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f2c37210bbd945cebbc24dde8a2eb151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a1fdc3feeaa4088a8df21e9735c820e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618038b1576d4a7e84781a4c0e8ba017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}