{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06d0c6d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
    "<br/>\n",
    "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### RapidFire AI RAG/Context Engineering Tutorial Use Case: Financial Opinion Q&A Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
    "import re, json\n",
    "from typing import List as listtype, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645aa86f",
   "metadata": {},
   "source": [
    "### Install Postgres Vector Store\n",
    "\n",
    "```python\n",
    "!pip install -qU langchain-postgres\n",
    "```\n",
    "\n",
    "Then spin up the postgres server\n",
    "```bash\n",
    "docker run --name pgvector-container -e POSTGRES_USER=langchain -e POSTGRES_PASSWORD=langchain -e POSTGRES_DB=langchain -p 6024:5432 -d pgvector/pgvector:pg16\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16327b",
   "metadata": {},
   "source": [
    "### Load Dataset and Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee571098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset directory is now in tutorial_notebooks/evals/datasets\n",
    "dataset_dir = Path(\"datasets\")\n",
    "\n",
    "fiqa_dataset = load_dataset(\"json\", data_files=str(dataset_dir / \"fiqa\" / \"queries.jsonl\"), split=\"train\").select(range(500))\n",
    "fiqa_dataset = fiqa_dataset.rename_columns({\"text\": \"query\", \"_id\": \"query_id\"})\n",
    "qrels = pd.read_csv(str(dataset_dir / \"fiqa\" / \"qrels.tsv\"), sep=\"\\t\")\n",
    "qrels = qrels.rename(\n",
    "    columns={\"query-id\": \"query_id\", \"corpus-id\": \"corpus_id\", \"score\": \"relevance\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28399289",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70816920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An experiment with the same name already exists. Created a new experiment 'exp1-fiqa-rag_39' with Experiment ID: 51 at /home/ubuntu/rapidfireai/rapidfire_experiments/exp1-fiqa-rag_39\n",
      "Created directory: /home/ubuntu/rapidfireai/logs/exp1-fiqa-rag_39\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(experiment_name=\"exp1-fiqa-rag\", mode=\"evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a21ee",
   "metadata": {},
   "source": [
    "### Define Partial Multi-Config Knobs for LangChain part of RAG Pipeline using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "# from langchain_postgres import PGVector\n",
    "\n",
    "# # Per-Actor batch size for hardware efficiency\n",
    "# batch_size = 128\n",
    "# # Connection to the hosted Postgres Vector DB\n",
    "# connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "    \n",
    "# # 2 chunk sizes x 2 reranking top-n = 4 combinations in total\n",
    "# rag_gpu = RFLangChainRagSpec(\n",
    "#     document_loader=DirectoryLoader(\n",
    "#         path=str(dataset_dir / \"fiqa\"),\n",
    "#         glob=\"corpus.jsonl\",\n",
    "#         loader_cls=JSONLoader,\n",
    "#         loader_kwargs={\n",
    "#             \"jq_schema\": \".\",\n",
    "#             \"content_key\": \"text\",\n",
    "#             \"metadata_func\": lambda record, metadata: {\n",
    "#                 \"corpus_id\": int(record.get(\"_id\"))\n",
    "#             },  # store the document id\n",
    "#             \"json_lines\": True,\n",
    "#             \"text_content\": False,\n",
    "#         },\n",
    "#         sample_seed=42,\n",
    "#     ),\n",
    "#     # 2 chunking strategies with different chunk_sizes\n",
    "#     text_splitter=List([\n",
    "#         RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#             encoding_name=\"gpt2\", chunk_size=64, chunk_overlap=32\n",
    "#         ),\n",
    "#         RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#             encoding_name=\"gpt2\", chunk_size=256, chunk_overlap=32\n",
    "#         )\n",
    "#     ]),\n",
    "#     embedding_cfg={\n",
    "#         \"class\": HuggingFaceEmbeddings,\n",
    "#         \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#         \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "#         \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size}\n",
    "#     },\n",
    "#     vector_store_cfg={\n",
    "#         \"type\": \"pgvector\",\n",
    "#         \"connection\": connection,\n",
    "#         # \"pre_delete_collection\": True, # Deletes the collection if already exists\n",
    "#         \"batch_size\": 1024, # Documents are embedded in batches. Different from generation batch size. Defaults to 128 if not set.\n",
    "#     },\n",
    "#     search_cfg={\n",
    "#         \"type\": \"similarity\",\n",
    "#         \"k\": 15\n",
    "#     },\n",
    "#     # 2 reranking strategies with different top-n values\n",
    "#     reranker_cfg={\n",
    "#         \"class\": CrossEncoderReranker,\n",
    "#         \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "#         \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "#         \"top_n\": List([2, 5]),\n",
    "#     },\n",
    "#     enable_gpu_search=True,  # GPU-based exact search instead of ANN index\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34cdba55-2fb5-4bd0-b722-18a5194f26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# Per-Actor batch size for hardware efficiency\n",
    "batch_size = 128\n",
    "# Connection to the hosted Postgres Vector DB\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "    \n",
    "# 2 chunk sizes x 2 reranking top-n = 4 combinations in total\n",
    "rag_gpu = RFLangChainRagSpec(\n",
    "    document_loader=None,\n",
    "    text_splitter=None,\n",
    "    embedding_cfg=None,\n",
    "    vector_store_cfg={\n",
    "        \"type\": \"pgvector\",\n",
    "        \"connection\": connection,\n",
    "        \"embedding_cfg\": {\n",
    "            \"class\": HuggingFaceEmbeddings,\n",
    "            \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "            \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size}\n",
    "        },\n",
    "        # 2 chunking strategies with different chunk_sizes\n",
    "        \"collection_name\": List([\"fiqa_chunk64\", \"fiqa_chunk256\"]) # prebuilt vector stores using the same embedding\n",
    "    },\n",
    "    search_cfg={\n",
    "        \"type\": \"similarity\",\n",
    "        \"k\": 15\n",
    "    },\n",
    "    # 2 reranking strategies with different top-n values\n",
    "    reranker_cfg={\n",
    "        \"class\": CrossEncoderReranker,\n",
    "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "        \"top_n\": List([2, 5]),\n",
    "    },\n",
    "    enable_gpu_search=True,  # GPU-based exact search instead of ANN index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fb0a8",
   "metadata": {},
   "source": [
    "### Define Data Processing and Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc17276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_preprocess_fn(\n",
    "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
    ") -> Dict[str, listtype]:\n",
    "    \"\"\"Function to prepare the final inputs given to the generator model\"\"\"\n",
    "\n",
    "    INSTRUCTIONS = \"Utilize your financial knowledge, give your answer or opinion to the input question or subject matter.\"\n",
    "\n",
    "    # Perform batched retrieval over all queries; returns a list of lists of k documents per query\n",
    "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
    "\n",
    "    # Extract the retrieved document ids from the context\n",
    "    retrieved_documents = [\n",
    "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
    "    ]\n",
    "\n",
    "    # Serialize the retrieved documents into a single string per query using the default template\n",
    "    serialized_context = rag.serialize_documents(all_context)\n",
    "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
    "\n",
    "    # Each batch to contain conversational prompt, retrieved documents, and original 'query_id', 'query', 'metadata'\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Here is some relevant context:\\n{context}. \\nNow answer the following question using the context provided earlier:\\n{question}\",\n",
    "                },\n",
    "            ]\n",
    "            for question, context in zip(batch[\"query\"], serialized_context)\n",
    "        ],\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
    "    \"\"\"Function to postprocess outputs produced by generator model\"\"\"\n",
    "    # Get ground truth documents for each query; can be done in preprocess_fn too but done here for clarity\n",
    "    batch[\"ground_truth_documents\"] = [\n",
    "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
    "        for query_id in batch[\"query_id\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Define Custom Eval Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=5):\n",
    "    \"\"\"Utility function to compute NDCG@k\"\"\"\n",
    "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    # IDCG: perfect ranking limited by min(k, len(expected_docs))\n",
    "    ideal_length = min(k, len(expected_docs))\n",
    "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
    "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
    "    \"\"\"Utility function to compute Reciprocal Rank (RR) for a single query\"\"\"\n",
    "    rr = 0\n",
    "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
    "        if retrieved_doc in expected_docs:\n",
    "            rr = 1 / (i + 1)\n",
    "            break\n",
    "    return rr\n",
    "\n",
    "\n",
    "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Function to compute all eval metrics based on retrievals and/or generations\"\"\"\n",
    "\n",
    "    true_positives, precisions, recalls, f1_scores, ndcgs, rrs = 0, [], [], [], [], []\n",
    "    total_queries = len(batch[\"query\"])\n",
    "\n",
    "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
    "        expected_set = set(gt)\n",
    "        retrieved_set = set(pred)\n",
    "\n",
    "        true_positives = len(expected_set.intersection(retrieved_set))\n",
    "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
    "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=5))\n",
    "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
    "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
    "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
    "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
    "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_accumulate_metrics_fn(\n",
    "    aggregated_metrics: Dict[str, listtype],\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Function to accumulate eval metrics across all batches\"\"\"\n",
    "\n",
    "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
    "    total_queries = sum(num_queries_per_batch)\n",
    "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        **{\n",
    "            metric: {\n",
    "                \"value\": sum(\n",
    "                    m[\"value\"] * queries\n",
    "                    for m, queries in zip(\n",
    "                        aggregated_metrics[metric], num_queries_per_batch\n",
    "                    )\n",
    "                )\n",
    "                / total_queries,\n",
    "                \"is_algebraic\": True,\n",
    "                \"value_range\": (0, 1),\n",
    "            }\n",
    "            for metric in algebraic_metrics\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57887bc",
   "metadata": {},
   "source": [
    "### Define Partial Multi-Config Knobs for vLLM Generator part of RAG Pipeline using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f5d0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 vLLM generator configs with different sizes of generator models\n",
    "\n",
    "vllm_config1 = RFvLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": False,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    "    rag=rag_gpu,\n",
    "    prompt_manager=None,\n",
    ")\n",
    "\n",
    "vllm_config2 = RFvLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": False,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    "    rag=rag_gpu,\n",
    "    prompt_manager=None,\n",
    ")\n",
    "\n",
    "config_set = {\n",
    "    \"vllm_config\": List([vllm_config1, vllm_config2]),  # Each represents 4 configs\n",
    "    \"batch_size\": batch_size,\n",
    "    \"preprocess_fn\": sample_preprocess_fn,\n",
    "    \"postprocess_fn\": sample_postprocess_fn,\n",
    "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
    "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
    "    \"online_strategy_kwargs\": {\n",
    "        \"strategy_name\": \"normal\",\n",
    "        \"confidence_level\": 0.95,\n",
    "        \"use_fpc\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7dd280",
   "metadata": {},
   "source": [
    "### Create Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f26d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 8 combinations in total\n",
    "config_group = RFGridSearch(config_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Multi-Config Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"controller_ae5c011c\" style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto;\">\n",
       "            <style>\n",
       "                #controller_ae5c011c h3 { margin: 10px 0; font-size: 1.2em; font-weight: 600; }\n",
       "                #controller_ae5c011c .header-info { display: flex; gap: 20px; margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 4px; font-size: 13px; }\n",
       "                #controller_ae5c011c .section { margin: 15px 0; }\n",
       "                #controller_ae5c011c .section-label { font-weight: 600; margin-bottom: 8px; font-size: 14px; }\n",
       "                #controller_ae5c011c .button-row { display: flex; gap: 8px; flex-wrap: wrap; margin: 10px 0; }\n",
       "                #controller_ae5c011c select { padding: 6px 12px; border: 1px solid #ccc; border-radius: 4px; font-size: 13px; background: white; min-width: 300px; cursor: pointer; }\n",
       "                #controller_ae5c011c button { padding: 6px 16px; border: none; border-radius: 4px; font-size: 13px; font-weight: 500; cursor: pointer; }\n",
       "                #controller_ae5c011c button:disabled { opacity: 0.5; cursor: not-allowed; }\n",
       "                #controller_ae5c011c .btn-success { background: #28a745; color: white; }\n",
       "                #controller_ae5c011c .btn-danger { background: #dc3545; color: white; }\n",
       "                #controller_ae5c011c .btn-info { background: #17a2b8; color: white; }\n",
       "                #controller_ae5c011c .btn-default { background: #6c757d; color: white; }\n",
       "                #controller_ae5c011c textarea { width: 100%; min-height: 200px; padding: 10px; border: 1px solid #ccc; border-radius: 4px; font-family: 'Courier New', monospace; font-size: 12px; box-sizing: border-box; }\n",
       "                #controller_ae5c011c .status-message { padding: 10px; margin: 10px 0; border-radius: 4px; display: none; }\n",
       "                #controller_ae5c011c .msg-success { background: #d4edda; color: #155724; }\n",
       "                #controller_ae5c011c .msg-error { background: #f8d7da; color: #721c24; }\n",
       "                #controller_ae5c011c .msg-info { background: #d1ecf1; color: #0c5460; }\n",
       "            </style>\n",
       "\n",
       "            <div>\n",
       "                <h3>Interactive Run Controller</h3>\n",
       "                <div class=\"header-info\">\n",
       "                    <div><b>Run ID:</b> <span id=\"pipeline-id-value\">N/A</span></div>\n",
       "                    <div><b>Status:</b> <span id=\"status-value\">Not loaded</span></div>\n",
       "                    <div><b>Last Update:</b> <span id=\"last-update\">Never</span></div>\n",
       "                </div>\n",
       "\n",
       "                <div id=\"status-message\" class=\"status-message\"></div>\n",
       "\n",
       "                <div class=\"section\">\n",
       "                    <div class=\"section-label\">Select a Config ID:</div>\n",
       "                    <select id=\"pipeline-selector\">\n",
       "                        <option value=\"\">Waiting for data...</option>\n",
       "                    </select>\n",
       "                </div>\n",
       "\n",
       "                <div class=\"section\">\n",
       "                    <div class=\"button-row\">\n",
       "                        <button class=\"btn-success\" id=\"resume-btn\">‚ñ∂ Resume</button>\n",
       "                        <button class=\"btn-danger\" id=\"stop-btn\">‚ñ† Stop</button>\n",
       "                        <button class=\"btn-danger\" id=\"delete-btn\">üóë Delete</button>\n",
       "                    </div>\n",
       "                </div>\n",
       "\n",
       "                <div class=\"section\">\n",
       "                    <div class=\"section-label\">Configuration: <span id=\"config-name\">N/A</span></div>\n",
       "                    <textarea id=\"config-text\" readonly>{}</textarea>\n",
       "                    <div class=\"button-row\">\n",
       "                        <button class=\"btn-info\" id=\"clone-btn\">Clone Run</button>\n",
       "                        <button class=\"btn-success\" id=\"submit-clone-btn\" disabled>‚úì Submit Clone</button>\n",
       "                        <button class=\"btn-danger\" id=\"cancel-clone-btn\" disabled>‚úó Cancel</button>\n",
       "                    </div>\n",
       "                </div>\n",
       "            </div>\n",
       "\n",
       "            <script>\n",
       "                (function() {\n",
       "                    const WIDGET_ID = 'controller_ae5c011c';\n",
       "                    const DISPATCHER_URL = 'http://127.0.0.1:8851';\n",
       "                    let currentPipelineId = null;\n",
       "                    let currentConfig = null;\n",
       "                    let currentContextId = null;\n",
       "                    let isCloneMode = false;\n",
       "                    let pollingInterval = null;\n",
       "\n",
       "                    // Elements\n",
       "                    const el = {\n",
       "                        pipelineIdValue: document.getElementById('pipeline-id-value'),\n",
       "                        statusValue: document.getElementById('status-value'),\n",
       "                        lastUpdate: document.getElementById('last-update'),\n",
       "                        statusMessage: document.getElementById('status-message'),\n",
       "                        pipelineSelector: document.getElementById('pipeline-selector'),\n",
       "                        resumeBtn: document.getElementById('resume-btn'),\n",
       "                        stopBtn: document.getElementById('stop-btn'),\n",
       "                        deleteBtn: document.getElementById('delete-btn'),\n",
       "                        configName: document.getElementById('config-name'),\n",
       "                        configText: document.getElementById('config-text'),\n",
       "                        cloneBtn: document.getElementById('clone-btn'),\n",
       "                        submitCloneBtn: document.getElementById('submit-clone-btn'),\n",
       "                        cancelCloneBtn: document.getElementById('cancel-clone-btn')\n",
       "                    };\n",
       "\n",
       "                    // Use fetch API with explicit CORS mode and optional auth token\n",
       "                    async function xhrRequest(url, method = 'GET', body = null) {\n",
       "                        const options = {\n",
       "                            method: method,\n",
       "                            headers: {\n",
       "                                'Content-Type': 'application/json'\n",
       "                            },\n",
       "                            mode: 'cors',\n",
       "                            credentials: 'omit'  // Include cookies for Colab proxy auth\n",
       "                        };\n",
       "\n",
       "                        if (body) {\n",
       "                            options.body = JSON.stringify(body);\n",
       "                        }\n",
       "\n",
       "                        const response = await fetch(url, options);\n",
       "                        if (!response.ok) {\n",
       "                            throw new Error('HTTP ' + response.status);\n",
       "                        }\n",
       "                        return await response.json();\n",
       "                    }\n",
       "\n",
       "                    async function fetchPipelines() {\n",
       "                        try {\n",
       "                            console.log('Fetching pipelines...');\n",
       "                            const pipelines = await xhrRequest(DISPATCHER_URL + '/dispatcher/list-all-pipeline-ids');\n",
       "                            console.log('Got pipelines:', pipelines.length);\n",
       "\n",
       "                            updatePipelinesDropdown(pipelines);\n",
       "                            el.lastUpdate.textContent = new Date().toLocaleTimeString();\n",
       "\n",
       "                        } catch (error) {\n",
       "                            console.error('Failed to fetch pipelines:', error);\n",
       "                            showMessage('Connection error: ' + error.message, 'error');\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    async function fetchPipelineConfig(pipelineId) {\n",
       "                        try {\n",
       "                            const data = await xhrRequest(DISPATCHER_URL + `/dispatcher/get-pipeline-config-json/${pipelineId}`);\n",
       "                            const config = data.pipeline_config_json || {};\n",
       "\n",
       "                            currentConfig = config;\n",
       "                            currentContextId = data.context_id;\n",
       "\n",
       "                            el.configName.textContent = config.pipeline_name || 'N/A';\n",
       "\n",
       "                            if (!isCloneMode) {\n",
       "                                el.configText.value = JSON.stringify(config, null, 2);\n",
       "                            }\n",
       "\n",
       "                        } catch (error) {\n",
       "                            console.error('Failed to fetch config:', error);\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    function updatePipelinesDropdown(pipelines) {\n",
       "                        const selector = el.pipelineSelector;\n",
       "                        const currentSelection = selector.value;\n",
       "\n",
       "                        selector.innerHTML = '';\n",
       "\n",
       "                        if (pipelines && pipelines.length > 0) {\n",
       "                            pipelines.forEach(p => {\n",
       "                                const option = document.createElement('option');\n",
       "                                option.value = p.pipeline_id;\n",
       "                                option.textContent = `Config ID: ${p.pipeline_id} (${p.status || 'unknown'})`;\n",
       "                                selector.appendChild(option);\n",
       "                            });\n",
       "\n",
       "                            if (currentSelection && pipelines.some(p => p.pipeline_id == currentSelection)) {\n",
       "                                selector.value = currentSelection;\n",
       "                                currentPipelineId = currentSelection;\n",
       "                            } else {\n",
       "                                selector.value = pipelines[0].pipeline_id;\n",
       "                                currentPipelineId = pipelines[0].pipeline_id;\n",
       "                                fetchPipelineConfig(currentPipelineId);\n",
       "                            }\n",
       "\n",
       "                            // Update status display\n",
       "                            const currentPipeline = pipelines.find(p => p.pipeline_id == currentPipelineId);\n",
       "                            if (currentPipeline) {\n",
       "                                el.pipelineIdValue.textContent = currentPipeline.pipeline_id;\n",
       "                                el.statusValue.textContent = currentPipeline.status || 'unknown';\n",
       "\n",
       "                                const isCompleted = currentPipeline.status?.toLowerCase() === 'completed';\n",
       "                                el.resumeBtn.disabled = isCompleted;\n",
       "                                el.stopBtn.disabled = isCompleted;\n",
       "                                el.deleteBtn.disabled = isCompleted;\n",
       "                                el.cloneBtn.disabled = isCompleted || !currentContextId;\n",
       "                            }\n",
       "                        } else {\n",
       "                            selector.innerHTML = '<option value=\"\">No pipelines found</option>';\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    function showMessage(message, type) {\n",
       "                        el.statusMessage.className = 'status-message msg-' + type;\n",
       "                        el.statusMessage.textContent = message;\n",
       "                        el.statusMessage.style.display = 'block';\n",
       "                        setTimeout(() => el.statusMessage.style.display = 'none', 5000);\n",
       "                    }\n",
       "\n",
       "                    async function handleAction(action) {\n",
       "                        if (!currentPipelineId) {\n",
       "                            showMessage('No pipeline selected', 'error');\n",
       "                            return;\n",
       "                        }\n",
       "\n",
       "                        try {\n",
       "                            const endpoint = DISPATCHER_URL + `/dispatcher/${action}-pipeline`;\n",
       "                            const result = await xhrRequest(endpoint, 'POST', { pipeline_id: currentPipelineId });\n",
       "\n",
       "                            showMessage(`‚úì ${action} completed for pipeline ${currentPipelineId}`, 'success');\n",
       "\n",
       "                            // Refresh after a short delay\n",
       "                            setTimeout(async () => {\n",
       "                                await fetchPipelines();\n",
       "                            }, 500);\n",
       "\n",
       "                        } catch (error) {\n",
       "                            showMessage(`Error: ${error.message}`, 'error');\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    function enableCloneMode() {\n",
       "                        isCloneMode = true;\n",
       "                        el.configText.readOnly = false;\n",
       "                        el.submitCloneBtn.disabled = false;\n",
       "                        el.cancelCloneBtn.disabled = false;\n",
       "                        el.cloneBtn.disabled = true;\n",
       "                        showMessage('Edit config and click Submit to clone', 'info');\n",
       "                    }\n",
       "\n",
       "                    function disableCloneMode() {\n",
       "                        isCloneMode = false;\n",
       "                        el.configText.readOnly = true;\n",
       "                        el.configText.value = JSON.stringify(currentConfig || {}, null, 2);\n",
       "                        el.submitCloneBtn.disabled = true;\n",
       "                        el.cancelCloneBtn.disabled = true;\n",
       "                        el.cloneBtn.disabled = false;\n",
       "                    }\n",
       "\n",
       "                    async function handleClone() {\n",
       "                        if (!currentPipelineId) {\n",
       "                            showMessage('No pipeline selected', 'error');\n",
       "                            return;\n",
       "                        }\n",
       "\n",
       "                        try {\n",
       "                            // Parse edited config\n",
       "                            let editedConfig;\n",
       "                            try {\n",
       "                                editedConfig = JSON.parse(el.configText.value);\n",
       "                            } catch (e) {\n",
       "                                showMessage('Invalid JSON: ' + e.message, 'error');\n",
       "                                return;\n",
       "                            }\n",
       "\n",
       "                            // Validate required fields\n",
       "                            if (!editedConfig.pipeline_type) {\n",
       "                                showMessage('config_json must include pipeline_type', 'error');\n",
       "                                return;\n",
       "                            }\n",
       "\n",
       "                            // Send clone request\n",
       "                            const cloneRequest = {\n",
       "                                parent_pipeline_id: currentPipelineId,\n",
       "                                config_json: editedConfig\n",
       "                            };\n",
       "\n",
       "                            const result = await xhrRequest(\n",
       "                                DISPATCHER_URL + '/dispatcher/clone-pipeline',\n",
       "                                'POST',\n",
       "                                cloneRequest\n",
       "                            );\n",
       "\n",
       "                            showMessage(`‚úì Cloned from Config ID ${currentPipelineId} successfully!`, 'success');\n",
       "                            disableCloneMode();\n",
       "\n",
       "                            // Refresh after delay\n",
       "                            setTimeout(async () => {\n",
       "                                await fetchPipelines();\n",
       "                            }, 1000);\n",
       "\n",
       "                        } catch (error) {\n",
       "                            showMessage(`Error cloning: ${error.message}`, 'error');\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    // Event listeners\n",
       "                    el.pipelineSelector.addEventListener('change', (e) => {\n",
       "                        if (e.target.value) {\n",
       "                            currentPipelineId = parseInt(e.target.value);\n",
       "                            fetchPipelineConfig(currentPipelineId);\n",
       "                        }\n",
       "                    });\n",
       "\n",
       "                    el.resumeBtn.addEventListener('click', () => handleAction('resume'));\n",
       "                    el.stopBtn.addEventListener('click', () => handleAction('stop'));\n",
       "                    el.deleteBtn.addEventListener('click', () => handleAction('delete'));\n",
       "\n",
       "                    el.cloneBtn.addEventListener('click', enableCloneMode);\n",
       "                    el.submitCloneBtn.addEventListener('click', handleClone);\n",
       "                    el.cancelCloneBtn.addEventListener('click', () => {\n",
       "                        disableCloneMode();\n",
       "                        showMessage('Cancelled clone', 'info');\n",
       "                    });\n",
       "\n",
       "                    // Initial fetch\n",
       "                    console.log('UI initialized, fetching initial data...');\n",
       "                    setTimeout(async () => {\n",
       "                        await fetchPipelines();\n",
       "\n",
       "                        // Start polling - use HTTP fetch (works even when kernel is busy)\n",
       "                        pollingInterval = setInterval(async () => {\n",
       "                            await fetchPipelines();\n",
       "                        }, 3000.0);\n",
       "                        console.log('Polling started: every 3.0s');\n",
       "                    }, 1000);\n",
       "\n",
       "                    // Cleanup on unload\n",
       "                    window.addEventListener('beforeunload', () => {\n",
       "                        if (pollingInterval) clearInterval(pollingInterval);\n",
       "                    });\n",
       "                })();\n",
       "            </script>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preprocessing RAG Sources ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dbc62\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_dbc62_level0_col0\" class=\"col_heading level0 col0\" >RAG Source ID</th>\n",
       "      <th id=\"T_dbc62_level0_col1\" class=\"col_heading level0 col1\" >Status</th>\n",
       "      <th id=\"T_dbc62_level0_col2\" class=\"col_heading level0 col2\" >Duration</th>\n",
       "      <th id=\"T_dbc62_level0_col3\" class=\"col_heading level0 col3\" >Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_dbc62_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_dbc62_row0_col1\" class=\"data row0 col1\" >Complete</td>\n",
       "      <td id=\"T_dbc62_row0_col2\" class=\"data row0 col2\" >15.8s</td>\n",
       "      <td id=\"T_dbc62_row0_col3\" class=\"data row0 col3\" >PGVector [fiqa_chunk64], GPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dbc62_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_dbc62_row1_col1\" class=\"data row1 col1\" >Complete</td>\n",
       "      <td id=\"T_dbc62_row1_col2\" class=\"data row1 col2\" >15.8s</td>\n",
       "      <td id=\"T_dbc62_row1_col3\" class=\"data row1 col3\" >PGVector [fiqa_chunk256], GPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79748c40ef30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multi-Config Experiment Progress ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b7113\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_b7113_level0_col0\" class=\"col_heading level0 col0\" >Run ID</th>\n",
       "      <th id=\"T_b7113_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_b7113_level0_col2\" class=\"col_heading level0 col2\" >Status</th>\n",
       "      <th id=\"T_b7113_level0_col3\" class=\"col_heading level0 col3\" >Progress</th>\n",
       "      <th id=\"T_b7113_level0_col4\" class=\"col_heading level0 col4\" >Conf. Interval</th>\n",
       "      <th id=\"T_b7113_level0_col5\" class=\"col_heading level0 col5\" >search_type</th>\n",
       "      <th id=\"T_b7113_level0_col6\" class=\"col_heading level0 col6\" >rag_k</th>\n",
       "      <th id=\"T_b7113_level0_col7\" class=\"col_heading level0 col7\" >top_n</th>\n",
       "      <th id=\"T_b7113_level0_col8\" class=\"col_heading level0 col8\" >sampling_params</th>\n",
       "      <th id=\"T_b7113_level0_col9\" class=\"col_heading level0 col9\" >model_config</th>\n",
       "      <th id=\"T_b7113_level0_col10\" class=\"col_heading level0 col10\" >Precision</th>\n",
       "      <th id=\"T_b7113_level0_col11\" class=\"col_heading level0 col11\" >Recall</th>\n",
       "      <th id=\"T_b7113_level0_col12\" class=\"col_heading level0 col12\" >F1 Score</th>\n",
       "      <th id=\"T_b7113_level0_col13\" class=\"col_heading level0 col13\" >NDCG@5</th>\n",
       "      <th id=\"T_b7113_level0_col14\" class=\"col_heading level0 col14\" >MRR</th>\n",
       "      <th id=\"T_b7113_level0_col15\" class=\"col_heading level0 col15\" >Throughput</th>\n",
       "      <th id=\"T_b7113_level0_col16\" class=\"col_heading level0 col16\" >Total</th>\n",
       "      <th id=\"T_b7113_level0_col17\" class=\"col_heading level0 col17\" >Samples Processed</th>\n",
       "      <th id=\"T_b7113_level0_col18\" class=\"col_heading level0 col18\" >Processing Time</th>\n",
       "      <th id=\"T_b7113_level0_col19\" class=\"col_heading level0 col19\" >Samples Per Second</th>\n",
       "      <th id=\"T_b7113_level0_col20\" class=\"col_heading level0 col20\" >model_name</th>\n",
       "      <th id=\"T_b7113_level0_col21\" class=\"col_heading level0 col21\" >run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_b7113_row0_col1\" class=\"data row0 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row0_col2\" class=\"data row0 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row0_col3\" class=\"data row0 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row0_col4\" class=\"data row0 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row0_col5\" class=\"data row0 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row0_col6\" class=\"data row0 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row0_col7\" class=\"data row0 col7\" >2.00</td>\n",
       "      <td id=\"T_b7113_row0_col8\" class=\"data row0 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row0_col9\" class=\"data row0 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row0_col10\" class=\"data row0 col10\" >25.20% [25.20%, 25.20%]</td>\n",
       "      <td id=\"T_b7113_row0_col11\" class=\"data row0 col11\" >25.15% [25.15%, 25.15%]</td>\n",
       "      <td id=\"T_b7113_row0_col12\" class=\"data row0 col12\" >23.03% [23.03%, 23.03%]</td>\n",
       "      <td id=\"T_b7113_row0_col13\" class=\"data row0 col13\" >8.14% [8.14%, 8.14%]</td>\n",
       "      <td id=\"T_b7113_row0_col14\" class=\"data row0 col14\" >30.30% [30.30%, 30.30%]</td>\n",
       "      <td id=\"T_b7113_row0_col15\" class=\"data row0 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row0_col16\" class=\"data row0 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row0_col17\" class=\"data row0 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row0_col18\" class=\"data row0 col18\" >1304.27 seconds</td>\n",
       "      <td id=\"T_b7113_row0_col19\" class=\"data row0 col19\" >0.38</td>\n",
       "      <td id=\"T_b7113_row0_col20\" class=\"data row0 col20\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row0_col21\" class=\"data row0 col21\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_b7113_row1_col1\" class=\"data row1 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row1_col2\" class=\"data row1 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row1_col3\" class=\"data row1 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row1_col4\" class=\"data row1 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row1_col5\" class=\"data row1 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row1_col6\" class=\"data row1 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row1_col7\" class=\"data row1 col7\" >5.00</td>\n",
       "      <td id=\"T_b7113_row1_col8\" class=\"data row1 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row1_col9\" class=\"data row1 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row1_col10\" class=\"data row1 col10\" >25.20% [25.20%, 25.20%]</td>\n",
       "      <td id=\"T_b7113_row1_col11\" class=\"data row1 col11\" >25.15% [25.15%, 25.15%]</td>\n",
       "      <td id=\"T_b7113_row1_col12\" class=\"data row1 col12\" >23.03% [23.03%, 23.03%]</td>\n",
       "      <td id=\"T_b7113_row1_col13\" class=\"data row1 col13\" >8.14% [8.14%, 8.14%]</td>\n",
       "      <td id=\"T_b7113_row1_col14\" class=\"data row1 col14\" >30.30% [30.30%, 30.30%]</td>\n",
       "      <td id=\"T_b7113_row1_col15\" class=\"data row1 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row1_col16\" class=\"data row1 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row1_col17\" class=\"data row1 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row1_col18\" class=\"data row1 col18\" >1249.12 seconds</td>\n",
       "      <td id=\"T_b7113_row1_col19\" class=\"data row1 col19\" >0.40</td>\n",
       "      <td id=\"T_b7113_row1_col20\" class=\"data row1 col20\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row1_col21\" class=\"data row1 col21\" >2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_b7113_row2_col1\" class=\"data row2 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row2_col2\" class=\"data row2 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row2_col3\" class=\"data row2 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row2_col4\" class=\"data row2 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row2_col5\" class=\"data row2 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row2_col6\" class=\"data row2 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row2_col7\" class=\"data row2 col7\" >2.00</td>\n",
       "      <td id=\"T_b7113_row2_col8\" class=\"data row2 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row2_col9\" class=\"data row2 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row2_col10\" class=\"data row2 col10\" >23.70% [23.70%, 23.70%]</td>\n",
       "      <td id=\"T_b7113_row2_col11\" class=\"data row2 col11\" >29.89% [29.89%, 29.89%]</td>\n",
       "      <td id=\"T_b7113_row2_col12\" class=\"data row2 col12\" >24.52% [24.52%, 24.52%]</td>\n",
       "      <td id=\"T_b7113_row2_col13\" class=\"data row2 col13\" >9.15% [9.15%, 9.15%]</td>\n",
       "      <td id=\"T_b7113_row2_col14\" class=\"data row2 col14\" >32.80% [32.80%, 32.80%]</td>\n",
       "      <td id=\"T_b7113_row2_col15\" class=\"data row2 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row2_col16\" class=\"data row2 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row2_col17\" class=\"data row2 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row2_col18\" class=\"data row2 col18\" >1205.78 seconds</td>\n",
       "      <td id=\"T_b7113_row2_col19\" class=\"data row2 col19\" >0.41</td>\n",
       "      <td id=\"T_b7113_row2_col20\" class=\"data row2 col20\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row2_col21\" class=\"data row2 col21\" >3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_b7113_row3_col1\" class=\"data row3 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row3_col2\" class=\"data row3 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row3_col3\" class=\"data row3 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row3_col4\" class=\"data row3 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row3_col5\" class=\"data row3 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row3_col6\" class=\"data row3 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row3_col7\" class=\"data row3 col7\" >5.00</td>\n",
       "      <td id=\"T_b7113_row3_col8\" class=\"data row3 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row3_col9\" class=\"data row3 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row3_col10\" class=\"data row3 col10\" >23.70% [23.70%, 23.70%]</td>\n",
       "      <td id=\"T_b7113_row3_col11\" class=\"data row3 col11\" >29.89% [29.89%, 29.89%]</td>\n",
       "      <td id=\"T_b7113_row3_col12\" class=\"data row3 col12\" >24.52% [24.52%, 24.52%]</td>\n",
       "      <td id=\"T_b7113_row3_col13\" class=\"data row3 col13\" >9.15% [9.15%, 9.15%]</td>\n",
       "      <td id=\"T_b7113_row3_col14\" class=\"data row3 col14\" >32.80% [32.80%, 32.80%]</td>\n",
       "      <td id=\"T_b7113_row3_col15\" class=\"data row3 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row3_col16\" class=\"data row3 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row3_col17\" class=\"data row3 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row3_col18\" class=\"data row3 col18\" >1204.29 seconds</td>\n",
       "      <td id=\"T_b7113_row3_col19\" class=\"data row3 col19\" >0.42</td>\n",
       "      <td id=\"T_b7113_row3_col20\" class=\"data row3 col20\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td id=\"T_b7113_row3_col21\" class=\"data row3 col21\" >4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_b7113_row4_col1\" class=\"data row4 col1\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row4_col2\" class=\"data row4 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row4_col3\" class=\"data row4 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row4_col4\" class=\"data row4 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row4_col5\" class=\"data row4 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row4_col6\" class=\"data row4 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row4_col7\" class=\"data row4 col7\" >2.00</td>\n",
       "      <td id=\"T_b7113_row4_col8\" class=\"data row4 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row4_col9\" class=\"data row4 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row4_col10\" class=\"data row4 col10\" >25.20% [25.20%, 25.20%]</td>\n",
       "      <td id=\"T_b7113_row4_col11\" class=\"data row4 col11\" >25.15% [25.15%, 25.15%]</td>\n",
       "      <td id=\"T_b7113_row4_col12\" class=\"data row4 col12\" >23.03% [23.03%, 23.03%]</td>\n",
       "      <td id=\"T_b7113_row4_col13\" class=\"data row4 col13\" >8.14% [8.14%, 8.14%]</td>\n",
       "      <td id=\"T_b7113_row4_col14\" class=\"data row4 col14\" >30.30% [30.30%, 30.30%]</td>\n",
       "      <td id=\"T_b7113_row4_col15\" class=\"data row4 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row4_col16\" class=\"data row4 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row4_col17\" class=\"data row4 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row4_col18\" class=\"data row4 col18\" >1157.67 seconds</td>\n",
       "      <td id=\"T_b7113_row4_col19\" class=\"data row4 col19\" >0.43</td>\n",
       "      <td id=\"T_b7113_row4_col20\" class=\"data row4 col20\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row4_col21\" class=\"data row4 col21\" >5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "      <td id=\"T_b7113_row5_col1\" class=\"data row5 col1\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row5_col2\" class=\"data row5 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row5_col3\" class=\"data row5 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row5_col4\" class=\"data row5 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row5_col5\" class=\"data row5 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row5_col6\" class=\"data row5 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row5_col7\" class=\"data row5 col7\" >5.00</td>\n",
       "      <td id=\"T_b7113_row5_col8\" class=\"data row5 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row5_col9\" class=\"data row5 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row5_col10\" class=\"data row5 col10\" >25.20% [25.20%, 25.20%]</td>\n",
       "      <td id=\"T_b7113_row5_col11\" class=\"data row5 col11\" >25.15% [25.15%, 25.15%]</td>\n",
       "      <td id=\"T_b7113_row5_col12\" class=\"data row5 col12\" >23.03% [23.03%, 23.03%]</td>\n",
       "      <td id=\"T_b7113_row5_col13\" class=\"data row5 col13\" >8.14% [8.14%, 8.14%]</td>\n",
       "      <td id=\"T_b7113_row5_col14\" class=\"data row5 col14\" >30.30% [30.30%, 30.30%]</td>\n",
       "      <td id=\"T_b7113_row5_col15\" class=\"data row5 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row5_col16\" class=\"data row5 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row5_col17\" class=\"data row5 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row5_col18\" class=\"data row5 col18\" >1100.49 seconds</td>\n",
       "      <td id=\"T_b7113_row5_col19\" class=\"data row5 col19\" >0.45</td>\n",
       "      <td id=\"T_b7113_row5_col20\" class=\"data row5 col20\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row5_col21\" class=\"data row5 col21\" >6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row6_col0\" class=\"data row6 col0\" >7</td>\n",
       "      <td id=\"T_b7113_row6_col1\" class=\"data row6 col1\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row6_col2\" class=\"data row6 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row6_col3\" class=\"data row6 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row6_col4\" class=\"data row6 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row6_col5\" class=\"data row6 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row6_col6\" class=\"data row6 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row6_col7\" class=\"data row6 col7\" >2.00</td>\n",
       "      <td id=\"T_b7113_row6_col8\" class=\"data row6 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row6_col9\" class=\"data row6 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row6_col10\" class=\"data row6 col10\" >23.70% [23.70%, 23.70%]</td>\n",
       "      <td id=\"T_b7113_row6_col11\" class=\"data row6 col11\" >29.89% [29.89%, 29.89%]</td>\n",
       "      <td id=\"T_b7113_row6_col12\" class=\"data row6 col12\" >24.52% [24.52%, 24.52%]</td>\n",
       "      <td id=\"T_b7113_row6_col13\" class=\"data row6 col13\" >9.15% [9.15%, 9.15%]</td>\n",
       "      <td id=\"T_b7113_row6_col14\" class=\"data row6 col14\" >32.80% [32.80%, 32.80%]</td>\n",
       "      <td id=\"T_b7113_row6_col15\" class=\"data row6 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row6_col16\" class=\"data row6 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row6_col17\" class=\"data row6 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row6_col18\" class=\"data row6 col18\" >1043.51 seconds</td>\n",
       "      <td id=\"T_b7113_row6_col19\" class=\"data row6 col19\" >0.48</td>\n",
       "      <td id=\"T_b7113_row6_col20\" class=\"data row6 col20\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row6_col21\" class=\"data row6 col21\" >7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b7113_row7_col0\" class=\"data row7 col0\" >8</td>\n",
       "      <td id=\"T_b7113_row7_col1\" class=\"data row7 col1\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row7_col2\" class=\"data row7 col2\" >COMPLETED</td>\n",
       "      <td id=\"T_b7113_row7_col3\" class=\"data row7 col3\" >4/4</td>\n",
       "      <td id=\"T_b7113_row7_col4\" class=\"data row7 col4\" >0.000</td>\n",
       "      <td id=\"T_b7113_row7_col5\" class=\"data row7 col5\" >similarity</td>\n",
       "      <td id=\"T_b7113_row7_col6\" class=\"data row7 col6\" >15.00</td>\n",
       "      <td id=\"T_b7113_row7_col7\" class=\"data row7 col7\" >5.00</td>\n",
       "      <td id=\"T_b7113_row7_col8\" class=\"data row7 col8\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 512}</td>\n",
       "      <td id=\"T_b7113_row7_col9\" class=\"data row7 col9\" >{'dtype': 'half', 'gpu_memory_utilization': 0.7, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'max_model_len': 4096, 'disable_log_stats': True}</td>\n",
       "      <td id=\"T_b7113_row7_col10\" class=\"data row7 col10\" >23.70% [23.70%, 23.70%]</td>\n",
       "      <td id=\"T_b7113_row7_col11\" class=\"data row7 col11\" >29.89% [29.89%, 29.89%]</td>\n",
       "      <td id=\"T_b7113_row7_col12\" class=\"data row7 col12\" >24.52% [24.52%, 24.52%]</td>\n",
       "      <td id=\"T_b7113_row7_col13\" class=\"data row7 col13\" >9.15% [9.15%, 9.15%]</td>\n",
       "      <td id=\"T_b7113_row7_col14\" class=\"data row7 col14\" >32.80% [32.80%, 32.80%]</td>\n",
       "      <td id=\"T_b7113_row7_col15\" class=\"data row7 col15\" >0.5/s</td>\n",
       "      <td id=\"T_b7113_row7_col16\" class=\"data row7 col16\" >500</td>\n",
       "      <td id=\"T_b7113_row7_col17\" class=\"data row7 col17\" >500</td>\n",
       "      <td id=\"T_b7113_row7_col18\" class=\"data row7 col18\" >1042.13 seconds</td>\n",
       "      <td id=\"T_b7113_row7_col19\" class=\"data row7 col19\" >0.48</td>\n",
       "      <td id=\"T_b7113_row7_col20\" class=\"data row7 col20\" >Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td id=\"T_b7113_row7_col21\" class=\"data row7 col21\" >8.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7974846138c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\n",
    "# num_actors: set to the number of GPUs you want to use for inference.\n",
    "results = experiment.run_evals(\n",
    "    config_group=config_group,\n",
    "    dataset=fiqa_dataset,\n",
    "    num_actors=4, # If not set, auto-detects and uses all available GPUs.\n",
    "    num_shards=4,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "127e7b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>search_type</th>\n",
       "      <th>rag_k</th>\n",
       "      <th>top_n</th>\n",
       "      <th>sampling_params</th>\n",
       "      <th>model_config</th>\n",
       "      <th>Samples Processed</th>\n",
       "      <th>Processing Time</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Total</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1304.27 seconds</td>\n",
       "      <td>0.38</td>\n",
       "      <td>500</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.251540</td>\n",
       "      <td>0.230257</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1249.12 seconds</td>\n",
       "      <td>0.40</td>\n",
       "      <td>500</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.251540</td>\n",
       "      <td>0.230257</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1205.78 seconds</td>\n",
       "      <td>0.41</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1204.29 seconds</td>\n",
       "      <td>0.42</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1157.67 seconds</td>\n",
       "      <td>0.43</td>\n",
       "      <td>500</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.251540</td>\n",
       "      <td>0.230257</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1100.49 seconds</td>\n",
       "      <td>0.45</td>\n",
       "      <td>500</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.251540</td>\n",
       "      <td>0.230257</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1043.51 seconds</td>\n",
       "      <td>0.48</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1042.13 seconds</td>\n",
       "      <td>0.48</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_id                  model_name search_type  rag_k  top_n  \\\n",
       "0       1  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      2   \n",
       "1       2  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      5   \n",
       "2       3  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      2   \n",
       "3       4  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      5   \n",
       "4       5    Qwen/Qwen2.5-3B-Instruct  similarity     15      2   \n",
       "5       6    Qwen/Qwen2.5-3B-Instruct  similarity     15      5   \n",
       "6       7    Qwen/Qwen2.5-3B-Instruct  similarity     15      2   \n",
       "7       8    Qwen/Qwen2.5-3B-Instruct  similarity     15      5   \n",
       "\n",
       "                                     sampling_params  \\\n",
       "0  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "1  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "2  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "3  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "4  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "5  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "6  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "7  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "\n",
       "                                        model_config  Samples Processed  \\\n",
       "0  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "1  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "2  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "3  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "4  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "5  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "6  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "7  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "\n",
       "   Processing Time Samples Per Second  Total  Precision    Recall  F1 Score  \\\n",
       "0  1304.27 seconds               0.38    500      0.252  0.251540  0.230257   \n",
       "1  1249.12 seconds               0.40    500      0.252  0.251540  0.230257   \n",
       "2  1205.78 seconds               0.41    500      0.237  0.298892  0.245213   \n",
       "3  1204.29 seconds               0.42    500      0.237  0.298892  0.245213   \n",
       "4  1157.67 seconds               0.43    500      0.252  0.251540  0.230257   \n",
       "5  1100.49 seconds               0.45    500      0.252  0.251540  0.230257   \n",
       "6  1043.51 seconds               0.48    500      0.237  0.298892  0.245213   \n",
       "7  1042.13 seconds               0.48    500      0.237  0.298892  0.245213   \n",
       "\n",
       "     NDCG@5    MRR  \n",
       "0  0.081448  0.303  \n",
       "1  0.081448  0.303  \n",
       "2  0.091532  0.328  \n",
       "3  0.091532  0.328  \n",
       "4  0.081448  0.303  \n",
       "5  0.081448  0.303  \n",
       "6  0.091532  0.328  \n",
       "7  0.091532  0.328  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results dict to DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
    "    for run_id, (_, metrics_dict) in results.items()\n",
    "])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment exp1-fiqa-rag_39 ended\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09265e66",
   "metadata": {},
   "source": [
    "### View RapidFire AI Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05379a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Log File: /home/ubuntu/rapidfireai/logs/exp1-fiqa-rag_39/rapidfire.log\n",
      "\n",
      "================================================================================\n",
      "Last 30 lines of rapidfire.log:\n",
      "================================================================================\n",
      "2026-02-27 04:58:18 | QueryProcessingActor-0 | INFO | query_actor.py:255 | [exp1-fiqa-rag_39:QueryProcessingActor-0] Recreated RAG spec with retrieval mode\n",
      "2026-02-27 04:58:18 | Controller | INFO | controller.py:1369 | [exp1-fiqa-rag_39:Controller] Scheduling pipeline 8 (Pipeline 8) on actor 2 for shard 3 (1 batches)\n",
      "2026-02-27 04:58:18 | QueryProcessingActor-2 | INFO | query_actor.py:121 | [exp1-fiqa-rag_39:QueryProcessingActor-2] Cleaning up old inference engine (hash: a71c506c)\n",
      "2026-02-27 04:58:22 | QueryProcessingActor-2 | INFO | query_actor.py:140 | [exp1-fiqa-rag_39:QueryProcessingActor-2] GPU memory cache cleared\n",
      "2026-02-27 04:58:22 | QueryProcessingActor-2 | INFO | query_actor.py:144 | [exp1-fiqa-rag_39:QueryProcessingActor-2] Initializing new inference engine (config hash: ad75bd6b)\n",
      "2026-02-27 04:59:16 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2026-02-27 04:59:17 | QueryProcessingActor-2 | INFO | query_actor.py:185 | [exp1-fiqa-rag_39:QueryProcessingActor-2] Recreated embedding function: HuggingFaceEmbeddings\n",
      "2026-02-27 04:59:17 | QueryProcessingActor-2 | INFO | query_actor.py:238 | [exp1-fiqa-rag_39:QueryProcessingActor-2] Recreated retriever with search_type=similarity\n",
      "2026-02-27 04:59:17 | QueryProcessingActor-2 | INFO | query_actor.py:255 | [exp1-fiqa-rag_39:QueryProcessingActor-2] Recreated RAG spec with retrieval mode\n",
      "2026-02-27 04:59:17 | Controller | INFO | controller.py:1161 | [exp1-fiqa-rag_39:Controller] Pipeline 6 completed all 4 shards\n",
      "2026-02-27 04:59:17 | Controller | INFO | controller.py:1266 | [exp1-fiqa-rag_39:Controller] Pipeline 6 completed shard 3 (1 batches, 61.11s)\n",
      "2026-02-27 04:59:18 | Controller | INFO | controller.py:1161 | [exp1-fiqa-rag_39:Controller] Pipeline 7 completed all 4 shards\n",
      "2026-02-27 04:59:18 | Controller | INFO | controller.py:1266 | [exp1-fiqa-rag_39:Controller] Pipeline 7 completed shard 3 (1 batches, 59.69s)\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:1161 | [exp1-fiqa-rag_39:Controller] Pipeline 8 completed all 4 shards\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:1266 | [exp1-fiqa-rag_39:Controller] Pipeline 8 completed shard 3 (1 batches, 44.06s)\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:1333 | [exp1-fiqa-rag_39:Controller] All pipelines completed all shards!\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:688 | [exp1-fiqa-rag_39:Controller] Computing final metrics for all pipelines...\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 1 (Pipeline 1) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 2 (Pipeline 2) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 3 (Pipeline 3) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 4 (Pipeline 4) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 5 (Pipeline 5) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 6 (Pipeline 6) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 7 (Pipeline 7) completed successfully\n",
      "2026-02-27 05:00:02 | Controller | INFO | controller.py:848 | [exp1-fiqa-rag_39:Controller] Pipeline 8 (Pipeline 8) completed successfully\n",
      "2026-02-27 05:00:02 | ExperimentUtils | INFO | experiment_utils.py:180 | [exp1-fiqa-rag_39:ExperimentUtils] Reset experiment states - marked ongoing pipelines, contexts, and tasks as failed\n",
      "2026-02-27 05:00:02 | ExperimentUtils | INFO | experiment_utils.py:188 | [exp1-fiqa-rag_39:ExperimentUtils] Experiment marked as cancelled. Ongoing pipelines, contexts, and tasks have been marked as failed.\n",
      "2026-02-27 05:00:02 | ExperimentUtils | INFO | experiment_utils.py:154 | [exp1-fiqa-rag_39:ExperimentUtils] Experiment exp1-fiqa-rag_39 ended\n",
      "2026-02-27 05:00:03 | Experiment | INFO | experiment.py:573 | [exp1-fiqa-rag_39:Experiment] All actors shut down\n",
      "2026-02-27 05:00:03 | Experiment | INFO | experiment.py:574 | [exp1-fiqa-rag_39:Experiment] Dispatcher will automatically shut down (daemon thread)\n"
     ]
    }
   ],
   "source": [
    "# Get the experiment-specific log file\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "print(f\"üìÑ Log File: {log_file}\")\n",
    "print()\n",
    "\n",
    "if log_file.exists():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Last 30 lines of {log_file.name}:\")\n",
    "    print(\"=\" * 80)\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            print(line.rstrip())\n",
    "else:\n",
    "    print(f\"‚ùå Log file not found: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac1e65-442f-4292-9ad1-df196e2c6a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
