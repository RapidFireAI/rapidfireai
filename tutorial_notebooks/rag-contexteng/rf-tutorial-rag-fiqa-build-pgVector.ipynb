{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06d0c6d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
    "<br/>\n",
    "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### RapidFire AI RAG/Context Engineering Tutorial Use Case: Financial Opinion Q&A Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
    "import re, json\n",
    "from typing import List as listtype, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645aa86f",
   "metadata": {},
   "source": [
    "### Install Postgres Vector Store\n",
    "\n",
    "```python\n",
    "!pip install -qU langchain-postgres\n",
    "```\n",
    "\n",
    "Then spin up the postgres server\n",
    "```bash\n",
    "docker run --name pgvector-container -e POSTGRES_USER=langchain -e POSTGRES_PASSWORD=langchain -e POSTGRES_DB=langchain -p 6024:5432 -d pgvector/pgvector:pg16\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16327b",
   "metadata": {},
   "source": [
    "### Load Dataset and Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee571098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset directory is now in tutorial_notebooks/evals/datasets\n",
    "dataset_dir = Path(\"datasets\")\n",
    "\n",
    "fiqa_dataset = load_dataset(\"json\", data_files=str(dataset_dir / \"fiqa\" / \"queries.jsonl\"), split=\"train\").select(range(500))\n",
    "fiqa_dataset = fiqa_dataset.rename_columns({\"text\": \"query\", \"_id\": \"query_id\"})\n",
    "qrels = pd.read_csv(str(dataset_dir / \"fiqa\" / \"qrels.tsv\"), sep=\"\\t\")\n",
    "qrels = qrels.rename(\n",
    "    columns={\"query-id\": \"query_id\", \"corpus-id\": \"corpus_id\", \"score\": \"relevance\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28399289",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70816920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previously running experiment exp1-fiqa-rag_20 was forcibly ended. Created a new experiment 'exp1-fiqa-rag_21' with Experiment ID: 33 at /home/ubuntu/rapidfireai/rapidfire_experiments/exp1-fiqa-rag_21\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(experiment_name=\"exp1-fiqa-rag\", mode=\"evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4357898b-ef32-49d0-851d-f640e6fd5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Per-Actor batch size for hardware efficiency\n",
    "batch_size = 128\n",
    "\n",
    "document_loader = DirectoryLoader(\n",
    "    path=str(dataset_dir / \"fiqa\"),\n",
    "    glob=\"corpus.jsonl\",\n",
    "    loader_cls=JSONLoader,\n",
    "    loader_kwargs={\n",
    "        \"jq_schema\": \".\",\n",
    "        \"content_key\": \"text\",\n",
    "        \"metadata_func\": lambda record, metadata: {\n",
    "            \"corpus_id\": int(record.get(\"_id\"))\n",
    "        },  # store the document id\n",
    "        \"json_lines\": True,\n",
    "        \"text_content\": False,\n",
    "    },\n",
    "    sample_seed=42,\n",
    ")\n",
    "\n",
    "text_splitter_1 = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"gpt2\", chunk_size=64, chunk_overlap=32\n",
    ")\n",
    "\n",
    "text_splitter_2 = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"gpt2\", chunk_size=256, chunk_overlap=32\n",
    ")\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cuda:0\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774b2f92-2fb2-4a38-ae2e-cc1e0503c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.retrievers import BaseRetriever\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# class PGRetriever(BaseRetriever):\n",
    "#     def __init__(vector_store: PGVector):\n",
    "#         self.vector_store = vector_store\n",
    "\n",
    "#     def _get_relevant_documents(self, query: str) -> list[Document]:\n",
    "#         \"\"\"Return the first k documents from the list of documents\"\"\"\n",
    "#         return self.vector_store.similarity_search(query, k=15)\n",
    "\n",
    "#     async def _aget_relevant_documents(self, query: str) -> list[Document]:\n",
    "#         \"\"\"(Optional) async native implementation.\"\"\"\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc6ca9c-cab3-46a9-b771-ae697a62e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents for fiqa_chunk64...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a410917f0414005bdd1ab0654fca288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing into 'fiqa_chunk64':   0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents for fiqa_chunk256...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b742e7ed9f84d419e80b61c56a62b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing into 'fiqa_chunk256':   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_postgres import PGVector\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"  # Uses psycopg3!\n",
    "\n",
    "vector_store_small_chunk = PGVector(\n",
    "    embeddings=embedding_function,\n",
    "    collection_name=\"fiqa_chunk64\",\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "vector_store_large_chunk = PGVector(\n",
    "    embeddings=embedding_function,\n",
    "    collection_name=\"fiqa_chunk256\",\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "def build_vector_store(document_loader, text_splitter, vector_store, batch_size=1024):\n",
    "    print(f\"Loading documents for {vector_store.collection_name}...\")\n",
    "    documents = document_loader.load()\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(documents), batch_size), total=num_batches, desc=f\"Indexing into '{vector_store.collection_name}'\"):\n",
    "        vector_store.add_documents(documents=documents[i: i + batch_size])\n",
    "\n",
    "# Indexing documents for each vector store. Adjust batch_size to avoid wire buffer overflow.\n",
    "build_vector_store(document_loader, text_splitter_1, vector_store_small_chunk, batch_size=1024)\n",
    "build_vector_store(document_loader, text_splitter_2, vector_store_large_chunk, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56f5f389-4d90-47a8-ad39-66b68c6f144d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6be4429d-381c-4a11-9454-459532415016', metadata={'corpus_id': 189279}, page_content='\\\\#2: [Plz no hitting](https://np.reddit.com/r/punchablefaces/comments/6n0h4d/plz_no_hitting/)   \\\\#3: [I just had a'),\n",
       " Document(id='907b22dc-1d0a-4eab-851f-42cb635df090', metadata={'corpus_id': 388076}, page_content='then # key, then 1 to confirm     press 1 to select a freeze     there will be a long pause at this point but when the bot comes back it goes very fast.          Write down the 10-digit pin provided XXXXXXXXXX then'),\n",
       " Document(id='e7af1b00-cd7f-469f-b0d4-9baf363e351b', metadata={'corpus_id': 235271}, page_content=\"double, 144 is the 4X. But I'm a math guy, and my logic might not be logical to OP. So -  Let's take the 20th root of 4.   This is the key to use. 4, (hit key) 20, equals. The result is 1.07177\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_small_chunk.similarity_search(\n",
    "    \"hit key\",\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25f59f6d-36ae-4e10-9a75-aed9e60dec2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='aa7492a2-8aa1-4075-842d-9aa0dd986a0d', metadata={'corpus_id': 501842}, page_content=\"Stupid article. The whole idea of pressing 3 far-away keys to reset the system is so it won't happen accidentally.  And Windows became very stable lately that I can't recall pressing these buttons in a looooonnnnggggg time.\"),\n",
       " Document(id='541f301f-4d20-427c-9c80-cf52ecd0dd19', metadata={'corpus_id': 308260}, page_content='\"I use keyboards with Cherry MX Blue switches. They are firm and responsive enough to know your key stroke was registered when typing in numbers on the number-pad quickly.  [Here is a good one](https://www.amazon.com/Corsair-Gaming-Mechanical-Keyboard-Backlit/dp/B01ER4B8S2/ref=sr_1_3?ie=UTF8&amp;qid=1508200718&amp;sr=8-3&amp;keywords=cherry+mx+blue). You can turn off the led back-lighting.  Search for \"\"Cherry MX Blue\"\" in Amazon if you want to see alternatives. Note: Cherry is a brand of keyboard switch manufacturer.\"'),\n",
       " Document(id='2f274303-390f-4f72-b50d-8eaaa900dff7', metadata={'corpus_id': 322638}, page_content='I disagree. Back in the day I hated any single button on the keyboard that automatically pulled up a new window/menu. The Windows Key is an example of something that was always getting in the way. A two key combo would have made more sense, but not a single key.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_large_chunk.similarity_search(\n",
    "    \"hit key\",\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfbc2d",
   "metadata": {},
   "source": [
    "### Register Custom Ray Serializers for PGVector\n",
    "\n",
    "`PGVector` holds a live `psycopg` database connection internally, which contains a `weakref` that Ray cannot pickle. Rather than serializing the connection object, we register custom serializers that capture only the primitives needed to reconstruct it (connection string, collection name, embeddings config). The deserializer re-runs the same `PGVector(...)` construction on each Ray worker, opening a fresh connection to the shared Postgres server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e97ba8-9c9f-4fa1-b973-89405e2e2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import copy\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def _pgvector_connection_string(obj) -> str:\n",
    "    # str(url) masks the password with '***' ‚Äî render_as_string preserves it\n",
    "    return obj._engine.url.render_as_string(hide_password=False)\n",
    "\n",
    "def _pgvector_primitives(obj) -> dict:\n",
    "    return {\n",
    "        \"connection\": _pgvector_connection_string(obj),\n",
    "        \"collection_name\": obj.collection_name,\n",
    "        \"embeddings\": obj.embedding_function,\n",
    "        \"use_jsonb\": obj.use_jsonb,\n",
    "    }\n",
    "\n",
    "# --- deepcopy patch ---\n",
    "# RapidFire calls copy.deepcopy(kwargs) at init time, which hits PGVector's\n",
    "# non-copyable SQLAlchemy internals. __deepcopy__ intercepts this and opens\n",
    "# a fresh connection instead of copying the live one.\n",
    "def _pgvector_deepcopy(self, memo):\n",
    "    return PGVector(**_pgvector_primitives(self))\n",
    "\n",
    "PGVector.__deepcopy__ = _pgvector_deepcopy\n",
    "\n",
    "# --- Ray serializer ---\n",
    "# Ray serializes objects when shipping them to workers. Same fix as deepcopy:\n",
    "# send only the primitives, reconstruct on the worker side.\n",
    "ray.util.register_serializer(\n",
    "    PGVector,\n",
    "    serializer=_pgvector_primitives,\n",
    "    deserializer=lambda data: PGVector(**data),\n",
    ")\n",
    "\n",
    "# HuggingFaceEmbeddings serializer (usually picklable, but included for safety)\n",
    "def _hf_primitives(obj) -> dict:\n",
    "    return {\n",
    "        \"model_name\": obj.model_name,\n",
    "        \"model_kwargs\": obj.model_kwargs,\n",
    "        \"encode_kwargs\": obj.encode_kwargs,\n",
    "    }\n",
    "\n",
    "ray.util.register_serializer(\n",
    "    HuggingFaceEmbeddings,\n",
    "    serializer=_hf_primitives,\n",
    "    deserializer=lambda data: HuggingFaceEmbeddings(**data),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a21ee",
   "metadata": {},
   "source": [
    "### Define Partial Multi-Config Knobs for LangChain part of RAG Pipeline using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e27db3-9345-456e-a43e-36ab095fa6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Checking Serializability of <langchain_postgres.vectorstores.PGVector object at 0x7a83bc02c530>\n",
      "================================================================================\n",
      "================================================================================\n",
      "Checking Serializability of <langchain_postgres.vectorstores.PGVector object at 0x7a835c32ac60>\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, set())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.util import inspect_serializability\n",
    "\n",
    "inspect_serializability(vector_store_large_chunk, name=\"test\")\n",
    "inspect_serializability(vector_store_small_chunk, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521fe6e4-4c2b-4694-a616-c75d8beb11e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4029294572.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m1. Create VS from scratch\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. Create VS from scratch (FAISS) [Indexing + retrieval]\n",
    "2. Using pre built VS (zenon) [retrieval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# Per-Actor batch size for hardware efficiency\n",
    "batch_size = 128\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "RFExternalStore(\n",
    "    class\n",
    "    connection\n",
    "    collection_name\n",
    "    embedding_config\n",
    ")\n",
    "\n",
    "{\n",
    "    class: PGVector / FAISS / PineCone\n",
    "    connection [optional]\n",
    "}\n",
    "    \n",
    "# 2 chunk sizes x 2 reranking top-n = 4 combinations in total\n",
    "rag_gpu = RFLangChainRagSpec(\n",
    "    document_loader=None,\n",
    "    text_splitter=None,\n",
    "    embedding_cfg={\n",
    "        \"class\": HuggingFaceEmbeddings\n",
    "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "        \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size}\n",
    "    },\n",
    "    # vector_store={class: FAISS}\n",
    "    vector_store=RFPGVector(\n",
    "        \"connection\": connection,\n",
    "        \"collection_name\": List([\"fiqa_chunk64\", \"fiqa_chunk256\"])\n",
    "        \"index_name\":\n",
    "        \"embedding_cfg\": \n",
    "    ),\n",
    "    search_cfg={\n",
    "        \"type\": \"similarity\",\n",
    "        \"k\": 15\n",
    "    },\n",
    "    # 2 reranking strategies with different top-n values\n",
    "    reranker_cfg={\n",
    "        \"class\": CrossEncoderReranker,\n",
    "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "        \"top_n\": List([2, 5]),\n",
    "    },\n",
    "    enable_gpu_search=True,  # GPU-based exact search instead of ANN index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fb0a8",
   "metadata": {},
   "source": [
    "### Define Data Processing and Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc17276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_preprocess_fn(\n",
    "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
    ") -> Dict[str, listtype]:\n",
    "    \"\"\"Function to prepare the final inputs given to the generator model\"\"\"\n",
    "\n",
    "    INSTRUCTIONS = \"Utilize your financial knowledge, give your answer or opinion to the input question or subject matter.\"\n",
    "\n",
    "    # Perform batched retrieval over all queries; returns a list of lists of k documents per query\n",
    "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
    "\n",
    "    # Extract the retrieved document ids from the context\n",
    "    retrieved_documents = [\n",
    "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
    "    ]\n",
    "\n",
    "    # Serialize the retrieved documents into a single string per query using the default template\n",
    "    serialized_context = rag.serialize_documents(all_context)\n",
    "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
    "\n",
    "    # Each batch to contain conversational prompt, retrieved documents, and original 'query_id', 'query', 'metadata'\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Here is some relevant context:\\n{context}. \\nNow answer the following question using the context provided earlier:\\n{question}\",\n",
    "                },\n",
    "            ]\n",
    "            for question, context in zip(batch[\"query\"], serialized_context)\n",
    "        ],\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
    "    \"\"\"Function to postprocess outputs produced by generator model\"\"\"\n",
    "    # Get ground truth documents for each query; can be done in preprocess_fn too but done here for clarity\n",
    "    batch[\"ground_truth_documents\"] = [\n",
    "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
    "        for query_id in batch[\"query_id\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Define Custom Eval Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=5):\n",
    "    \"\"\"Utility function to compute NDCG@k\"\"\"\n",
    "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    # IDCG: perfect ranking limited by min(k, len(expected_docs))\n",
    "    ideal_length = min(k, len(expected_docs))\n",
    "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
    "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
    "    \"\"\"Utility function to compute Reciprocal Rank (RR) for a single query\"\"\"\n",
    "    rr = 0\n",
    "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
    "        if retrieved_doc in expected_docs:\n",
    "            rr = 1 / (i + 1)\n",
    "            break\n",
    "    return rr\n",
    "\n",
    "\n",
    "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Function to compute all eval metrics based on retrievals and/or generations\"\"\"\n",
    "\n",
    "    true_positives, precisions, recalls, f1_scores, ndcgs, rrs = 0, [], [], [], [], []\n",
    "    total_queries = len(batch[\"query\"])\n",
    "\n",
    "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
    "        expected_set = set(gt)\n",
    "        retrieved_set = set(pred)\n",
    "\n",
    "        true_positives = len(expected_set.intersection(retrieved_set))\n",
    "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
    "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=5))\n",
    "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
    "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
    "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
    "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
    "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_accumulate_metrics_fn(\n",
    "    aggregated_metrics: Dict[str, listtype],\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Function to accumulate eval metrics across all batches\"\"\"\n",
    "\n",
    "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
    "    total_queries = sum(num_queries_per_batch)\n",
    "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        **{\n",
    "            metric: {\n",
    "                \"value\": sum(\n",
    "                    m[\"value\"] * queries\n",
    "                    for m, queries in zip(\n",
    "                        aggregated_metrics[metric], num_queries_per_batch\n",
    "                    )\n",
    "                )\n",
    "                / total_queries,\n",
    "                \"is_algebraic\": True,\n",
    "                \"value_range\": (0, 1),\n",
    "            }\n",
    "            for metric in algebraic_metrics\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57887bc",
   "metadata": {},
   "source": [
    "### Define Partial Multi-Config Knobs for vLLM Generator part of RAG Pipeline using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f5d0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 vLLM generator configs with different sizes of generator models\n",
    "\n",
    "vllm_config1 = RFvLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": False,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    "    rag=rag_gpu,\n",
    "    prompt_manager=None,\n",
    ")\n",
    "\n",
    "vllm_config2 = RFvLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": False,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    "    rag=rag_gpu,\n",
    "    prompt_manager=None,\n",
    ")\n",
    "\n",
    "config_set = {\n",
    "    \"vllm_config\": List([vllm_config1, vllm_config2]),  # Each represents 4 configs\n",
    "    \"batch_size\": batch_size,\n",
    "    \"preprocess_fn\": sample_preprocess_fn,\n",
    "    \"postprocess_fn\": sample_postprocess_fn,\n",
    "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
    "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
    "    \"online_strategy_kwargs\": {\n",
    "        \"strategy_name\": \"normal\",\n",
    "        \"confidence_level\": 0.95,\n",
    "        \"use_fpc\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7dd280",
   "metadata": {},
   "source": [
    "### Create Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f26d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 8 combinations in total\n",
    "config_group = RFGridSearch(config_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Multi-Config Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"controller_95c87ec6\" style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto;\">\n",
       "            <style>\n",
       "                #controller_95c87ec6 h3 { margin: 10px 0; font-size: 1.2em; font-weight: 600; }\n",
       "                #controller_95c87ec6 .header-info { display: flex; gap: 20px; margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 4px; font-size: 13px; }\n",
       "                #controller_95c87ec6 .section { margin: 15px 0; }\n",
       "                #controller_95c87ec6 .section-label { font-weight: 600; margin-bottom: 8px; font-size: 14px; }\n",
       "                #controller_95c87ec6 .button-row { display: flex; gap: 8px; flex-wrap: wrap; margin: 10px 0; }\n",
       "                #controller_95c87ec6 select { padding: 6px 12px; border: 1px solid #ccc; border-radius: 4px; font-size: 13px; background: white; min-width: 300px; cursor: pointer; }\n",
       "                #controller_95c87ec6 button { padding: 6px 16px; border: none; border-radius: 4px; font-size: 13px; font-weight: 500; cursor: pointer; }\n",
       "                #controller_95c87ec6 button:disabled { opacity: 0.5; cursor: not-allowed; }\n",
       "                #controller_95c87ec6 .btn-success { background: #28a745; color: white; }\n",
       "                #controller_95c87ec6 .btn-danger { background: #dc3545; color: white; }\n",
       "                #controller_95c87ec6 .btn-info { background: #17a2b8; color: white; }\n",
       "                #controller_95c87ec6 .btn-default { background: #6c757d; color: white; }\n",
       "                #controller_95c87ec6 textarea { width: 100%; min-height: 200px; padding: 10px; border: 1px solid #ccc; border-radius: 4px; font-family: 'Courier New', monospace; font-size: 12px; box-sizing: border-box; }\n",
       "                #controller_95c87ec6 .status-message { padding: 10px; margin: 10px 0; border-radius: 4px; display: none; }\n",
       "                #controller_95c87ec6 .msg-success { background: #d4edda; color: #155724; }\n",
       "                #controller_95c87ec6 .msg-error { background: #f8d7da; color: #721c24; }\n",
       "                #controller_95c87ec6 .msg-info { background: #d1ecf1; color: #0c5460; }\n",
       "            </style>\n",
       "\n",
       "            <div>\n",
       "                <h3>Interactive Run Controller</h3>\n",
       "                <div class=\"header-info\">\n",
       "                    <div><b>Run ID:</b> <span id=\"pipeline-id-value\">N/A</span></div>\n",
       "                    <div><b>Status:</b> <span id=\"status-value\">Not loaded</span></div>\n",
       "                    <div><b>Last Update:</b> <span id=\"last-update\">Never</span></div>\n",
       "                </div>\n",
       "\n",
       "                <div id=\"status-message\" class=\"status-message\"></div>\n",
       "\n",
       "                <div class=\"section\">\n",
       "                    <div class=\"section-label\">Select a Config ID:</div>\n",
       "                    <select id=\"pipeline-selector\">\n",
       "                        <option value=\"\">Waiting for data...</option>\n",
       "                    </select>\n",
       "                </div>\n",
       "\n",
       "                <div class=\"section\">\n",
       "                    <div class=\"button-row\">\n",
       "                        <button class=\"btn-success\" id=\"resume-btn\">‚ñ∂ Resume</button>\n",
       "                        <button class=\"btn-danger\" id=\"stop-btn\">‚ñ† Stop</button>\n",
       "                        <button class=\"btn-danger\" id=\"delete-btn\">üóë Delete</button>\n",
       "                    </div>\n",
       "                </div>\n",
       "\n",
       "                <div class=\"section\">\n",
       "                    <div class=\"section-label\">Configuration: <span id=\"config-name\">N/A</span></div>\n",
       "                    <textarea id=\"config-text\" readonly>{}</textarea>\n",
       "                    <div class=\"button-row\">\n",
       "                        <button class=\"btn-info\" id=\"clone-btn\">Clone Run</button>\n",
       "                        <button class=\"btn-success\" id=\"submit-clone-btn\" disabled>‚úì Submit Clone</button>\n",
       "                        <button class=\"btn-danger\" id=\"cancel-clone-btn\" disabled>‚úó Cancel</button>\n",
       "                    </div>\n",
       "                </div>\n",
       "            </div>\n",
       "\n",
       "            <script>\n",
       "                (function() {\n",
       "                    const WIDGET_ID = 'controller_95c87ec6';\n",
       "                    const DISPATCHER_URL = 'http://127.0.0.1:8851';\n",
       "                    let currentPipelineId = null;\n",
       "                    let currentConfig = null;\n",
       "                    let currentContextId = null;\n",
       "                    let isCloneMode = false;\n",
       "                    let pollingInterval = null;\n",
       "\n",
       "                    // Elements\n",
       "                    const el = {\n",
       "                        pipelineIdValue: document.getElementById('pipeline-id-value'),\n",
       "                        statusValue: document.getElementById('status-value'),\n",
       "                        lastUpdate: document.getElementById('last-update'),\n",
       "                        statusMessage: document.getElementById('status-message'),\n",
       "                        pipelineSelector: document.getElementById('pipeline-selector'),\n",
       "                        resumeBtn: document.getElementById('resume-btn'),\n",
       "                        stopBtn: document.getElementById('stop-btn'),\n",
       "                        deleteBtn: document.getElementById('delete-btn'),\n",
       "                        configName: document.getElementById('config-name'),\n",
       "                        configText: document.getElementById('config-text'),\n",
       "                        cloneBtn: document.getElementById('clone-btn'),\n",
       "                        submitCloneBtn: document.getElementById('submit-clone-btn'),\n",
       "                        cancelCloneBtn: document.getElementById('cancel-clone-btn')\n",
       "                    };\n",
       "\n",
       "                    // Use fetch API with explicit CORS mode and optional auth token\n",
       "                    async function xhrRequest(url, method = 'GET', body = null) {\n",
       "                        const options = {\n",
       "                            method: method,\n",
       "                            headers: {\n",
       "                                'Content-Type': 'application/json'\n",
       "                            },\n",
       "                            mode: 'cors',\n",
       "                            credentials: 'omit'  // Include cookies for Colab proxy auth\n",
       "                        };\n",
       "\n",
       "                        if (body) {\n",
       "                            options.body = JSON.stringify(body);\n",
       "                        }\n",
       "\n",
       "                        const response = await fetch(url, options);\n",
       "                        if (!response.ok) {\n",
       "                            throw new Error('HTTP ' + response.status);\n",
       "                        }\n",
       "                        return await response.json();\n",
       "                    }\n",
       "\n",
       "                    async function fetchPipelines() {\n",
       "                        try {\n",
       "                            console.log('Fetching pipelines...');\n",
       "                            const pipelines = await xhrRequest(DISPATCHER_URL + '/dispatcher/list-all-pipeline-ids');\n",
       "                            console.log('Got pipelines:', pipelines.length);\n",
       "\n",
       "                            updatePipelinesDropdown(pipelines);\n",
       "                            el.lastUpdate.textContent = new Date().toLocaleTimeString();\n",
       "\n",
       "                        } catch (error) {\n",
       "                            console.error('Failed to fetch pipelines:', error);\n",
       "                            showMessage('Connection error: ' + error.message, 'error');\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    async function fetchPipelineConfig(pipelineId) {\n",
       "                        try {\n",
       "                            const data = await xhrRequest(DISPATCHER_URL + `/dispatcher/get-pipeline-config-json/${pipelineId}`);\n",
       "                            const config = data.pipeline_config_json || {};\n",
       "\n",
       "                            currentConfig = config;\n",
       "                            currentContextId = data.context_id;\n",
       "\n",
       "                            el.configName.textContent = config.pipeline_name || 'N/A';\n",
       "\n",
       "                            if (!isCloneMode) {\n",
       "                                el.configText.value = JSON.stringify(config, null, 2);\n",
       "                            }\n",
       "\n",
       "                        } catch (error) {\n",
       "                            console.error('Failed to fetch config:', error);\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    function updatePipelinesDropdown(pipelines) {\n",
       "                        const selector = el.pipelineSelector;\n",
       "                        const currentSelection = selector.value;\n",
       "\n",
       "                        selector.innerHTML = '';\n",
       "\n",
       "                        if (pipelines && pipelines.length > 0) {\n",
       "                            pipelines.forEach(p => {\n",
       "                                const option = document.createElement('option');\n",
       "                                option.value = p.pipeline_id;\n",
       "                                option.textContent = `Config ID: ${p.pipeline_id} (${p.status || 'unknown'})`;\n",
       "                                selector.appendChild(option);\n",
       "                            });\n",
       "\n",
       "                            if (currentSelection && pipelines.some(p => p.pipeline_id == currentSelection)) {\n",
       "                                selector.value = currentSelection;\n",
       "                                currentPipelineId = currentSelection;\n",
       "                            } else {\n",
       "                                selector.value = pipelines[0].pipeline_id;\n",
       "                                currentPipelineId = pipelines[0].pipeline_id;\n",
       "                                fetchPipelineConfig(currentPipelineId);\n",
       "                            }\n",
       "\n",
       "                            // Update status display\n",
       "                            const currentPipeline = pipelines.find(p => p.pipeline_id == currentPipelineId);\n",
       "                            if (currentPipeline) {\n",
       "                                el.pipelineIdValue.textContent = currentPipeline.pipeline_id;\n",
       "                                el.statusValue.textContent = currentPipeline.status || 'unknown';\n",
       "\n",
       "                                const isCompleted = currentPipeline.status?.toLowerCase() === 'completed';\n",
       "                                el.resumeBtn.disabled = isCompleted;\n",
       "                                el.stopBtn.disabled = isCompleted;\n",
       "                                el.deleteBtn.disabled = isCompleted;\n",
       "                                el.cloneBtn.disabled = isCompleted || !currentContextId;\n",
       "                            }\n",
       "                        } else {\n",
       "                            selector.innerHTML = '<option value=\"\">No pipelines found</option>';\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    function showMessage(message, type) {\n",
       "                        el.statusMessage.className = 'status-message msg-' + type;\n",
       "                        el.statusMessage.textContent = message;\n",
       "                        el.statusMessage.style.display = 'block';\n",
       "                        setTimeout(() => el.statusMessage.style.display = 'none', 5000);\n",
       "                    }\n",
       "\n",
       "                    async function handleAction(action) {\n",
       "                        if (!currentPipelineId) {\n",
       "                            showMessage('No pipeline selected', 'error');\n",
       "                            return;\n",
       "                        }\n",
       "\n",
       "                        try {\n",
       "                            const endpoint = DISPATCHER_URL + `/dispatcher/${action}-pipeline`;\n",
       "                            const result = await xhrRequest(endpoint, 'POST', { pipeline_id: currentPipelineId });\n",
       "\n",
       "                            showMessage(`‚úì ${action} completed for pipeline ${currentPipelineId}`, 'success');\n",
       "\n",
       "                            // Refresh after a short delay\n",
       "                            setTimeout(async () => {\n",
       "                                await fetchPipelines();\n",
       "                            }, 500);\n",
       "\n",
       "                        } catch (error) {\n",
       "                            showMessage(`Error: ${error.message}`, 'error');\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    function enableCloneMode() {\n",
       "                        isCloneMode = true;\n",
       "                        el.configText.readOnly = false;\n",
       "                        el.submitCloneBtn.disabled = false;\n",
       "                        el.cancelCloneBtn.disabled = false;\n",
       "                        el.cloneBtn.disabled = true;\n",
       "                        showMessage('Edit config and click Submit to clone', 'info');\n",
       "                    }\n",
       "\n",
       "                    function disableCloneMode() {\n",
       "                        isCloneMode = false;\n",
       "                        el.configText.readOnly = true;\n",
       "                        el.configText.value = JSON.stringify(currentConfig || {}, null, 2);\n",
       "                        el.submitCloneBtn.disabled = true;\n",
       "                        el.cancelCloneBtn.disabled = true;\n",
       "                        el.cloneBtn.disabled = false;\n",
       "                    }\n",
       "\n",
       "                    async function handleClone() {\n",
       "                        if (!currentPipelineId) {\n",
       "                            showMessage('No pipeline selected', 'error');\n",
       "                            return;\n",
       "                        }\n",
       "\n",
       "                        try {\n",
       "                            // Parse edited config\n",
       "                            let editedConfig;\n",
       "                            try {\n",
       "                                editedConfig = JSON.parse(el.configText.value);\n",
       "                            } catch (e) {\n",
       "                                showMessage('Invalid JSON: ' + e.message, 'error');\n",
       "                                return;\n",
       "                            }\n",
       "\n",
       "                            // Validate required fields\n",
       "                            if (!editedConfig.pipeline_type) {\n",
       "                                showMessage('config_json must include pipeline_type', 'error');\n",
       "                                return;\n",
       "                            }\n",
       "\n",
       "                            // Send clone request\n",
       "                            const cloneRequest = {\n",
       "                                parent_pipeline_id: currentPipelineId,\n",
       "                                config_json: editedConfig\n",
       "                            };\n",
       "\n",
       "                            const result = await xhrRequest(\n",
       "                                DISPATCHER_URL + '/dispatcher/clone-pipeline',\n",
       "                                'POST',\n",
       "                                cloneRequest\n",
       "                            );\n",
       "\n",
       "                            showMessage(`‚úì Cloned from Config ID ${currentPipelineId} successfully!`, 'success');\n",
       "                            disableCloneMode();\n",
       "\n",
       "                            // Refresh after delay\n",
       "                            setTimeout(async () => {\n",
       "                                await fetchPipelines();\n",
       "                            }, 1000);\n",
       "\n",
       "                        } catch (error) {\n",
       "                            showMessage(`Error cloning: ${error.message}`, 'error');\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                    // Event listeners\n",
       "                    el.pipelineSelector.addEventListener('change', (e) => {\n",
       "                        if (e.target.value) {\n",
       "                            currentPipelineId = parseInt(e.target.value);\n",
       "                            fetchPipelineConfig(currentPipelineId);\n",
       "                        }\n",
       "                    });\n",
       "\n",
       "                    el.resumeBtn.addEventListener('click', () => handleAction('resume'));\n",
       "                    el.stopBtn.addEventListener('click', () => handleAction('stop'));\n",
       "                    el.deleteBtn.addEventListener('click', () => handleAction('delete'));\n",
       "\n",
       "                    el.cloneBtn.addEventListener('click', enableCloneMode);\n",
       "                    el.submitCloneBtn.addEventListener('click', handleClone);\n",
       "                    el.cancelCloneBtn.addEventListener('click', () => {\n",
       "                        disableCloneMode();\n",
       "                        showMessage('Cancelled clone', 'info');\n",
       "                    });\n",
       "\n",
       "                    // Initial fetch\n",
       "                    console.log('UI initialized, fetching initial data...');\n",
       "                    setTimeout(async () => {\n",
       "                        await fetchPipelines();\n",
       "\n",
       "                        // Start polling - use HTTP fetch (works even when kernel is busy)\n",
       "                        pollingInterval = setInterval(async () => {\n",
       "                            await fetchPipelines();\n",
       "                        }, 3000.0);\n",
       "                        console.log('Polling started: every 3.0s');\n",
       "                    }, 1000);\n",
       "\n",
       "                    // Cleanup on unload\n",
       "                    window.addEventListener('beforeunload', () => {\n",
       "                        if (pollingInterval) clearInterval(pollingInterval);\n",
       "                    });\n",
       "                })();\n",
       "            </script>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preprocessing RAG Sources ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_62b4b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_62b4b_level0_col0\" class=\"col_heading level0 col0\" >RAG Source ID</th>\n",
       "      <th id=\"T_62b4b_level0_col1\" class=\"col_heading level0 col1\" >Status</th>\n",
       "      <th id=\"T_62b4b_level0_col2\" class=\"col_heading level0 col2\" >Duration</th>\n",
       "      <th id=\"T_62b4b_level0_col3\" class=\"col_heading level0 col3\" >Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_62b4b_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_62b4b_row0_col1\" class=\"data row0 col1\" >Failed</td>\n",
       "      <td id=\"T_62b4b_row0_col2\" class=\"data row0 col2\" >16.7s</td>\n",
       "      <td id=\"T_62b4b_row0_col3\" class=\"data row0 col3\" >FAISS, GPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a75633dfa70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚ùå CRITICAL ERROR: RAG Source Preprocessing Failed\n",
      "================================================================================\n",
      "RAG Source ID: 1\n",
      "Context Hash: f3e0b040461b9df6...\n",
      "Error: ray::DocProcessingActor.build_rag_components() (pid=190166, ip=10.128.0.47, actor_id=93ee1c8446e91187d5b26e6801000000, repr=<rapidfireai.evals.actors.doc_actor.DocProcessingActor object at 0x7debed3e2cc0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/miniconda3/envs/update/lib/python3.12/site-packages/ray/cloudpickle/cloudpickle.py\", line 1479, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/ubuntu/miniconda3/envs/update/lib/python3.12/site-packages/ray/cloudpickle/cloudpickle.py\", line 1245, in dump\n",
      "    return super().dump(obj)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "TypeError: cannot pickle 'weakref.ReferenceType' object\n",
      "================================================================================\n",
      "\n",
      "The experiment has been halted. Please fix the error and try again.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Context creation failed for context 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(TypeError)\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code_update/rapidfireai/rapidfireai/evals/scheduling/controller.py:449\u001b[39m, in \u001b[36mController.build_rag_components\u001b[39m\u001b[34m(self, contexts_to_build, db)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# Wait for this specific build to complete\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     components = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfuture\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/update/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:22\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/update/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/update/lib/python3.12/site-packages/ray/_private/worker.py:2981\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(object_refs, timeout, _use_object_store)\u001b[39m\n\u001b[32m   2976\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2977\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, is given. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2978\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobject_refs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2979\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2981\u001b[39m values, debugger_breakpoint = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_object_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_use_object_store\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/update/lib/python3.12/site-packages/ray/_private/worker.py:1012\u001b[39m, in \u001b[36mWorker.get_objects\u001b[39m\u001b[34m(self, object_refs, timeout, return_exceptions, skip_deserialization, use_object_store)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.as_instanceof_cause()\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRayTaskError(TypeError)\u001b[39m: \u001b[36mray::DocProcessingActor.build_rag_components()\u001b[39m (pid=190166, ip=10.128.0.47, actor_id=93ee1c8446e91187d5b26e6801000000, repr=<rapidfireai.evals.actors.doc_actor.DocProcessingActor object at 0x7debed3e2cc0>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n             ^^^^^^^^^^^^^\n  File \"/home/ubuntu/miniconda3/envs/update/lib/python3.12/site-packages/ray/cloudpickle/cloudpickle.py\", line 1479, in dumps\n    cp.dump(obj)\n  File \"/home/ubuntu/miniconda3/envs/update/lib/python3.12/site-packages/ray/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\nTypeError: cannot pickle 'weakref.ReferenceType' object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# num_actors: set to the number of GPUs you want to use for inference.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_evals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiqa_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_actors\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# If not set, auto-detects and uses all available GPUs.\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code_update/rapidfireai/rapidfireai/experiment.py:411\u001b[39m, in \u001b[36mExperiment.run_evals\u001b[39m\u001b[34m(self, config_group, dataset, num_shards, seed, num_actors, gpus_per_actor, cpus_per_actor)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Delegate all complexity to Controller\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_multi_pipeline_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_actors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_actors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpus_per_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpus_per_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.exception(\u001b[33m\"\u001b[39m\u001b[33mError running multi-config experiment\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code_update/rapidfireai/rapidfireai/evals/scheduling/controller.py:882\u001b[39m, in \u001b[36mController.run_multi_pipeline_inference\u001b[39m\u001b[34m(self, experiment_id, config_group, dataset, num_shards, seed, num_actors, num_gpus, num_cpus)\u001b[39m\n\u001b[32m    879\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(config_leaves)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pipeline configuration(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    881\u001b[39m \u001b[38;5;66;03m# PHASE 3: Setup context generators (collect unique, check DB, build if needed)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_context_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_leaves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[38;5;66;03m# PHASE 4: Create query processing actors (shared pool)\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Actors are created without any pipeline or context information\u001b[39;00m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# They will receive pipeline-specific context when scheduled\u001b[39;00m\n\u001b[32m    887\u001b[39m query_actors = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code_update/rapidfireai/rapidfireai/evals/scheduling/controller.py:336\u001b[39m, in \u001b[36mController._setup_context_generators\u001b[39m\u001b[34m(self, config_leaves, db)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBuilding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(contexts_to_build)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m context(s) in parallel...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_rag_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts_to_build\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.exception(\u001b[33m\"\u001b[39m\u001b[33mFailed to build contexts in parallel\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code_update/rapidfireai/rapidfireai/evals/scheduling/controller.py:491\u001b[39m, in \u001b[36mController.build_rag_components\u001b[39m\u001b[34m(self, contexts_to_build, db)\u001b[39m\n\u001b[32m    480\u001b[39m     error_message = (\n\u001b[32m    481\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå CRITICAL ERROR: RAG Source Preprocessing Failed\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThe experiment has been halted. Please fix the error and try again.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    489\u001b[39m     )\n\u001b[32m    490\u001b[39m     \u001b[38;5;28mprint\u001b[39m(error_message)\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContext creation failed for context \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;66;03m# Clean up DocProcessingActor\u001b[39;00m\n\u001b[32m    495\u001b[39m     ray.kill(task[\u001b[33m\"\u001b[39m\u001b[33mactor\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mRuntimeError\u001b[39m: Context creation failed for context 1"
     ]
    }
   ],
   "source": [
    "# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\n",
    "# num_actors: set to the number of GPUs you want to use for inference.\n",
    "results = experiment.run_evals(\n",
    "    config_group=config_group,\n",
    "    dataset=fiqa_dataset,\n",
    "    num_actors=4, # If not set, auto-detects and uses all available GPUs.\n",
    "    num_shards=4,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127e7b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>search_type</th>\n",
       "      <th>rag_k</th>\n",
       "      <th>top_n</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>sampling_params</th>\n",
       "      <th>model_config</th>\n",
       "      <th>Samples Processed</th>\n",
       "      <th>Processing Time</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Total</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1363.58 seconds</td>\n",
       "      <td>0.37</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1305.53 seconds</td>\n",
       "      <td>0.38</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1259.23 seconds</td>\n",
       "      <td>0.40</td>\n",
       "      <td>500</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1255.08 seconds</td>\n",
       "      <td>0.40</td>\n",
       "      <td>500</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1209.11 seconds</td>\n",
       "      <td>0.41</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1149.83 seconds</td>\n",
       "      <td>0.43</td>\n",
       "      <td>500</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.245213</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1089.76 seconds</td>\n",
       "      <td>0.46</td>\n",
       "      <td>500</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
       "      <td>similarity</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
       "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
       "      <td>500</td>\n",
       "      <td>1085.33 seconds</td>\n",
       "      <td>0.46</td>\n",
       "      <td>500</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_id                  model_name search_type  rag_k  top_n  chunk_size  \\\n",
       "0       1  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      2         256   \n",
       "1       2  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      5         256   \n",
       "2       3  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      2         128   \n",
       "3       4  Qwen/Qwen2.5-0.5B-Instruct  similarity     15      5         128   \n",
       "4       5    Qwen/Qwen2.5-3B-Instruct  similarity     15      2         256   \n",
       "5       6    Qwen/Qwen2.5-3B-Instruct  similarity     15      5         256   \n",
       "6       7    Qwen/Qwen2.5-3B-Instruct  similarity     15      2         128   \n",
       "7       8    Qwen/Qwen2.5-3B-Instruct  similarity     15      5         128   \n",
       "\n",
       "   chunk_overlap                                    sampling_params  \\\n",
       "0             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "1             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "2             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "3             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "4             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "5             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "6             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "7             32  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
       "\n",
       "                                        model_config  Samples Processed  \\\n",
       "0  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "1  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "2  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "3  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "4  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "5  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "6  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "7  {'dtype': 'half', 'gpu_memory_utilization': 0....                500   \n",
       "\n",
       "   Processing Time Samples Per Second  Total  Precision    Recall  F1 Score  \\\n",
       "0  1363.58 seconds               0.37    500      0.237  0.298892  0.245213   \n",
       "1  1305.53 seconds               0.38    500      0.237  0.298892  0.245213   \n",
       "2  1259.23 seconds               0.40    500      0.243  0.280844  0.241424   \n",
       "3  1255.08 seconds               0.40    500      0.243  0.280844  0.241424   \n",
       "4  1209.11 seconds               0.41    500      0.237  0.298892  0.245213   \n",
       "5  1149.83 seconds               0.43    500      0.237  0.298892  0.245213   \n",
       "6  1089.76 seconds               0.46    500      0.243  0.280844  0.241424   \n",
       "7  1085.33 seconds               0.46    500      0.243  0.280844  0.241424   \n",
       "\n",
       "     NDCG@5    MRR  \n",
       "0  0.091532  0.328  \n",
       "1  0.091532  0.328  \n",
       "2  0.087110  0.318  \n",
       "3  0.087110  0.318  \n",
       "4  0.091532  0.328  \n",
       "5  0.091532  0.328  \n",
       "6  0.087110  0.318  \n",
       "7  0.087110  0.318  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results dict to DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
    "    for run_id, (_, metrics_dict) in results.items()\n",
    "])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment exp1-fiqa-rag_8 ended\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09265e66",
   "metadata": {},
   "source": [
    "### View RapidFire AI Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05379a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Log File: /home/ubuntu/rapidfireai/logs/exp1-fiqa-rag_7/rapidfire.log\n",
      "\n",
      "================================================================================\n",
      "Last 30 lines of rapidfire.log:\n",
      "================================================================================\n",
      "2026-02-19 02:44:10 | QueryProcessingActor-2 | INFO | query_actor.py:191 | [exp1-fiqa-rag_7:QueryProcessingActor-2] Deserializing FAISS index for this actor...\n",
      "2026-02-19 02:44:11 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2026-02-19 02:44:12 | QueryProcessingActor-2 | INFO | query_actor.py:212 | [exp1-fiqa-rag_7:QueryProcessingActor-2] Recreated embedding function: HuggingFaceEmbeddings\n",
      "2026-02-19 02:44:12 | QueryProcessingActor-2 | INFO | query_actor.py:221 | [exp1-fiqa-rag_7:QueryProcessingActor-2] Created independent FAISS vector store for this actor\n",
      "2026-02-19 02:44:12 | QueryProcessingActor-2 | INFO | query_actor.py:228 | [exp1-fiqa-rag_7:QueryProcessingActor-2] Recreated retriever with search_type=similarity\n",
      "2026-02-19 02:44:12 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:219 | Use pytorch device_name: cuda:0\n",
      "2026-02-19 02:44:12 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:227 | Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2026-02-19 02:44:14 | sentence_transformers.cross_encoder.CrossEncoder | INFO | CrossEncoder.py:228 | Use pytorch device: cuda:0\n",
      "2026-02-19 02:44:14 | QueryProcessingActor-2 | INFO | query_actor.py:249 | [exp1-fiqa-rag_7:QueryProcessingActor-2] Recreated RAG spec with retriever and template\n",
      "2026-02-19 02:44:14 | Controller | INFO | controller.py:1145 | [exp1-fiqa-rag_7:Controller] Pipeline 6 completed all 4 shards\n",
      "2026-02-19 02:44:14 | Controller | INFO | controller.py:1250 | [exp1-fiqa-rag_7:Controller] Pipeline 6 completed shard 3 (1 batches, 65.64s)\n",
      "2026-02-19 02:44:14 | Controller | INFO | controller.py:1145 | [exp1-fiqa-rag_7:Controller] Pipeline 7 completed all 4 shards\n",
      "2026-02-19 02:44:14 | Controller | INFO | controller.py:1250 | [exp1-fiqa-rag_7:Controller] Pipeline 7 completed shard 3 (1 batches, 61.18s)\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:1145 | [exp1-fiqa-rag_7:Controller] Pipeline 8 completed all 4 shards\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:1250 | [exp1-fiqa-rag_7:Controller] Pipeline 8 completed shard 3 (1 batches, 27.55s)\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:1317 | [exp1-fiqa-rag_7:Controller] All pipelines completed all shards!\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:672 | [exp1-fiqa-rag_7:Controller] Computing final metrics for all pipelines...\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 1 (Pipeline 1) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 2 (Pipeline 2) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 3 (Pipeline 3) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 4 (Pipeline 4) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 5 (Pipeline 5) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 6 (Pipeline 6) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 7 (Pipeline 7) completed successfully\n",
      "2026-02-19 02:44:42 | Controller | INFO | controller.py:832 | [exp1-fiqa-rag_7:Controller] Pipeline 8 (Pipeline 8) completed successfully\n",
      "2026-02-19 02:44:42 | ExperimentUtils | INFO | experiment_utils.py:180 | [exp1-fiqa-rag_7:ExperimentUtils] Reset experiment states - marked ongoing pipelines, contexts, and tasks as failed\n",
      "2026-02-19 02:44:42 | ExperimentUtils | INFO | experiment_utils.py:188 | [exp1-fiqa-rag_7:ExperimentUtils] Experiment marked as cancelled. Ongoing pipelines, contexts, and tasks have been marked as failed.\n",
      "2026-02-19 02:44:42 | ExperimentUtils | INFO | experiment_utils.py:154 | [exp1-fiqa-rag_7:ExperimentUtils] Experiment exp1-fiqa-rag_7 ended\n",
      "2026-02-19 02:44:43 | Experiment | INFO | experiment.py:573 | [exp1-fiqa-rag_7:Experiment] All actors shut down\n",
      "2026-02-19 02:44:43 | Experiment | INFO | experiment.py:574 | [exp1-fiqa-rag_7:Experiment] Dispatcher will automatically shut down (daemon thread)\n"
     ]
    }
   ],
   "source": [
    "# Get the experiment-specific log file\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "print(f\"üìÑ Log File: {log_file}\")\n",
    "print()\n",
    "\n",
    "if log_file.exists():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Last 30 lines of {log_file.name}:\")\n",
    "    print(\"=\" * 80)\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            print(line.rstrip())\n",
    "else:\n",
    "    print(f\"‚ùå Log file not found: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
