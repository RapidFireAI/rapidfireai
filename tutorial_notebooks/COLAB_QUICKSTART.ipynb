{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RapidFireAI/rapidfireai/blob/main/tutorial_notebooks/COLAB_QUICKSTART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ RapidFire AI - Google Colab Quickstart\n",
    "\n",
    "This notebook demonstrates how to run RapidFire AI in Google Colab.\n",
    "\n",
    "## What You'll Learn\n",
    "- ‚úÖ Install RapidFire AI in Colab\n",
    "- ‚úÖ Start RapidFire services with Cloudflare Tunnel (free, public URLs)\n",
    "- ‚úÖ Access the RapidFire Dashboard, MLflow UI, and API\n",
    "- ‚úÖ Run a simple fine-tuning experiment\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab account (free tier works!)\n",
    "- GPU runtime enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-gpu"
   },
   "source": [
    "## 1Ô∏è‚É£ Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-section"
   },
   "source": [
    "## 2Ô∏è‚É£ Install RapidFire AI\n",
    "\n",
    "This will install RapidFire AI and all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-rapidfire"
   },
   "outputs": [],
   "source": [
    "# Install RapidFire AI from PyPI\n",
    "!pip install -q rapidfireai\n",
    "\n",
    "# Initialize RapidFire (installs additional dependencies)\n",
    "!rapidfireai init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start-services-section"
   },
   "source": [
    "## 3Ô∏è‚É£ Start RapidFire Services\n",
    "\n",
    "This cell will:\n",
    "- Start MLflow server (port 5002)\n",
    "- Start Dispatcher API (port 8080)\n",
    "- Start Frontend Dashboard (port 3000)\n",
    "- Expose all services using **Cloudflare Tunnel** (free, public URLs)\n",
    "\n",
    "**Note**: \n",
    "- This cell will keep running\n",
    "- Look for `.trycloudflare.com` URLs in the output\n",
    "- URLs are publicly accessible (great for sharing with team)\n",
    "- Takes ~30 seconds to start and expose all services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-services"
   },
   "outputs": [],
   "source": [
    "# Start RapidFire in Colab mode\n",
    "# Uses Cloudflare Tunnel by default (free, public URLs)\n",
    "!rapidfireai start --colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alternative-native"
   },
   "source": [
    "### Alternative: Using Native Colab Port Forwarding\n",
    "\n",
    "If you prefer Colab's native port forwarding (authenticated, private URLs), use the Python API.\n",
    "\n",
    "**Important**: Native forwarding only works when called from a notebook cell, not from CLI.\n",
    "\n",
    "Run these cells instead of the one above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-background"
   },
   "outputs": [],
   "source": [
    "# Step 1: Start services in background (without tunneling)\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set hosts to 0.0.0.0 for external access\n",
    "os.environ['RF_MLFLOW_HOST'] = '0.0.0.0'\n",
    "os.environ['RF_API_HOST'] = '0.0.0.0'\n",
    "os.environ['RF_FRONTEND_HOST'] = '0.0.0.0'\n",
    "\n",
    "# Start services in background\n",
    "print(\"Starting RapidFire services in background...\")\n",
    "proc = subprocess.Popen(\n",
    "    ['rapidfireai', 'start'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"Waiting for services to start (30 seconds)...\")\n",
    "time.sleep(30)\n",
    "print(\"‚úÖ Services should be running now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expose-native"
   },
   "outputs": [],
   "source": [
    "# Step 2: Expose ports using native Colab forwarding\n",
    "# This MUST run from a notebook cell (won't work from CLI)\n",
    "from rapidfireai.utils.colab_helper import expose_rapidfire_services\n",
    "\n",
    "print(\"Exposing services via native Colab port forwarding...\\n\")\n",
    "urls = expose_rapidfire_services(\n",
    "    method='native',\n",
    "    mlflow_port=5002,\n",
    "    dispatcher_port=8080,\n",
    "    frontend_port=3000\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Services exposed:\")\n",
    "for service, url in urls.items():\n",
    "    if url:\n",
    "        print(f\"  {service.upper()}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "### üîÑ Cloudflare vs Native: Which to Use?\n",
    "\n",
    "| Feature | Cloudflare (Default) | Native Colab |\n",
    "|---------|---------------------|-------------|\n",
    "| **Setup** | One command | Two steps |\n",
    "| **URLs** | Public (shareable) | Private (authenticated) |\n",
    "| **Speed** | ~30 sec startup | Instant |\n",
    "| **Access** | Anyone with URL | Only you |\n",
    "| **Use Case** | Team collaboration | Personal use |\n",
    "\n",
    "**Recommendation**: Use Cloudflare (default) unless you need private URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-dashboard"
   },
   "source": [
    "## 4Ô∏è‚É£ Access Your Services\n",
    "\n",
    "After running the cell above, you should see three URLs:\n",
    "\n",
    "1. **RapidFire Dashboard** - Main UI for:\n",
    "   - Viewing experiment runs\n",
    "   - Real-time metrics\n",
    "   - Interactive Control Operations (stop/resume/clone)\n",
    "\n",
    "2. **MLflow Tracking UI** - For:\n",
    "   - Detailed experiment tracking\n",
    "   - Parameter comparison\n",
    "   - Model artifacts\n",
    "\n",
    "3. **Dispatcher API** - REST API endpoint\n",
    "\n",
    "Click the URLs to open them in new tabs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-experiment-section"
   },
   "source": [
    "## 5Ô∏è‚É£ Run a Simple Experiment\n",
    "\n",
    "Now let's create a minimal fine-tuning experiment to test everything works.\n",
    "\n",
    "**Note**: Open a new notebook or create new cells for this, as the services need to keep running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simple-experiment"
   },
   "outputs": [],
   "source": [
    "# Minimal example - see other tutorial notebooks for full examples\n",
    "from rapidfireai import Experiment\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Create experiment\n",
    "exp = Experiment(\"colab_quickstart_test\")\n",
    "\n",
    "# Simple configuration\n",
    "config = {\n",
    "    'trainer_type': 'SFT',\n",
    "    'training_args': {\n",
    "        'learning_rate': 1e-5,\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'max_steps': 10,  # Short for demo\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load tiny dataset\n",
    "dataset = load_dataset('imdb', split='train[:50]')\n",
    "\n",
    "# Model creation function\n",
    "def create_model(config):\n",
    "    model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "# Run experiment\n",
    "print(\"üöÄ Starting experiment...\")\n",
    "exp.run_fit(\n",
    "    param_config=config,\n",
    "    create_model_fn=create_model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset.select(range(10)),\n",
    "    num_chunks=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Experiment complete! Check the dashboard for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that you have RapidFire running in Colab, try:\n",
    "\n",
    "### 1. Explore Other Tutorials\n",
    "- `rf-tutorial-sft-chatqa-lite.ipynb` - Supervised fine-tuning\n",
    "- `rf-tutorial-dpo-alignment-lite.ipynb` - DPO training\n",
    "- `rf-tutorial-grpo-mathreasoning-lite.ipynb` - GRPO for math\n",
    "\n",
    "### 2. Try AutoML\n",
    "```python\n",
    "from rapidfireai.automl import GridSearch, RFModelConfig\n",
    "\n",
    "config = RFModelConfig(\n",
    "    training_args={\n",
    "        'learning_rate': [1e-4, 1e-5, 1e-6],\n",
    "        'batch_size': [8, 16]\n",
    "    }\n",
    ")\n",
    "\n",
    "grid = GridSearch(configs=[config])\n",
    "exp.run_fit(param_config=grid, ...)\n",
    "```\n",
    "\n",
    "### 3. Use Interactive Control\n",
    "In the dashboard:\n",
    "- Stop/resume runs mid-training\n",
    "- Clone runs with modified hyperparameters\n",
    "- Warm-start from previous checkpoints\n",
    "- Compare results in real-time\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Documentation**: https://rapidfire-ai-oss-docs.readthedocs-hosted.com/\n",
    "- **GitHub**: https://github.com/RapidFireAI/rapidfireai\n",
    "- **Full Colab Guide**: [COLAB_SETUP.md](https://github.com/RapidFireAI/rapidfireai/blob/main/tutorial_notebooks/COLAB_SETUP.md)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Troubleshooting\n",
    "\n",
    "**Services not starting?**\n",
    "```bash\n",
    "# Check if ports are in use\n",
    "!lsof -ti :5002 :8080 :3000\n",
    "```\n",
    "\n",
    "**Cloudflare tunnel failing?**\n",
    "```python\n",
    "# Try ngrok instead (requires auth token)\n",
    "import os\n",
    "os.environ['RF_NGROK_TOKEN'] = 'your_token'\n",
    "!rapidfireai start --colab --tunnel ngrok\n",
    "```\n",
    "\n",
    "**Out of memory?**\n",
    "- Use smaller models\n",
    "- Reduce batch size\n",
    "- Enable gradient checkpointing\n",
    "\n",
    "---\n",
    "\n",
    "**Happy fine-tuning! üéâ**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
