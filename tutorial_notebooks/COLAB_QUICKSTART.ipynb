{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RapidFireAI/rapidfireai/blob/main/tutorial_notebooks/COLAB_QUICKSTART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ RapidFire AI - Google Colab Quickstart\n",
    "\n",
    "This notebook demonstrates how to run RapidFire AI in Google Colab with native port forwarding.\n",
    "\n",
    "## What You'll Learn\n",
    "- ‚úÖ Install RapidFire AI in Colab\n",
    "- ‚úÖ Start RapidFire services with native port forwarding\n",
    "- ‚úÖ Access the RapidFire Dashboard, MLflow UI, and API\n",
    "- ‚úÖ Run a simple fine-tuning experiment\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab account (free tier works!)\n",
    "- GPU runtime enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-gpu"
   },
   "source": [
    "## 1Ô∏è‚É£ Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-section"
   },
   "source": [
    "## 2Ô∏è‚É£ Install RapidFire AI\n",
    "\n",
    "This will install RapidFire AI and all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-rapidfire"
   },
   "outputs": [],
   "source": [
    "# Install RapidFire AI from PyPI\n",
    "!pip install -q rapidfireai\n",
    "\n",
    "# Initialize RapidFire (installs additional dependencies)\n",
    "!rapidfireai init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start-services-section"
   },
   "source": [
    "## 3Ô∏è‚É£ Start RapidFire Services\n",
    "\n",
    "This cell will:\n",
    "- Start MLflow server (port 5002)\n",
    "- Start Dispatcher API (port 8080)\n",
    "- Start Frontend Dashboard (port 3000)\n",
    "- Expose all services using Colab's native port forwarding\n",
    "\n",
    "**Note**: This cell will keep running. Click the URLs that appear to open the services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-services"
   },
   "outputs": [],
   "source": [
    "# Start RapidFire in Colab mode\n",
    "# This will automatically expose ports using native Colab forwarding\n",
    "!rapidfireai start --colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alternative-tunnel"
   },
   "source": [
    "### Alternative: Using Cloudflare Tunnel\n",
    "\n",
    "If you want public URLs (shareable), use Cloudflare Tunnel instead:\n",
    "\n",
    "```python\n",
    "!rapidfireai start --colab --tunnel cloudflare\n",
    "```\n",
    "\n",
    "This is useful if you want to:\n",
    "- Share the dashboard with team members\n",
    "- Access from multiple devices\n",
    "- Keep URLs stable across restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "manual-port-forwarding"
   },
   "source": [
    "## 4Ô∏è‚É£ Manual Port Forwarding (Optional)\n",
    "\n",
    "If you prefer to start services manually and control port forwarding yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual-start"
   },
   "outputs": [],
   "source": [
    "# Alternative: Use Python API for more control\n",
    "from rapidfireai.utils.colab_helper import expose_rapidfire_services, is_colab\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check if in Colab\n",
    "if not is_colab():\n",
    "    print(\"‚ö†Ô∏è  This notebook is designed for Google Colab\")\n",
    "else:\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "\n",
    "# Start services in background (using start.sh)\n",
    "# Note: You'll need to manually start the services first\n",
    "# Then expose them:\n",
    "\n",
    "# Expose services with native Colab forwarding\n",
    "urls = expose_rapidfire_services(\n",
    "    method='native',  # or 'cloudflare' or 'ngrok'\n",
    "    mlflow_port=5002,\n",
    "    dispatcher_port=8080,\n",
    "    frontend_port=3000\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Access your services at:\")\n",
    "for service, url in urls.items():\n",
    "    print(f\"  {service}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-experiment-section"
   },
   "source": [
    "## 5Ô∏è‚É£ Run a Simple Experiment\n",
    "\n",
    "Now that services are running, let's create a simple fine-tuning experiment.\n",
    "\n",
    "**Note**: Open a new code cell or notebook for this, as the services are running in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simple-experiment"
   },
   "outputs": [],
   "source": [
    "# This is a minimal example - see other tutorial notebooks for full examples\n",
    "from rapidfireai import Experiment\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create experiment\n",
    "exp = Experiment(\"colab_quickstart_test\")\n",
    "\n",
    "# Define your configuration\n",
    "config = {\n",
    "    'trainer_type': 'SFT',\n",
    "    'training_args': {\n",
    "        'learning_rate': 1e-5,\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'num_train_epochs': 1,\n",
    "        'max_steps': 10,  # Short for demo\n",
    "    },\n",
    "    'peft_params': {\n",
    "        'r': 8,\n",
    "        'lora_alpha': 32,\n",
    "        'target_modules': ['q_proj', 'v_proj'],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load a small dataset for testing\n",
    "dataset = load_dataset('imdb', split='train[:100]')  # Just 100 samples for demo\n",
    "\n",
    "# Define model creation function\n",
    "def create_model(config):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Run the experiment\n",
    "print(\"üöÄ Starting experiment...\")\n",
    "exp.run_fit(\n",
    "    param_config=config,\n",
    "    create_model_fn=create_model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset.select(range(20)),  # Small eval set\n",
    "    num_chunks=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get results\n",
    "results = exp.get_results()\n",
    "print(\"\\n‚úÖ Experiment complete!\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-dashboard"
   },
   "source": [
    "## 6Ô∏è‚É£ View Results\n",
    "\n",
    "After running the experiment:\n",
    "\n",
    "1. **RapidFire Dashboard**: Click the frontend URL from step 3 to see:\n",
    "   - Real-time run status\n",
    "   - Interactive Control Operations (stop, resume, clone)\n",
    "   - Metrics visualization\n",
    "\n",
    "2. **MLflow UI**: Click the MLflow URL to see:\n",
    "   - Detailed metrics and parameters\n",
    "   - Model artifacts\n",
    "   - Comparison across runs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that you have RapidFire running in Colab, try:\n",
    "\n",
    "1. **Explore Other Tutorials**:\n",
    "   - `rf-tutorial-sft-chatqa.ipynb` - Supervised fine-tuning\n",
    "   - `rf-tutorial-dpo-alignment.ipynb` - DPO training\n",
    "   - `rf-tutorial-grpo-mathreasoning.ipynb` - GRPO for math\n",
    "\n",
    "2. **Use AutoML**:\n",
    "   ```python\n",
    "   from rapidfireai.automl import GridSearch, RFModelConfig\n",
    "   \n",
    "   config = RFModelConfig(\n",
    "       training_args={\n",
    "           'learning_rate': [1e-4, 1e-5, 1e-6],\n",
    "           'batch_size': [8, 16]\n",
    "       }\n",
    "   )\n",
    "   \n",
    "   grid = GridSearch(configs=[config])\n",
    "   exp.run_fit(param_config=grid, ...)\n",
    "   ```\n",
    "\n",
    "3. **Interactive Control**: \n",
    "   - Stop/resume runs mid-training\n",
    "   - Clone promising runs with modified configs\n",
    "   - Compare results in real-time\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Documentation**: https://rapidfire-ai-oss-docs.readthedocs-hosted.com/\n",
    "- **GitHub**: https://github.com/RapidFireAI/rapidfireai\n",
    "- **Discord**: Join our community for help\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Troubleshooting\n",
    "\n",
    "**Services not starting?**\n",
    "```bash\n",
    "# Check logs\n",
    "!tail -50 mlflow.log\n",
    "!tail -50 api.log\n",
    "!tail -50 frontend.log\n",
    "```\n",
    "\n",
    "**Port conflicts?**\n",
    "```python\n",
    "import os\n",
    "os.environ['RF_MLFLOW_PORT'] = '5003'\n",
    "os.environ['RF_API_PORT'] = '8081'\n",
    "os.environ['RF_FRONTEND_PORT'] = '3001'\n",
    "```\n",
    "\n",
    "**Native forwarding not working?**\n",
    "- Try Cloudflare: `!rapidfireai start --colab --tunnel cloudflare`\n",
    "- Or ngrok (requires token): `!rapidfireai start --colab --tunnel ngrok`\n",
    "\n",
    "---\n",
    "\n",
    "**Happy fine-tuning! üéâ**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
