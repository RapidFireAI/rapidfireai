{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVCZ8hwnWeLi"
      },
      "source": [
        "<div align=\"center\">\n",
        "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/images/discord-button.svg\" width=\"145\"></a>\n",
        "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/images/documentation-button.svg\" width=\"125\"></a>\n",
        "<br/>\n",
        "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
        "<br/>\n",
        "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3DmTK3ZWeLp"
      },
      "source": [
        "‚ö†Ô∏è **IMPORTANT:** Do not let the Colab notebook tab stay idle for more than 5min; Colab will disconnect otherwise. Refresh the TensorBoard screen or interact with the cells to avoid disconnection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHIblXRsAxJh"
      },
      "source": [
        "# RapidFire AI in Google Colab with TensorBoard\n",
        "\n",
        "This tutorial demonstrates how to use RapidFire AI in Google Colab with in-built TensorBoard for real-time metrics visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7rkXU_0AxJj"
      },
      "source": [
        "## Start RapidFire Services in Colab Mode\n",
        "\n",
        "RapidFire requires the API Server to manage experiment state. Open the Colab terminal (Tools > Command palette > Terminal) and run:\n",
        "\n",
        "```bash\n",
        "pip install rapidfireai # Takes 1 min\n",
        "rapidfireai init # Takes 1 min\n",
        "export RF_TRACKING_BACKEND=tensorboard\n",
        "rapidfireai start --colab & # Takes 0.5 min\n",
        "```\n",
        "\n",
        "The `--colab` flag will:\n",
        "- ‚úÖ Start the API Server (required for experiment state management)\n",
        "- ‚äó Skip the frontend server (using TensorBoard instead)\n",
        "\n",
        "You should see output like:\n",
        "```\n",
        "üì¶ RapidFire AI Initializing...\n",
        "‚úÖ [1/1] Dispatcher server started\n",
        "üöÄ RapidFire running in Colab mode!\n",
        "üìä Use TensorBoard for metrics visualization:\n",
        "   %tensorboard --logdir ~/experiments/{experiment_name}/tensorboard_logs\n",
        "```\n",
        "\n",
        "**IMPORTANT: Leave this terminal running while you work in your notebook!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v66VLdtAxJj"
      },
      "source": [
        "## Configure RapidFire to Use TensorBoard\n",
        "\n",
        "We'll set environment variables to tell RapidFire to use TensorBoard instead of MLflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbo1EcUmAxJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Configure RapidFire to use TensorBoard\n",
        "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'  # Options: 'mlflow', 'tensorboard', 'both'\n",
        "# TensorBoard log directory will be auto-created in experiment path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Xdz9eZFOGh"
      },
      "source": [
        "## Configure Hugging Face token\n",
        "\n",
        "Install huggingface-hub and provide your HF token in place of YOUR-TOKEN-HERE.\n",
        "\n",
        "**IMPORTANT: Hugging Face does not allow us to provide a public HF token. You need to sign up for a Hugging Face account and obtain a token.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEGBEgyFFM9X"
      },
      "outputs": [],
      "source": [
        "!pip install \"huggingface-hub[cli]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVbdruEiFaYv"
      },
      "outputs": [],
      "source": [
        "!hf auth login --token YOUR-TOKEN-HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgK5ZQFnAxJj"
      },
      "source": [
        "## Import RapidFire Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAde0aIfAxJk"
      },
      "outputs": [],
      "source": [
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adszyLwxAxJk"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vil1zbTeAxJk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
        "\n",
        "# REDUCED dataset for memory constraints in Colab\n",
        "train_dataset = dataset[\"train\"].select(range(64))  # Reduced from 128\n",
        "eval_dataset = dataset[\"train\"].select(range(50, 60))  # 10 examples\n",
        "train_dataset = train_dataset.shuffle(seed=42)\n",
        "eval_dataset = eval_dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3iorhjgAxJk"
      },
      "source": [
        "## Define Data Processing Function\n",
        "\n",
        "We'll format the data as Q&A pairs for GPT-2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpnc3duXAxJk"
      },
      "outputs": [],
      "source": [
        "def sample_formatting_function(example):\n",
        "    \"\"\"Format the dataset for GPT-2 while preserving original fields\"\"\"\n",
        "    return {\n",
        "        \"text\": f\"Question: {example['instruction']}\\nAnswer: {example['response']}\",\n",
        "        \"instruction\": example['instruction'],  # Keep original\n",
        "        \"response\": example['response']  # Keep original\n",
        "    }\n",
        "\n",
        "# Apply formatting to datasets\n",
        "eval_dataset = eval_dataset.map(sample_formatting_function)\n",
        "train_dataset = train_dataset.map(sample_formatting_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--lWb0qnAxJk"
      },
      "source": [
        "## Define Metrics Function\n",
        "\n",
        "We'll use a lightweight metrics computation with just ROUGE-L to save memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gqa6JduAxJk"
      },
      "outputs": [],
      "source": [
        "def sample_compute_metrics(eval_preds):\n",
        "    \"\"\"Lightweight metrics computation\"\"\"\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    try:\n",
        "        import evaluate\n",
        "\n",
        "        # Only compute ROUGE-L (skip BLEU to save memory)\n",
        "        rouge = evaluate.load(\"rouge\")\n",
        "        rouge_output = rouge.compute(\n",
        "            predictions=predictions,\n",
        "            references=labels,\n",
        "            use_stemmer=True,\n",
        "            rouge_types=[\"rougeL\"]  # Only compute rougeL\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"rougeL\": round(rouge_output[\"rougeL\"], 4),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Fallback if metrics fail\n",
        "        print(f\"Metrics computation failed: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zW2g7CJAxJk"
      },
      "source": [
        "## Initialize Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ6mbRK6AxJl"
      },
      "outputs": [],
      "source": [
        "# Create experiment with unique name\n",
        "my_experiment = \"tensorboard-demo-1\"\n",
        "experiment = Experiment(experiment_name=my_experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAYPi61cAxJl"
      },
      "source": [
        "## Get TensorBoard Log Directory\n",
        "\n",
        "The TensorBoard logs are stored in the experiment directory. Let's get the path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QPR4y-YAxJl"
      },
      "outputs": [],
      "source": [
        "# Get experiment path\n",
        "from rapidfireai.db.rf_db import RfDb\n",
        "\n",
        "db = RfDb()\n",
        "experiment_path = db.get_experiments_path(my_experiment)\n",
        "tensorboard_log_dir = f\"{experiment_path}/{my_experiment}/tensorboard_logs\"\n",
        "\n",
        "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pALJJyZcAxJl"
      },
      "source": [
        "## Define Model Configurations\n",
        "\n",
        "This tutorial showcases GPT-2 (124M parameters), which is perfect for Colab's memory constraints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shEQVD7kAxJl"
      },
      "outputs": [],
      "source": [
        "# GPT-2 specific LoRA configs - different module names!\n",
        "peft_configs_lite = List([\n",
        "    RFLoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"c_attn\"],  # GPT-2 combines Q,K,V in c_attn\n",
        "        bias=\"none\"\n",
        "    ),\n",
        "    RFLoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"c_attn\", \"c_proj\"],  # c_attn (QKV) + c_proj (output)\n",
        "        bias=\"none\"\n",
        "    )\n",
        "])\n",
        "\n",
        "# 2 configs with GPT-2\n",
        "config_set_lite = List([\n",
        "    RFModelConfig(\n",
        "        model_name=\"gpt2\",  # Only 124M params\n",
        "        peft_config=peft_configs_lite,\n",
        "        training_args=RFSFTConfig(\n",
        "            learning_rate=5e-4,  # Low lr for more stability\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,  # Effective bs = 4\n",
        "            max_steps=64, # Raise this to see more learning\n",
        "            logging_steps=2,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=4,\n",
        "            per_device_eval_batch_size=2,\n",
        "            fp16=True,\n",
        "            gradient_checkpointing=True,  # Save memory\n",
        "            report_to=\"none\",  # Disables wandb\n",
        "        ),\n",
        "        model_type=\"causal_lm\",\n",
        "        model_kwargs={\n",
        "            \"device_map\": \"auto\",\n",
        "            \"torch_dtype\": \"float16\",  # Explicit fp16\n",
        "            \"use_cache\": False\n",
        "        },\n",
        "        formatting_func=sample_formatting_function,\n",
        "        compute_metrics=sample_compute_metrics,\n",
        "        generation_config={\n",
        "            \"max_new_tokens\": 128,  # Reduced from 256\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 40,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": 50256,  # GPT-2's EOS token\n",
        "        }\n",
        "    ),\n",
        "    RFModelConfig(\n",
        "        model_name=\"gpt2\",\n",
        "        peft_config=peft_configs_lite,\n",
        "        training_args=RFSFTConfig(\n",
        "            learning_rate=2e-4,  # Even more conservative\n",
        "            lr_scheduler_type=\"cosine\",  # Try cosine schedule\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,\n",
        "            max_steps=64, # Raise this to see more learning behviors\n",
        "            logging_steps=2,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=4,\n",
        "            per_device_eval_batch_size=2,\n",
        "            fp16=True,\n",
        "            gradient_checkpointing=True,\n",
        "            report_to=\"none\",  # Disables wandb\n",
        "            warmup_steps=10,  # Add warmup for stability\n",
        "        ),\n",
        "        model_type=\"causal_lm\",\n",
        "        model_kwargs={\n",
        "            \"device_map\": \"auto\",\n",
        "            \"torch_dtype\": \"float16\",\n",
        "            \"use_cache\": False\n",
        "        },\n",
        "        formatting_func=sample_formatting_function,\n",
        "        compute_metrics=sample_compute_metrics,\n",
        "        generation_config={\n",
        "            \"max_new_tokens\": 128,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 40,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": 50256,\n",
        "        }\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuo9B8WrAxJl"
      },
      "outputs": [],
      "source": [
        "def sample_create_model(model_config):\n",
        "    \"\"\"Function to create model object with GPT-2 adjustments\"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "    model_name = model_config[\"model_name\"]\n",
        "    model_type = model_config[\"model_type\"]\n",
        "    model_kwargs = model_config[\"model_kwargs\"]\n",
        "\n",
        "    if model_type == \"causal_lm\":\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "    else:\n",
        "        # Default to causal LM\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # GPT-2 specific: Set pad token (GPT-2 doesn't have one by default)\n",
        "    if \"gpt2\" in model_name.lower():\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"left\"  # GPT-2 works better with left padding\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    return (model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7NOeq0PAxJl"
      },
      "outputs": [],
      "source": [
        "# Simple grid search across all config combinations: 4 total (2 LoRA configs √ó 2 trainer configs)\n",
        "config_group = RFGridSearch(\n",
        "    configs=config_set_lite,\n",
        "    trainer_type=\"SFT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrFU-wjkAxJm"
      },
      "source": [
        "## Launch Interactive Run Controller\n",
        "\n",
        "RapidFire AI provides an Interactive Controller that lets you manage executing runs dynamically in real-time from the notebook:\n",
        "\n",
        "- ‚èπÔ∏è **Stop**: Gracefully stop a running config\n",
        "- ‚ñ∂Ô∏è **Resume**: Resume a stopped run\n",
        "- üóëÔ∏è **Delete**: Remove a run from this experiment\n",
        "- üìã **Clone**: Create a new run by editing the config dictionary of a parent run to try new knob values; optional warm start of parameters\n",
        "- üîÑ **Refresh**: Update run status and metrics\n",
        "\n",
        "The Controller uses ipywidgets and is compatible with both Colab (ipywidgets 7.x) and Jupyter (ipywidgets 8.x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNc_MLbGL8xC"
      },
      "outputs": [],
      "source": [
        "# Create Interactive Controller\n",
        "from rapidfireai.utils.interactive_controller import InteractiveController\n",
        "\n",
        "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8081\")\n",
        "controller.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJfXimPpAxJl"
      },
      "source": [
        "## Start TensorBoard\n",
        "\n",
        "**IMPORTANT: Make sure to start TensorBoard BEFORE invoking run_fit() below so that you can watch metrics appear in real-time!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVVWU42vKBTN"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {tensorboard_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGOs_rZYAxJm"
      },
      "source": [
        "## Run Training + Validation\n",
        "\n",
        "Now we get to the main function for running multi-config training and evals. The metrics will appear in TensorBoard above in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AykcHp33AxJm"
      },
      "outputs": [],
      "source": [
        "# Launch training\n",
        "experiment.run_fit(\n",
        "    config_group,\n",
        "    sample_create_model,\n",
        "    train_dataset,\n",
        "    eval_dataset,\n",
        "    num_chunks=4,  # 4 chunks for hyperparallel execution\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujpATTaRAxJm"
      },
      "source": [
        "## End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOTTI-rVAxJm"
      },
      "outputs": [],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuiwNvldAxJm"
      },
      "source": [
        "## View TensorBoard Plots and Logs\n",
        "\n",
        "After your experiment is ended, you can still view the full logs in TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvbsKwE2AxJm"
      },
      "outputs": [],
      "source": [
        "# View final logs\n",
        "%tensorboard --logdir {tensorboard_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXeBD1QjWeLv"
      },
      "source": [
        "# View RapidFire AI Log Files\n",
        "\n",
        "You can track the work being done by the system via the RapidFire AI-produced log files in rapidfire_experiments/ folder. To see the log files, open the Colab terminal and run the commands:\n",
        "\n",
        "```bash\n",
        "tail -n 20 rapidfire_experiments/rapidfire.log\n",
        "tail -n 20 rapidfire_experiments/training.log\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
