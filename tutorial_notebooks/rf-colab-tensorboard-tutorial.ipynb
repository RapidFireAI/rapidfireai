{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RapidFire AI with TensorBoard in Google Colab\n",
        "\n",
        "This tutorial demonstrates how to use RapidFire AI with TensorBoard for real-time metrics visualization in Google Colab.\n",
        "\n",
        "## Why TensorBoard in Colab?\n",
        "\n",
        "- **Real-time visualization**: View training metrics as they happen\n",
        "- **No frontend loading delay**: TensorBoard loads instantly in Colab\n",
        "- **Native Colab support**: TensorBoard works natively with `%tensorboard` magic\n",
        "- **Live updates**: Metrics update every 30 seconds while training cell is blocked\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install RapidFire AI and load the TensorBoard extension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install RapidFire AI\n",
        "!pip install rapidfireai\n",
        "\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure RapidFire to Use TensorBoard\n",
        "\n",
        "We'll set environment variables to tell RapidFire to use TensorBoard instead of MLflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Configure RapidFire to use TensorBoard\n",
        "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'  # Options: 'mlflow', 'tensorboard', 'both'\n",
        "# TensorBoard log directory will be auto-created in experiment path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import RapidFire Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
        "\n",
        "# Select a subset for demonstration\n",
        "train_dataset = dataset[\"train\"].select(range(128))\n",
        "eval_dataset = dataset[\"train\"].select(range(100, 124))\n",
        "train_dataset = train_dataset.shuffle(seed=42)\n",
        "eval_dataset = eval_dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Data Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_formatting_function(row):\n",
        "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
        "    SYSTEM_PROMPT = \"You are a helpful and friendly customer support assistant.\"\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
        "        ],\n",
        "        \"completion\": [\n",
        "            {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create experiment with unique name\n",
        "experiment = Experiment(experiment_name=\"tensorboard-demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get TensorBoard Log Directory\n",
        "\n",
        "The TensorBoard logs are stored in the experiment directory. Let's get the path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get experiment path\n",
        "from rapidfireai.utils.datapaths import DataPath\n",
        "from rapidfireai.db.rf_db import RfDb\n",
        "\n",
        "db = RfDb()\n",
        "experiment_path = db.get_experiments_path(\"tensorboard-demo\")\n",
        "tensorboard_log_dir = f\"{experiment_path}/tensorboard_logs\"\n",
        "\n",
        "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start TensorBoard\n",
        "\n",
        "**IMPORTANT**: Start TensorBoard BEFORE running training, so you can watch metrics update in real-time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start TensorBoard (will update automatically as training progresses)\n",
        "%tensorboard --logdir {tensorboard_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model Configuration\n",
        "\n",
        "We'll use a small model (TinyLlama) for fast training in Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LoRA configs\n",
        "peft_configs = List([\n",
        "    RFLoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        bias=\"none\"\n",
        "    ),\n",
        "    RFLoraConfig(\n",
        "        r=32,\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        bias=\"none\"\n",
        "    )\n",
        "])\n",
        "\n",
        "# Define model configs\n",
        "config_set = List([\n",
        "    RFModelConfig(\n",
        "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        peft_config=peft_configs,\n",
        "        training_args=RFSFTConfig(\n",
        "            learning_rate=1e-3,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=4,\n",
        "            max_steps=64,  # Short training for demo\n",
        "            gradient_accumulation_steps=1,\n",
        "            logging_steps=2,  # Frequent logging for TensorBoard\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=8,\n",
        "            fp16=True,\n",
        "        ),\n",
        "        model_type=\"causal_lm\",\n",
        "        model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
        "        formatting_func=sample_formatting_function,\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model Creation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_create_model(model_config):\n",
        "    \"\"\"Function to create model object for any given config\"\"\"\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    \n",
        "    model_name = model_config[\"model_name\"]\n",
        "    model_kwargs = model_config[\"model_kwargs\"]\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    return (model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Config Group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple grid search\n",
        "config_group = RFGridSearch(\n",
        "    configs=config_set,\n",
        "    trainer_type=\"SFT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Training\n",
        "\n",
        "**IMPORTANT**: While this cell is running:\n",
        "1. Switch to the TensorBoard tab above\n",
        "2. Watch metrics update in real-time (every 30 seconds)\n",
        "3. See training loss, learning rate, and other metrics appear\n",
        "\n",
        "This is the key advantage of TensorBoard in Colab - you can monitor training progress even while the cell is blocked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch training - metrics will appear in TensorBoard above!\n",
        "experiment.run_fit(\n",
        "    config_group, \n",
        "    sample_create_model, \n",
        "    train_dataset, \n",
        "    eval_dataset, \n",
        "    num_chunks=2,  # 2 chunks for demo\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View TensorBoard Logs\n",
        "\n",
        "After training completes, you can still view the full logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View final logs\n",
        "%tensorboard --logdir {tensorboard_log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Both MLflow and TensorBoard\n",
        "\n",
        "You can also log to both backends simultaneously by setting:\n",
        "\n",
        "```python\n",
        "os.environ['RF_TRACKING_BACKEND'] = 'both'\n",
        "```\n",
        "\n",
        "This gives you:\n",
        "- **TensorBoard**: Real-time visualization during training\n",
        "- **MLflow**: Experiment comparison and model registry\n",
        "\n",
        "## Tips for Colab + TensorBoard\n",
        "\n",
        "1. **Start TensorBoard first**: Always start TensorBoard before training\n",
        "2. **Frequent logging**: Set `logging_steps` to a small value (e.g., 2-5) for responsive updates\n",
        "3. **Refresh rate**: TensorBoard polls logs every 30 seconds in Colab\n",
        "4. **Multiple experiments**: Use different experiment names for different runs\n",
        "5. **Clean logs**: Delete old logs with `!rm -rf {tensorboard_log_dir}` to start fresh\n",
        "\n",
        "## Comparison: TensorBoard vs MLflow in Colab\n",
        "\n",
        "| Feature | TensorBoard | MLflow |\n",
        "|---------|-------------|--------|\n",
        "| Real-time updates | ‚úÖ Yes (30s polling) | ‚ùå No (frontend load time) |\n",
        "| Colab native | ‚úÖ %tensorboard magic | ‚ùå Requires tunneling |\n",
        "| Load time | ‚úÖ Instant | ‚ùå 3-5 minutes via tunnel |\n",
        "| Model registry | ‚ùå No | ‚úÖ Yes |\n",
        "| Experiment comparison | ‚úÖ Basic | ‚úÖ Advanced |\n",
        "\n",
        "**Recommendation**: Use `'both'` backend to get the best of both worlds!\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Try different model configs and compare in TensorBoard\n",
        "- Experiment with `'both'` backend for comprehensive tracking\n",
        "- Check out other RapidFire tutorials for DPO and GRPO training\n",
        "\n",
        "Happy training! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
