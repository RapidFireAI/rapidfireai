{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RapidFire AI Tutorial Use Case: SFT for Customer Support Q&A Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Specify Train and Eval Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "\n",
    "# Select a subset of the dataset for demo purposes\n",
    "train_dataset=dataset[\"train\"].select(range(50))\n",
    "eval_dataset=dataset[\"train\"].select(range(5000,5020))\n",
    "train_dataset=train_dataset.shuffle(seed=42)\n",
    "eval_dataset=eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_formatting_function(row):\n",
    "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
    "    # Special tokens for formatting\n",
    "    SYSTEM_PROMPT = \"You are a helpful and friendly customer support assistant. Please answer the user's query to the best of your ability.\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "            \n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/12 18:01:10 INFO mlflow.tracking.fluent: Experiment with name 'exp2-chatqa_26' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An experiment with the same name already exists. Created a new experiment with name 'exp2-chatqa_26' with Experiment ID: 27 and MLFlow Experiment ID: 27 saved at /home/palebluedot/rapidfireai/tutorial_notebooks/rapidfire_experiments/exp2-chatqa_26\n"
     ]
    }
   ],
   "source": [
    "# Every experiment instance must be uniquely named\n",
    "experiment = Experiment(experiment_name=\"exp2-chatqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Eval Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_compute_metrics(eval_preds):  \n",
    "    \"\"\"Optional function to compute eval metrics based on predictions and labels\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Standard text-based eval metrics: Rouge and BLEU\n",
    "    import evaluate\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
    "    rouge_l = rouge_output[\"rougeL\"]\n",
    "    bleu_output = bleu.compute(predictions=predictions, references=labels)\n",
    "    bleu_score = bleu_output[\"bleu\"]\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_l, 4),\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multi-Config Knobs for Model, LoRA, and SFT Trainer using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 LoRA PEFT configs with different adapter capacities\n",
    "peft_configs = List([\n",
    "    RFLoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    # RFLoraConfig(\n",
    "    #         r=128,\n",
    "    #         lora_alpha=256,\n",
    "    #         lora_dropout=0.05,\n",
    "    #         target_modules=[\"q_proj\",\"k_proj\", \"v_proj\",\"o_proj\"],\n",
    "    #         bias=\"none\"\n",
    "    # )\n",
    "])\n",
    "\n",
    "# 2 base models x 2 peft configs = 4 combinations in total\n",
    "config_set = List([\n",
    "    # RFModelConfig(\n",
    "    #     model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    #     peft_config=peft_configs,\n",
    "    #     training_args=RFSFTConfig(\n",
    "    #         learning_rate=2e-4,\n",
    "    #         lr_scheduler_type=\"linear\",\n",
    "    #         per_device_train_batch_size=4,\n",
    "    #         per_device_eval_batch_size=8,\n",
    "    #         num_train_epochs=2,\n",
    "    #         gradient_accumulation_steps=4,\n",
    "    #         logging_steps=5,\n",
    "    #         eval_strategy=\"steps\",\n",
    "    #         eval_steps=25,\n",
    "    #         fp16=True,\n",
    "    #         save_strategy=\"epoch\"\n",
    "    #     ),\n",
    "    #     model_type=\"causal_lm\",\n",
    "    #     model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": \"auto\",\"use_cache\":False},\n",
    "    #     formatting_func = sample_formatting_function,\n",
    "    #     compute_metrics = sample_compute_metrics,\n",
    "    #     generation_config = { # This is for text based evaluation/prediction for causal_lm models\n",
    "    #         \"max_new_tokens\": 256,\n",
    "    #         \"temperature\": 0.6,\n",
    "    #         \"top_p\": 0.9,\n",
    "    #         \"top_k\": 40,\n",
    "    #         \"repetition_penalty\": 1.18,\n",
    "    #     }\n",
    "    # ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        peft_config=peft_configs,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-5,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            num_train_epochs=1,\n",
    "            optim=\"adamw_torch\",\n",
    "            # gradient_accumulation_steps=4,\n",
    "            logging_steps=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            fp16=False,\n",
    "            bf16=True,\n",
    "            save_strategy=\"epoch\",\n",
    "            fsdp=\"full_shard auto_wrap\",\n",
    "            fsdp_config={\"backward_prefetch\": \"backward_pre\",\"forward_prefetch\": True,\"use_orig_params\": True,  \"cpu_ram_efficient_loading\": False,\"sync_module_states\": True,\"min_num_params\": 1000000,\"limit_all_gathers\": True}\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\"device_map\": None, \"torch_dtype\": \"auto\",\"use_cache\":False},\n",
    "        formatting_func = sample_formatting_function,\n",
    "        compute_metrics = sample_compute_metrics,\n",
    "        generation_config = { # This is for text based evaluation/prediction for causal_lm models\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.18,\n",
    "        }\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Creation Function for All Model Types Across Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_create_model(model_config): \n",
    "     \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
    "     from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM\n",
    "\n",
    "     model_name = model_config[\"model_name\"]\n",
    "     model_type = model_config[\"model_type\"]\n",
    "     model_kwargs = model_config[\"model_kwargs\"]\n",
    " \n",
    "     if model_type == \"causal_lm\":\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"seq2seq_lm\":\n",
    "          model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"masked_lm\":\n",
    "          model = AutoModelForMaskedLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"custom\":\n",
    "          # Handle custom model loading logic, e.g., loading your own checkpoints\n",
    "          # model = ... \n",
    "          pass\n",
    "     else:\n",
    "          # Default to causal LM\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "      \n",
    "     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      \n",
    "     return (model,tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-Config Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 2 worker processes successfully\n",
      "Created workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected optimizer state\n",
      "collected optimizer state\n",
      "saving checkpoint to shared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W912 18:03:12.947456174 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:03:12.105953276 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank0]:[W912 18:03:13.505211751 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:03:13.675970764 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected optimizer state\n",
      "collected optimizer state\n",
      "saving checkpoint to shared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W912 18:05:04.930245445 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:05:04.231817030 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank0]:[W912 18:05:05.464500083 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:05:05.790420210 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected optimizer state\n",
      "collected optimizer state\n",
      "saving checkpoint to shared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W912 18:06:58.552095945 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:06:58.842705200 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank0]:[W912 18:06:58.143615374 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:06:58.403545935 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/palebluedot/rapidfireai/oss_venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected optimizer state\n",
      "collected optimizer state\n",
      "saving checkpoint to shared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W912 18:08:57.341952612 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:08:58.562669751 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank0]:[W912 18:08:58.166398702 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n",
      "[rank1]:[W912 18:08:58.390810982 PyInterpreter.cpp:263] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\n"
     ]
    }
   ],
   "source": [
    "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
    "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=2, num_gpus=2,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Current Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active MLflow run to clear\n",
      "Experiment exp2-chatqa_26 ended\n",
      "Workers stopped\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
