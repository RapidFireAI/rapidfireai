{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RapidFire AI Tutorial Use Case: SFT for Customer Support Q&A Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: Minimum hardware required for this notebook is 2*A10 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7d1c60b26030>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Specify Train and Eval Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "\n",
    "# Select a subset of the dataset for demo purposes\n",
    "train_dataset=dataset[\"train\"].select(range(128))\n",
    "eval_dataset=dataset[\"train\"].select(range(128,144))\n",
    "train_dataset=train_dataset.shuffle(seed=42)\n",
    "eval_dataset=eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_formatting_function(row):\n",
    "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
    "    # Special tokens for formatting\n",
    "    SYSTEM_PROMPT = \"You are a helpful and friendly customer support assistant. Please answer the user's query to the best of your ability.\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "            \n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every experiment instance must be uniquely named\n",
    "experiment = Experiment(experiment_name=\"exp1-chatqa-fsdp-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Eval Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_compute_metrics(eval_preds):  \n",
    "    \"\"\"Optional function to compute eval metrics based on predictions and labels\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Standard text-based eval metrics: Rouge and BLEU\n",
    "    import evaluate\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
    "    rouge_l = rouge_output[\"rougeL\"]\n",
    "    bleu_output = bleu.compute(predictions=predictions, references=labels)\n",
    "    bleu_score = bleu_output[\"bleu\"]\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_l, 4),\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multi-Config Knobs for Model, LoRA, and SFT Trainer using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 LoRA PEFT configs lite with different adapter capacities\n",
    "peft_configs_lite = List([\n",
    "    RFLoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    RFLoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  \n",
    "        bias=\"none\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "# 2 learning rates  and 2 peft configs = 4 combinations in total\n",
    "config_set_lite = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B model\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=List([1e-3, 1e-4]),  \n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            num_train_epochs=1,\n",
    "            gradient_accumulation_steps=1,   \n",
    "            logging_steps=1,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=8,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "            bf16=True,\n",
    "            fp16=False,\n",
    "            fsdp=\"full_shard auto_wrap\",\n",
    "            fsdp_config={\"backward_prefetch\": \"backward_pre\",\"forward_prefetch\": True,\"use_orig_params\": False,  \"cpu_ram_efficient_loading\": True,\"offload_params\": True, \"sync_module_states\": True,\"limit_all_gathers\": True, \"sharding_strategy\": \"FULL_SHARD\",\n",
    "                \"auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\"},   \n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\"device_map\":None, \"torch_dtype\": \"bfloat16\", \"use_cache\": False},\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config = { # This is for text based evaluation/prediction for causal_lm models\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.18,\n",
    "        },\n",
    "        num_gpus=2,\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Creation Function for All Model Types Across Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_create_model(model_config): \n",
    "     \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
    "     from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, GptOssForCausalLM\n",
    "\n",
    "     model_name = model_config[\"model_name\"]\n",
    "     model_type = model_config[\"model_type\"]\n",
    "     model_kwargs = model_config[\"model_kwargs\"]\n",
    "     \n",
    " \n",
    "     if model_type == \"causal_lm\":\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"seq2seq_lm\":\n",
    "          model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"masked_lm\":\n",
    "          model = AutoModelForMaskedLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"custom\":\n",
    "          # Handle custom model loading logic, e.g., loading your own checkpoints\n",
    "          # model = ...\n",
    "          pass\n",
    "     else:\n",
    "          # Default to causal LM\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "      \n",
    "     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "     return (model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set_lite,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-Config Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
    "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=4, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Current Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
