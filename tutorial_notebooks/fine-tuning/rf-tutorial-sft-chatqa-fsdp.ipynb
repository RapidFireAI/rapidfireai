{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RapidFire AI Tutorial Use Case: SFT for Customer Support Q&A Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Note: Minimum hardware required for this notebook is 4*A10 GPUs. Heavily downsampled data for demo purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset and Specify Train and Eval Partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset=load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
        "\n",
        "train_dataset=dataset[\"train\"].select(range(320))\n",
        "eval_dataset=dataset[\"train\"].select(range(320,328))\n",
        "train_dataset=train_dataset.shuffle(seed=42)\n",
        "eval_dataset=eval_dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Data Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_formatting_function(row):\n",
        "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
        "    # Special tokens for formatting\n",
        "    SYSTEM_PROMPT = \"You are a helpful and friendly customer support assistant. Please answer the user's query to the best of your ability.\"\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
        "            \n",
        "        ],\n",
        "        \"completion\": [\n",
        "            {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Every experiment instance must be uniquely named\n",
        "experiment = Experiment(experiment_name=\"exp1-chatqa-fsdp77\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Custom Eval Metrics Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_compute_metrics(eval_preds):  \n",
        "    \"\"\"Optional function to compute eval metrics based on predictions and labels\"\"\"\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Standard text-based eval metrics: Rouge and BLEU\n",
        "    import evaluate\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
        "    rouge_l = rouge_output[\"rougeL\"]\n",
        "    bleu_output = bleu.compute(predictions=predictions, references=labels)\n",
        "    bleu_score = bleu_output[\"bleu\"]\n",
        "\n",
        "    return {\n",
        "        \"rougeL\": round(rouge_l, 4),\n",
        "        \"bleu\": round(bleu_score, 4),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Multi-Config Knobs for Model, LoRA, and SFT Trainer using RapidFire AI Wrapper APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2 LoRA PEFT configs lite with different adapter capacities\n",
        "peft_configs_lite = List([\n",
        "    RFLoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"], \n",
        "        bias=\"none\"\n",
        "    ),\n",
        "    RFLoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"], \n",
        "        bias=\"none\"\n",
        "    ),\n",
        "])\n",
        "\n",
        "# 2 combinations in total\n",
        "config_set_lite = List([\n",
        "    RFModelConfig(\n",
        "        model_name=\"Qwen/Qwen3-32B\",  # 32B model\n",
        "        peft_config=peft_configs_lite,\n",
        "        training_args=RFSFTConfig(\n",
        "            learning_rate=1e-4,  \n",
        "            lr_scheduler_type=\"linear\",\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            # eval_accumulation_steps=4,\n",
        "            num_train_epochs=1,\n",
        "            gradient_accumulation_steps=4,   \n",
        "            logging_steps=1,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=6,\n",
        "            gradient_checkpointing=True,\n",
        "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "            bf16=True,\n",
        "            tf32=True,\n",
        "            max_length=256,\n",
        "            fsdp=\"full_shard auto_wrap\",\n",
        "            fsdp_config={\"backward_prefetch\": \"backward_pre\",\"forward_prefetch\": False,\"use_orig_params\": False,  \"cpu_ram_efficient_loading\": True,\"offload_params\": True, \"sync_module_states\": True,\"limit_all_gathers\": True, \"sharding_strategy\": \"FULL_SHARD\",\n",
        "                \"auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\"},\n",
        "            fsdp_transformer_layer_cls_to_wrap=\"Qwen3DecoderLayer\",   \n",
        "        ),\n",
        "        model_type=\"causal_lm\",\n",
        "        model_kwargs={\"device_map\":None, \"torch_dtype\": \"bfloat16\", \"use_cache\": False},\n",
        "        formatting_func=sample_formatting_function,\n",
        "        compute_metrics = sample_compute_metrics,\n",
        "        generation_config={\n",
        "            \"max_new_tokens\": 128,\n",
        "            \"do_sample\": False,\n",
        "            \"use_cache\": True,\n",
        "        }\n",
        "\n",
        "    )\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Model Creation Function for All Model Types Across Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def sample_create_model(model_config): \n",
        "     \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
        "     from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, GptOssForCausalLM\n",
        "\n",
        "     model_name = model_config[\"model_name\"]\n",
        "     model_type = model_config[\"model_type\"]\n",
        "     model_kwargs = model_config[\"model_kwargs\"]\n",
        "     \n",
        " \n",
        "     if model_type == \"causal_lm\":\n",
        "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "     elif model_type == \"gpt\":                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
        "          model = GptOssForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "     elif model_type == \"seq2seq_lm\":\n",
        "          model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **model_kwargs)\n",
        "     elif model_type == \"masked_lm\":\n",
        "          model = AutoModelForMaskedLM.from_pretrained(model_name, **model_kwargs)\n",
        "     elif model_type == \"custom\":\n",
        "          # Handle custom model loading logic, e.g., loading your own checkpoints\n",
        "          # model = ... \n",
        "          pass\n",
        "     else:\n",
        "          # Default to causal LM\n",
        "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
        "      \n",
        "     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "     model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "     return (model,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate Config Group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
        "config_group = RFGridSearch(\n",
        "    configs=config_set_lite,\n",
        "    trainer_type=\"SFT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Multi-Config Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
        "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=4,seed=42, num_gpus=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End Current Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
