{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RapidFire AI Tutorial Use Case: GRPO for Math Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/dev/shm/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFGRPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Specify Train and Eval Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] \n",
    "    return data \n",
    "\n",
    "# Select a subset of the dataset for demo purposes\n",
    "train_dataset = get_gsm8k_questions(split=\"train\").select(range(128))\n",
    "eval_dataset = get_gsm8k_questions(split=\"test\").select(range(24))\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset =  eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_formatting_function(row):\n",
    "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
    "\n",
    "    def extract_hash_answer(text: str) -> str | None:\n",
    "        if \"####\" not in text:\n",
    "            return None\n",
    "        answer = text.split(\"####\")[1].strip()\n",
    "        try:\n",
    "            answer = answer.replace(\",\", \"\")\n",
    "        except:\n",
    "            return None\n",
    "        return answer\n",
    "        \n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    Respond in the following format:\n",
    "    <reasoning>\n",
    "    ...\n",
    "    </reasoning>\n",
    "    <answer>\n",
    "    ...\n",
    "    </answer>\n",
    "    \"\"\"\n",
    "    return { # Return a conversation format dictionary\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': row['question']}\n",
    "        ],\n",
    "        'question': row['question'],\n",
    "        'answer': extract_hash_answer(row['answer'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active MLflow run to clear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 07:44:06 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/01/29 07:44:06 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/29 07:44:06 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/29 07:44:06 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/29 07:44:06 INFO mlflow.tracking.fluent: Experiment with name 'exp1-math-reasoning_51' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previously running experiment exp1-math-reasoning_50 was forcibly ended. Created a new experiment with name 'exp1-math-reasoning_51' with Experiment ID: 55 and MLFlow Experiment ID: 55 saved at /home/palebluedot/rapidfireai/tutorial_notebooks/grpo_mathreasoning/rapidfire_experiments/exp1-math-reasoning_51\n"
     ]
    }
   ],
   "source": [
    "# Every experiment instance must be uniquely named\n",
    "experiment = Experiment(experiment_name=\"exp1-math-reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "\n",
    "    def extract_xml_answer(text: str) -> str:\n",
    "        answer = text.split(\"<answer>\")[-1]\n",
    "        answer = answer.split(\"</answer>\")[0]\n",
    "        return answer.strip()\n",
    "\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \n",
    "    def extract_xml_answer(text: str) -> str:\n",
    "        answer = text.split(\"<answer>\")[-1]\n",
    "        answer = answer.split(\"</answer>\")[0]\n",
    "        return answer.strip()\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    import re\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    import re\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    def count_xml(text) -> float:\n",
    "        count = 0.0\n",
    "        if text.count(\"<reasoning>\\n\") == 1:\n",
    "            count += 0.125\n",
    "        if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "            count += 0.125\n",
    "        if text.count(\"\\n<answer>\\n\") == 1:\n",
    "            count += 0.125\n",
    "            count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "        if text.count(\"\\n</answer>\") == 1:\n",
    "            count += 0.125\n",
    "            count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "        return count\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multi-Config Knobs for Model, LoRA, and GRPO Trainer using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:196: FutureWarning: The `max_prompt_length` argument is deprecated and will be removed in version 0.28.0. You should instead filter your dataset before training to ensure that prompts do not exceed your desired length.\n",
      "<string>:196: FutureWarning: The `max_prompt_length` argument is deprecated and will be removed in version 0.28.0. You should instead filter your dataset before training to ensure that prompts do not exceed your desired length.\n"
     ]
    }
   ],
   "source": [
    "lora_config = RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "grpo_config1 = RFGRPOConfig(\n",
    "    use_vllm=True,\n",
    "    vllm_mode=\"colocate\",\n",
    "    vllm_gpu_memory_utilization=0.1,\n",
    "    vllm_tensor_parallel_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.1,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1, \n",
    "    num_generations=4,\n",
    "    optim =\"adamw_torch\",\n",
    "    num_train_epochs=1,\n",
    "    max_prompt_length=1024,\n",
    "    max_completion_length=1024,\n",
    "    logging_steps=2,\n",
    "    log_level=\"error\",\n",
    "    eval_steps=5,\n",
    "    torch_compile=False,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    fsdp=\"full_shard auto_wrap\",\n",
    "    fsdp_config={\"backward_prefetch\": \"backward_pre\",\"forward_prefetch\": True,\"use_orig_params\": True,  \"cpu_ram_efficient_loading\": True,\"offload_params\":True,\"sync_module_states\": True,\"min_num_params\": 1000000,\"limit_all_gathers\": True, \"sharding_strategy\": \"FULL_SHARD\",\n",
    "        \"auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\"}\n",
    ")\n",
    "\n",
    "grpo_config2 = grpo_config1.copy()\n",
    "grpo_config2.learning_rate = 1e-5\n",
    "\n",
    "reward_funcs = [\n",
    "    correctness_reward_func,\n",
    "    int_reward_func,\n",
    "    strict_format_reward_func,\n",
    "    soft_format_reward_func,\n",
    "    xmlcount_reward_func,\n",
    "]\n",
    "\n",
    "# List of 4 separate configs\n",
    "config_set = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        peft_config=lora_config,\n",
    "        training_args=grpo_config1,\n",
    "        formatting_func=sample_formatting_function,\n",
    "        reward_funcs=reward_funcs,\n",
    "        model_kwargs={\"device_map\": None, \"torch_dtype\": \"bfloat16\"},\n",
    "        tokenizer_kwargs={\"model_max_length\": 2048, \"padding_side\": \"left\", \"truncation\": True}\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        peft_config=lora_config,\n",
    "        training_args=grpo_config2,\n",
    "        formatting_func=sample_formatting_function,\n",
    "        reward_funcs=reward_funcs,\n",
    "        model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        tokenizer_kwargs={\"model_max_length\": 2048, \"padding_side\": \"left\", \"truncation\": True}\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_create_model(model_config):\n",
    "   \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
    "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "   \n",
    "   model_name = model_config[\"model_name\"]\n",
    "   model_kwargs = model_config[\"model_kwargs\"]\n",
    "   tokenizer_kwargs = model_config[\"tokenizer_kwargs\"]\n",
    "   return (\n",
    "      AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs, attn_implementation=\"eager\"),\n",
    "      AutoTokenizer.from_pretrained(model_name, **tokenizer_kwargs)\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set,\n",
    "    trainer_type=\"GRPO\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-Config Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 2 worker processes successfully\n",
      "Created workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.44it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.43it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1156003)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.80it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.79it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1156000)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 56.85it/s][0;36m(EngineCore_DP0 pid=1156000)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 57.36it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 61.07it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 60.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 has failed: vLLM subprocess error: 'LLMEngine' object has no attribute 'model_executor'Traceback (most recent call last):\n",
      "  File \"/home/palebluedot/rapidfireai/rapidfireai/backend/worker.py\", line 498, in serve_forever\n",
      "    self.run_fit(run_id, chunk_id, multi_worker_details, create_model_fn)\n",
      "  File \"/home/palebluedot/rapidfireai/rapidfireai/backend/worker.py\", line 321, in run_fit\n",
      "    trainer_instance.train()\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 2325, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 2674, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py\", line 1168, in training_step\n",
      "    output = super().training_step(model, inputs, num_items_in_batch)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 4014, in training_step\n",
      "    inputs = self._prepare_inputs(inputs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/trl/extras/profiling.py\", line 199, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py\", line 1197, in _prepare_inputs\n",
      "    generation_batch = self._generate_and_score_completions(generation_batch)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py\", line 1855, in _generate_and_score_completions\n",
      "    ) = self._generate(prompts)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py\", line 1737, in _generate\n",
      "    prompt_ids, completion_ids, logprobs, extra_fields = self._generate_single_turn(prompts)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py\", line 1306, in _generate_single_turn\n",
      "    self._move_model_to_vllm()\n",
      "  File \"/home/palebluedot/rapidfireai/rapidfireai/ml/trainer.py\", line 165, in _move_model_to_vllm\n",
      "    llm_model.load_weights([(name, param.data)])\n",
      "  File \"/home/palebluedot/rapidfireai/rapidfireai/utils/vllm_subprocess.py\", line 245, in load_weights\n",
      "    raise RuntimeError(f\"vLLM subprocess error: {response.get('error')}\")\n",
      "RuntimeError: vLLM subprocess error: 'LLMEngine' object has no attribute 'model_executor'\n",
      "\n",
      "Run 2 has failed: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstandingTraceback (most recent call last):\n",
      "  File \"/home/palebluedot/rapidfireai/rapidfireai/backend/worker.py\", line 498, in serve_forever\n",
      "    self.run_fit(run_id, chunk_id, multi_worker_details, create_model_fn)\n",
      "  File \"/home/palebluedot/rapidfireai/rapidfireai/backend/worker.py\", line 321, in run_fit\n",
      "    trainer_instance.train()\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 2325, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/transformers/trainer.py\", line 2740, in _inner_training_loop\n",
      "    self.optimizer.step()\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/accelerate/optimizer.py\", line 179, in step\n",
      "    self.optimizer.step(closure)\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py\", line 133, in wrapper\n",
      "    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 82, in _use_grad\n",
      "    ret = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/adam.py\", line 247, in step\n",
      "    adam(\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 150, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/adam.py\", line 953, in adam\n",
      "    func(\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/adam.py\", line 613, in _multi_tensor_adam\n",
      "    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 546, in _group_tensors_by_device_and_dtype\n",
      "    return _group_tensors_by_device_and_dtype(tensorlistlist, with_indices)  # type: ignore[return-value, arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/palebluedot/rapidfireai/.venv/lib/python3.12/site-packages/torch/utils/_foreach_utils.py\", line 48, in _group_tensors_by_device_and_dtype\n",
      "    return torch._C._group_tensors_by_device_and_dtype(tensorlistlist, with_indices)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
    "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=4,num_gpus=2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Current Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
