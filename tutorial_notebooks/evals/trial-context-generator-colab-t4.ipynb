{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from rapidfireai.evals.experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a605a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee571098",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\").select(range(512))\n",
    "print(f\"Loaded {len(dataset)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bbf90",
   "metadata": {},
   "source": [
    "### Inference Pipeline Config using `VLLMModelConfig`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecf9d6",
   "metadata": {},
   "source": [
    "##### We will now build a fast inference pipeline using vLLM with a full suite of context engineering capabilities including retrieval augmented generation (RAG), in-context learning with fewshot examples, with evaluation metrics powered by online aggregation.\n",
    "\n",
    "You will be able to experiment with and tune configurations:\n",
    "- Document splitting and chunking\n",
    "- Embeddings\n",
    "- Document and vector stores\n",
    "- Retrieval techniques\n",
    "- Reranking techniques\n",
    "- Prompt engineering and in-context learning\n",
    "- Dynamically selecting fewshot examples\n",
    "- Model generation settings\n",
    "- Post-processing\n",
    "- Evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77c516",
   "metadata": {},
   "source": [
    "##### We will use locally hosted models for both embedding and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a21ee",
   "metadata": {},
   "source": [
    "### RAG Implementation using `LangChainRagSpec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from rapidfireai.evals.rag.rag_pipeline import LangChainRagSpec\n",
    "\n",
    "batch_size = 32  # Reduced from 128 for T4\n",
    "\n",
    "# Shared document loader and text splitter\n",
    "document_loader = DirectoryLoader(\n",
    "    path=\"../data/gsm8k\",\n",
    "    glob=\"*.txt\",\n",
    "    recursive=True,\n",
    "    sample_seed=1337\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"gpt2\", chunk_size=128, chunk_overlap=32\n",
    ")\n",
    "\n",
    "# GPU-based RAG - Configuration 1 (retrieve 2 documents - reduced from 3)\n",
    "rag_gpu_k2 = LangChainRagSpec(\n",
    "    document_loader=document_loader,\n",
    "    text_splitter=text_splitter,\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    retriever=None,\n",
    "    vector_store=None,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},  # Retrieve 2 documents\n",
    "    reranker=None,\n",
    "    enable_gpu_search=True\n",
    ")\n",
    "\n",
    "# GPU-based RAG - Configuration 2 (retrieve 3 documents)\n",
    "rag_gpu_k3 = LangChainRagSpec(\n",
    "    document_loader=document_loader,\n",
    "    text_splitter=text_splitter,\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    retriever=None,\n",
    "    vector_store=None,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},  # Retrieve 3 documents\n",
    "    reranker=None,\n",
    "    enable_gpu_search=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776902e",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Manager using `PromptManager`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from rapidfireai.evals.rag.prompt_manager import PromptManager\n",
    "\n",
    "INSTRUCTIONS = \"You are a helpful assistant that is good at solving math problems. You think step by step and ALWAYS output the final answer after '####'.\"\n",
    "\n",
    "# Keep same examples but reduce k values\n",
    "\n",
    "# Prompt Manager Configuration 1 (1 fewshot example - reduced from 2)\n",
    "fewshot_prompt_manager_k1 = PromptManager(\n",
    "    instructions=INSTRUCTIONS,\n",
    "    examples=examples,  # Same examples as before\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    example_selector_cls=SemanticSimilarityExampleSelector,\n",
    "    example_prompt_template=PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=\"Question: {question}\\nAnswer: {answer}\",\n",
    "    ),\n",
    "    k=1,  # 1 fewshot example\n",
    ")\n",
    "\n",
    "# Prompt Manager Configuration 2 (2 fewshot examples - reduced from 3)\n",
    "fewshot_prompt_manager_k2 = PromptManager(\n",
    "    instructions=INSTRUCTIONS,\n",
    "    examples=examples,\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        'model_name': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'encode_kwargs': {'normalize_embeddings': True, 'batch_size': batch_size}\n",
    "    },\n",
    "    example_selector_cls=SemanticSimilarityExampleSelector,\n",
    "    example_prompt_template=PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=\"Question: {question}\\nAnswer: {answer}\",\n",
    "    ),\n",
    "    k=2,  # 2 fewshot examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8050d",
   "metadata": {},
   "source": [
    "### Context engineering using `ContextGenerator` = RAG + Prompt Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai.evals.rag.context_generator import ContextGenerator\n",
    "\n",
    "# Context Generator 1: k=2 RAG docs + k=1 fewshot example (lightweight)\n",
    "context_generator_1 = ContextGenerator(\n",
    "    rag_spec=rag_gpu_k2,\n",
    "    prompt_manager=fewshot_prompt_manager_k1\n",
    ")\n",
    "\n",
    "# Context Generator 2: k=3 RAG docs + k=2 fewshot examples (richer context)\n",
    "context_generator_2 = ContextGenerator(\n",
    "    rag_spec=rag_gpu_k3,\n",
    "    prompt_manager=fewshot_prompt_manager_k2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546221",
   "metadata": {},
   "source": [
    "### Model config using `VLLMModelConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956201bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai.evals.utils.config import VLLMModelConfig\n",
    "\n",
    "# Pipeline 1: Qwen 0.5B + Context 1 (ultra-light, fits easily in T4)\n",
    "pipeline_1 = VLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",  # 0.5B params\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.5,  # Conservative for T4\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 1536,  # Reduced from 2048\n",
    "        \"disable_log_stats\": True,\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 384,  # Reduced from 512\n",
    "    },\n",
    "    context_generator=context_generator_1\n",
    ")\n",
    "\n",
    "# Pipeline 2: Qwen 1.5B + Context 2 (still fits in T4 with room to spare)\n",
    "pipeline_2 = VLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\",  # 1.5B params\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.5,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"max_model_len\": 1536,\n",
    "        \"disable_log_stats\": True,\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 384,\n",
    "    },\n",
    "    context_generator=context_generator_2\n",
    ")\n",
    "\n",
    "# Create list of (name, config) tuples - ONLY 2 pipelines\n",
    "pipelines = [\n",
    "    (\"0.5B_LightContext\", pipeline_1),\n",
    "    (\"1.5B_RichContext\", pipeline_2),\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(pipelines)} pipelines (T4-optimized):\")\n",
    "print(f\"  - Pipeline 1: Qwen 0.5B + light context (2 RAG docs, 1 fewshot)\")\n",
    "print(f\"  - Pipeline 2: Qwen 1.5B + rich context (3 RAG docs, 2 fewshot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Utility, Preprocessor, Postprocessor, Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(answer):\n",
    "    solution = re.search(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", answer)\n",
    "    if solution is None:\n",
    "        return \"0\"\n",
    "    final_solution = solution.group(0)\n",
    "    final_solution = final_solution.split(\"#### \")[1].replace(\",\", \"\")\n",
    "    return final_solution\n",
    "\n",
    "def preprocess_fn(batch: Dict[str, List], context_generator: ContextGenerator) -> Dict[str, List]:\n",
    "\n",
    "    INSTRUCTIONS = context_generator.get_instructions()\n",
    "\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": INSTRUCTIONS\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f'Here are some examples: \\n{examples}. \\nHere is some additional context:\\n{context}. \\nNow answer the following question:\\n{question}'\n",
    "                }\n",
    "            ]\n",
    "            for question, examples,context in zip(\n",
    "                batch[\"question\"],\n",
    "                context_generator.get_fewshot_examples(user_queries=batch[\"question\"]),\n",
    "                context_generator.get_context(batch_queries=batch[\"question\"])\n",
    "            )\n",
    "        ],\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "def postprocess_fn(batch: Dict[str, List]) -> Dict[str, List]:\n",
    "    batch[\"model_answer\"] = [extract_solution(answer) for answer in batch[\"generated_text\"]]\n",
    "    batch[\"ground_truth\"] = [extract_solution(answer) for answer in batch[\"answer\"]]\n",
    "    return batch\n",
    "\n",
    "def compute_metrics_fn(batch: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    correct = sum(1 for pred, gt in zip(batch[\"model_answer\"], batch[\"ground_truth\"])\n",
    "                  if pred == gt)\n",
    "    total = len(batch[\"model_answer\"])\n",
    "    return {\n",
    "        \"Correct\": {\"value\": correct},\n",
    "        \"Total\": {\"value\": total},\n",
    "    }\n",
    "\n",
    "def accumulate_metrics_fn(aggregated_metrics: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    # aggregated_metrics is a dict of lists: {\"Correct\": [5, 3, 7], \"Total\": [10, 8, 12]}\n",
    "    correct = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Correct\", [{}]))\n",
    "    total = sum(m.get(\"value\", 0) for m in aggregated_metrics.get(\"Total\", [{}]))\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return {\n",
    "        \"Total\": {\"value\": total},\n",
    "        \"Correct\": {\"value\": correct, \"is_distributive\": True, \"value_range\": (0, 1)}, # 0 (min) if not correct, 1 if correct (max)\n",
    "        \"Accuracy\": {\"value\": accuracy, \"is_algebraic\": True, \"value_range\": (0, 1)} # Algebraic metric for online aggregation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4699f9",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4 is a single GPU, so use num_actors=1\n",
    "experiment = Experiment(\n",
    "    experiment_name=\"trial-context-generator-colab-t4\",\n",
    "    num_actors=1  # Changed from 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_run = experiment.run_evals(\n",
    "    configs=pipelines,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,  # 32\n",
    "    num_shards=4,\n",
    "    preprocess_fn=preprocess_fn,\n",
    "    postprocess_fn=postprocess_fn,\n",
    "    compute_metrics_fn=compute_metrics_fn,\n",
    "    accumulate_metrics_fn=accumulate_metrics_fn,\n",
    "    online_strategy_kwargs={\"strategy_name\": \"normal\", \"confidence_level\": 0.95, \"use_fpc\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756344e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS FOR ALL PIPELINES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for pipeline_id, (aggregated_results, metrics) in results_by_pipeline.items():\n",
    "    # Get pipeline name from the original configs\n",
    "    pipeline_name = [name for name, _ in pipelines][pipeline_id - 1] if pipeline_id <= len(pipelines) else f\"Pipeline {pipeline_id}\"\n",
    "\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Pipeline: {pipeline_name.upper()} (ID: {pipeline_id})\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(json.dumps(metrics, indent=4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE OUTPUTS FROM EACH PIPELINE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "num_examples_to_show = 2  # Show 2 examples per pipeline\n",
    "\n",
    "for pipeline_id, (aggregated_results, metrics) in results_by_pipeline.items():\n",
    "    # Get pipeline name from the original configs\n",
    "    pipeline_name = [name for name, _ in pipelines][pipeline_id - 1] if pipeline_id <= len(pipelines) else f\"Pipeline {pipeline_id}\"\n",
    "\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Pipeline: {pipeline_name.upper()} (ID: {pipeline_id})\")\n",
    "    print(f\"{'-'*80}\\n\")\n",
    "\n",
    "    samples_available = min(num_examples_to_show, metrics['Samples Processed']['value'])\n",
    "\n",
    "    for i in range(samples_available):\n",
    "        print(f\"\\n{'~'*40}\")\n",
    "        print(f\"Example {i+1}/{samples_available}\")\n",
    "        print(f\"{'~'*40}\")\n",
    "\n",
    "        prompt = aggregated_results['prompts'][i]\n",
    "        print(f\"\\nSystem Instructions:\")\n",
    "        print(f\"{prompt[0]['content'][:200]}...\")  # Truncate long instructions\n",
    "\n",
    "        print(f\"\\nUser Query + Context:\")\n",
    "        print(f\"{prompt[1]['content'][:300]}...\")  # Truncate long context\n",
    "\n",
    "        print(f\"\\nModel Output:\")\n",
    "        print(f\"{aggregated_results['generated_text'][i]}\")\n",
    "\n",
    "        print(f\"\\nGround Truth:\")\n",
    "        print(f\"{aggregated_results['ground_truth'][i]}\")\n",
    "\n",
    "        print(f\"\\nExtracted Answer:\")\n",
    "        print(f\"Model: {aggregated_results['model_answer'][i]} | Truth: {aggregated_results['ground_truth'][i]}\")\n",
    "\n",
    "        is_correct = aggregated_results['model_answer'][i] == aggregated_results['ground_truth'][i]\n",
    "        print(f\"✓ CORRECT\" if is_correct else \"✗ INCORRECT\")\n",
    "\n",
    "    print(f\"\\n{'-'*80}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
