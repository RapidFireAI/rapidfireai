{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFV9RqPPlQxF"
   },
   "source": [
    "# Project: SFT for Child-facing Chatbot\n",
    "- **Dataset**: Self-generated dataset using the Gemini API, synthesised by combining labels and children's speech texts from two Hugging Face children-focused datasets, and augmenting them with age-appropriate, label-consistent responses generated by Gemini. [Link: https://huggingface.co/datasets/yxpan/children_sft_dataset]\n",
    "\n",
    "- **Goal**: Train a small language model on the self-generated dataset to produce age-appropriate, supportive, and instruction-aligned responses to children's questions.\n",
    "\n",
    "- **Baseline Model**: Tiny Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWN-5snHmvNZ"
   },
   "source": [
    "# Install rapidfireai and start service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmQBVcgsmpJF",
    "outputId": "6be03963-d6bc-4b3c-9d6c-173305551437"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import rapidfireai\n",
    "    print(\"\u2705 rapidfireai already installed\")\n",
    "except ImportError:\n",
    "    %pip install rapidfireai\n",
    "    !rapidfireai init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erOwNqdbm9R2",
    "outputId": "fa7ab231-72b0-41ec-c2b5-6aa35fa488cd"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from time import sleep\n",
    "subprocess.Popen([\"rapidfireai\", \"start\"])\n",
    "# sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itbUBvVe0ci6"
   },
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2CvIbo4nKXV"
   },
   "source": [
    "# Load and Train-Eval Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "ec113aab063e46609ad1a08f345e1757",
      "7706cbf435a7410f85558ad344fb9f35",
      "506f0cb96ecc42109d53316491c58bc6",
      "710790060a0d4e27a22b615604fe819e",
      "786c54e0b3814704a6b379d1df235615",
      "d5a97230304042689ccdf871febdeb44",
      "203e2d9ca25d435198fd5121dfbf4b60",
      "e0863a2cdef747da8a0b65b12fe9c4c2",
      "dc9b6b8123074c24a47654b180e2fdd7",
      "78d6fa38d66b4cd091c1cb65cad0e412",
      "7bc2474fa39a49cbbe96e95a482e8c90",
      "7424eef9a8cb4df98ec25ffffb963f0f",
      "d1028f15105d469cb979e9203c6d6fc6",
      "b0d81234e3e644c39a46cf3a48499474",
      "3226969bf35a492fadfa1a5a2ec2fa40",
      "007e7348a29f4324b3ac6aba9dd02327",
      "5cfb729390494eb1b872ba06921f5e1d",
      "daca230cbe9f4896b24297886bc46e23",
      "38e4df0ecefc4050aa3a5bfd78546ac4",
      "2012df5432c64eff84c57d7f43c616d3",
      "99aba5da1e124f99a734c318b4b48e17",
      "33fb88241f9f4c82865d11e87f2145f7",
      "23da0cb434204c97ab5734f91affc6b7",
      "5a056bc095d14c23a8293e6ebc4840ca",
      "fcad54a73d4348079d6b569818e4d0e0",
      "bb4d0e5f278a41b097036578719a753c",
      "884175e22e1c4c78ac9d41856dfbac6f",
      "ad52ef0a84bb4470b8832cbee9e539a6",
      "84f9fbbb78164864a858547f3d2e770c",
      "4df943a4adf244b9968aed31e39c84e4",
      "b1b4479201c646a584dd61fc9320137b",
      "85f885251fe6480f8610dab898306e42",
      "4a50cdb5ffef4e42a48ba13516d429f8"
     ]
    },
    "id": "M6uOGvvmmrNT",
    "outputId": "1276874e-0d5f-48d1-985a-ceea06e1d15a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"yxpan/children_sft_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDN6_M2NzlAQ"
   },
   "outputs": [],
   "source": [
    "train_dataset = ds['train'].select(range(500))    # !! make sure it does not exceed GPU memory constraints\n",
    "eval_dataset = ds['train'].select(range(500,600))\n",
    "train_dataset=train_dataset.shuffle(seed=42)\n",
    "eval_dataset=eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fUwhJjsqkOI",
    "outputId": "f1ff7ec1-f3ab-444f-dd33-2ee1359e0091"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXG3uh-_nW4m"
   },
   "source": [
    "# Dataprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNwEsGPmpFMI"
   },
   "outputs": [],
   "source": [
    "def sample_formatting_function(row):\n",
    "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
    "\n",
    "    system_content = (\n",
    "        f\"You are talking to a child aged {row['age']}. \"\n",
    "        f\"Generate a friendly response with age-appropriate knowledge.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Standard ChatML-style dictionary\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": row['instruction']},\n",
    "            {\"role\": \"assistant\", \"content\": row['response']}\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfAT1LW1LfLP",
    "outputId": "d2229767-4a3c-455a-edfc-4b46fde1f386"
   },
   "outputs": [],
   "source": [
    "sample_formatting_function(eval_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EYEGyMe0F7v"
   },
   "source": [
    "# Define metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdIZhFZdCIod",
    "outputId": "d3cefa8a-55fd-47c7-a083-c8ec8ddc5f11"
   },
   "outputs": [],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNArZTy20J-U"
   },
   "outputs": [],
   "source": [
    "def sample_compute_metrics(eval_preds):\n",
    "    import evaluate\n",
    "    import numpy as np\n",
    "\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=labels)\n",
    "    bert_results = bertscore.compute(predictions=predictions, references=labels, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_results[\"rougeL\"], 4),\n",
    "        \"bert_f1\": round(np.mean(bert_results[\"f1\"]), 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg-el78g0z8x"
   },
   "source": [
    "# Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3kbo4z001lX",
    "outputId": "61375afe-703e-4540-bf41-543e2c502ac4"
   },
   "outputs": [],
   "source": [
    "my_experiment = 'sft-child-age'\n",
    "experiment = Experiment(experiment_name=my_experiment, mode=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQbRg-ND1TJr"
   },
   "source": [
    "# Create Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85UgXwp-yu0I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Configure RapidFire to use TensorBoard\n",
    "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'  # Options: 'mlflow', 'tensorboard', 'both'\n",
    "# TensorBoard log directory will be auto-created in experiment path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLpsw1Ct1XYs",
    "outputId": "2be2e97c-10b5-4045-c199-bf7e55add631"
   },
   "outputs": [],
   "source": [
    "# Get experiment path\n",
    "from rapidfireai.fit.db.rf_db import RfDb\n",
    "\n",
    "db = RfDb()\n",
    "experiment_path = db.get_experiments_path(my_experiment)\n",
    "tensorboard_log_dir = f\"{experiment_path}/tensorboard_logs/{my_experiment}\"\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW-cenjb1hOa"
   },
   "source": [
    "# Define experiment configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQizdV4V1crN"
   },
   "outputs": [],
   "source": [
    "# 2 LoRA PEFT configs lite with different adapter capacities\n",
    "peft_configs_lite = List([\n",
    "    RFLoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=4,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # Standard transformer naming\n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Standard naming\n",
    "        bias=\"none\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2 base models x 3 peft configs = 6 combinations in total\n",
    "config_set_lite = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B model\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=1e-5,  # Higher LR for very small model\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            max_steps=256,\n",
    "            gradient_accumulation_steps=1,   # No accumulation needed\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=20,\n",
    "            fp16=True,\n",
    "            # report_to=\"tensorboard\",\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.8,  # Higher temp for tiny model\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 30,         # Reduced top_k\n",
    "            \"repetition_penalty\": 1.05,\n",
    "        }\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B model\n",
    "        peft_config=peft_configs_lite,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=1e-4,  # Higher LR for very small model\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=4,  # Larger batch size\n",
    "            per_device_eval_batch_size=4,\n",
    "            max_steps=256,\n",
    "            gradient_accumulation_steps=1,   # No accumulation needed\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=20,\n",
    "            fp16=True,\n",
    "            # report_to=\"tensorboard\",\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        formatting_func=sample_formatting_function,\n",
    "        compute_metrics=sample_compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.8,  # Higher temp for tiny model\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 30,         # Reduced top_k\n",
    "            \"repetition_penalty\": 1.05,\n",
    "        }\n",
    "    ),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0khKzoRf2z3I"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "def sample_create_model(model_config):\n",
    "     \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
    "     from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM\n",
    "\n",
    "     model_name = model_config[\"model_name\"]\n",
    "     model_type = model_config[\"model_type\"]\n",
    "     model_kwargs = model_config[\"model_kwargs\"]\n",
    "\n",
    "     if model_type == \"causal_lm\":\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"seq2seq_lm\":\n",
    "          model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"masked_lm\":\n",
    "          model = AutoModelForMaskedLM.from_pretrained(model_name, **model_kwargs)\n",
    "     elif model_type == \"custom\":\n",
    "          # Handle custom model loading logic, e.g., loading your own checkpoints\n",
    "          # model = ...\n",
    "          pass\n",
    "     else:\n",
    "          # Default to causal LM\n",
    "          model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "     return (model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq-AWYSV28JV"
   },
   "outputs": [],
   "source": [
    "# Grid search across all 4 config combinations\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set_lite,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4UgYFOl3FLL"
   },
   "source": [
    "# Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "sEF7j_fJ3HCu",
    "outputId": "c0b0d5f8-9b36-4741-cea4-ea4ddfccf3b3"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Q61qfr2wAR"
   },
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "6QNmICOu3KZR",
    "outputId": "278daac5-5ff0-4e47-87c3-15d8a1195aef"
   },
   "outputs": [],
   "source": [
    "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
    "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=4, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwf6JoRomim4"
   },
   "source": [
    "# Launch Interactive Run Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616,
     "referenced_widgets": [
      "81d5c4530fdb45bc9cc346d35bb47016",
      "1cba2bec6a7740da8efb903a9baa9725",
      "b872595244824dd686671d5a7de58a87",
      "916138f8f6644280ad71b8e1fb53eee4",
      "3cee02423c1b443aa4f0501031dfef72",
      "0b77ac3a049b4e9ca41588f1f7208249",
      "691cecdfd5954e5a82d390277c139a3e",
      "3e48a26796a94b3db76250e56b581888",
      "e20465ab335d4508aa8e7abb3d398b93",
      "6b35dd3675ad460c862c5238936b9b64",
      "808695f0a0a440ff8dfd90e699349c10",
      "0f92aee3128a4cd8a4c288bf388a19a7",
      "8fa0791d1f57410ab6b1596849911b27",
      "011da64fc5fe4e29b7f223ea6b4bd1fc",
      "b8c7fdf9285847f89e1e1f72d791b3fc",
      "dd541466ec0d4bf09653d238aa4d2e7e",
      "86cf731dcbb245d4a714b36216d31729",
      "a84799be34de4389a4b50ae8039a7bfd",
      "2cc1554598d24937a1547cbbf0880ba3",
      "52dce46c16e24a15a21cbf3c214b1152",
      "8a2af27efea54dbca645dcac4a1dd433",
      "d0335bb0ba464683b9499b2daa82eb8a",
      "4f5a1845937d4a08bbbade89d483b538",
      "4ef8704dfe48442d9c5d1e0e33388fc8",
      "db41942b78fe439b952c640a8a877964",
      "0b4d780be9954b849cd187538b2cb761",
      "4d74616869384b4c869002a1c19fbe1f",
      "1e5519cf5e164c90b89f40198c9dee87",
      "1dd2c937697443aaa07c171d2c97aeee",
      "7f8ba98ad19f4b18ac055eff18e62727",
      "d4d64f3e148e4d94b3c1b415db1cdda1",
      "ad48411e27124ee19a9e14d32b0d2c04",
      "957ce0fbf08f446ba27af5301c255dbe",
      "a11accd747c24a64b0b1539e7fbb639a",
      "3302e58efffe4eaf9d1d399e2e6b42c4",
      "65b65095082c47d09705f9aee9355bd0",
      "450a2b0d9e694f2d80229e6c58c7648f",
      "134fd9cd4d6f49cbb5895f5c07f27b5a",
      "21c5cb3688684be3ab7389ad3f04fefa",
      "86a288864eb540e49ed9a1812cce1a8d",
      "d415ecd2d1c8477bb5f0bf2d92502477",
      "032929aade34450fbca0a32cbe3c066d",
      "c2935295fa4b49b69b4ab9a498db72b8",
      "25a5b768ae9842b1a634d77be5fab4e9",
      "545969db88124e75a74242c91cf87115",
      "99804549b26348d1b58a0115892b8bf3",
      "c023f0e560ee49d1af9464e05b8359aa",
      "c02c6f9202324ef7bcee986bbc1bf6d5",
      "1a60d3bf982c42f79b461cc388b2a759",
      "980c60e870b145d39763a7d434a3c858",
      "b3bc2036074e4957924458150274be44",
      "be867f3fc4614e4bb69b36ab2085d4d1",
      "bb75eba518c74a92bc976bf3b83465d3",
      "42dba492e8ad4d789ee1d677adeb385c",
      "de7c10a51a044737bbe3e2a06617949c",
      "a86336bf217b4deb9240755ba02383b2",
      "ffce2faa89924f2c9796d1dba66092d4",
      "9aa8aff69721476692f51d6494917908",
      "3bec3bb1526a4ca9a34b375f522f55ce",
      "cb62274999f54d87b4bedfea97e233a4",
      "fd1480c666b04da7b54b88c172384d34",
      "4996f81b4a48472ba2440903c5c4d9a2",
      "25208fa27872440d9fee9f97a5706567",
      "7bff2b885b3a4191a2c5dd9ea6643cd8",
      "93053d0d2f024b1ab90fadc076bef9ec",
      "f17cbb978a2948cba9347e46e004502f",
      "06cf4986572e491a821b7de71b00398a",
      "617f4b94ac6f4ab6bc9e667fff86377d",
      "2f1a5dafc8a34d7d8f0926959bb5e2a4",
      "2fa13c3987e44ff187a01560a66525cb",
      "910b93d5e2be4605a6e1480f060478c8",
      "660c39d47ae94f778b27760b01430240",
      "7364f64f91384be2ae63640f1f165610"
     ]
    },
    "id": "mGjsfhgzml3o",
    "outputId": "6cd895a3-e570-48dc-ab0a-533bf413c612"
   },
   "outputs": [],
   "source": [
    "# Create Interactive Controller\n",
    "sleep(15)\n",
    "from rapidfireai.fit.utils.interactive_controller import InteractiveController\n",
    "\n",
    "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8851\")\n",
    "controller.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhGCPKSVmmiB"
   },
   "source": [
    "# Press the Button to End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 60
    },
    "id": "rn08IFZkmpIq",
    "outputId": "df1f4388-8bda-440e-f79d-3b7fc8a913b9"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n",
    "'''))\n",
    "\n",
    "# eval_js blocks until the Promise resolves\n",
    "output.eval_js('''\n",
    "new Promise((resolve) => {\n",
    "    document.getElementById(\"continue-btn\").onclick = () => {\n",
    "        document.getElementById(\"continue-btn\").disabled = true;\n",
    "        document.getElementById(\"continue-btn\").innerText = \"Continuing...\";\n",
    "        resolve(\"clicked\");\n",
    "    };\n",
    "})\n",
    "''')\n",
    "\n",
    "# Actually end the experiment after the button is clicked\n",
    "experiment.end()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE6mF6vkmu9m"
   },
   "source": [
    "# View Tensorboard Plots and Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "id": "cPvEvN9ynAOx",
    "outputId": "b6b21d66-492e-49c1-f8d7-308dffa07a77"
   },
   "outputs": [],
   "source": [
    "# View final logs\n",
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SIs48YDnecq"
   },
   "source": [
    "# View Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TkjpAwONngMr",
    "outputId": "fb666067-e5b1-4240-fe6f-ff58b7d40cbf"
   },
   "outputs": [],
   "source": [
    "# Get the experiment-specific log file\n",
    "from IPython.display import display, Pretty\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "display(Pretty(f\"\ud83d\udcc4 Experiment Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"\u274c Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "Q2Fo4yu2nj7v",
    "outputId": "64e6856f-f554-4555-be77-8a0e4b258782"
   },
   "outputs": [],
   "source": [
    "# Get the training-specific log file\n",
    "log_file = experiment.get_log_file_path(\"training\")\n",
    "\n",
    "display(Pretty(f\"\ud83d\udcc4 Training Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"\u274c Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPShjzSdyVPj"
   },
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}