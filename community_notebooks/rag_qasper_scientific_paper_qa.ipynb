{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamRolander/RAG-Experiments/blob/main/Final_RapidFire_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHXZdlFM8TA"
      },
      "source": [
        "# Retrieval-First RAG Optimization: A Systematic Study on QASPER\n",
        "\n",
        "**Dataset:** QASPER (Question Answering on Scientific Papers)\n",
        "\n",
        "## Abstract\n",
        "This notebook implements a systematic, two-phase optimization strategy to maximize retrieval performance (MRR, Recall@K, NDCG@K) on the complex QASPER dataset. Unlike standard RAG implementations, we decouple the pipeline into isolated experimental variables:\n",
        "\n",
        "\n",
        "1.  **Phase 1 (Granularity):** Comparing Fixed-size vs. Semantic/Structural chunking & overlap strategies.\n",
        "2.  **Phase 2 (Representation):** Benchmarking 2021 baselines against SOTA 2024 embedding models (BGE-M3, Nomic, etc.).\n",
        "\n",
        "**Framework:** RapidFire AI is utilized for hyperparallel experiment execution and grid-search capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMQfNLc4sM4o"
      },
      "source": [
        "## 1. Environment Setup & Shared Utilities\n",
        "\n",
        "We initialize the environment by installing the RapidFire framework and necessary NLP dependencies. To ensure code modularity, we define our **Evaluation Metrics** (MRR, Recall@K, NDCG) and **Data Processing** functions globally here. These will be reused across all three experimental phases to ensure consistent measurement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pJxBZwWsXgK",
        "outputId": "39fea9b3-64db-402e-ca79-10ff69394cbf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import gc\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "os.system(\"pkill -f pyright\")\n",
        "os.system(\"pkill -f vllm\")\n",
        "os.system(\"ray stop --force\")\n",
        "\n",
        "!pip uninstall fsspec -y -q\n",
        "!pip install \"fsspec==2025.3.0\" # Pinning to a stable 2024 version is safer for now\n",
        "!pip install \"huggingface-hub<1.0.0\"\n",
        "!pip install sentence-transformers rank_bm25 nltk -q\n",
        "!apt install libomp-dev\n",
        "!pip install --upgrade faiss-gpu-cu12\n",
        "!pip install \"datasets==2.21.0\" \"huggingface-hub<1.0.0\"\n",
        "!pip install einops -q\n",
        "\n",
        "# Check if libraries are present; if not, run setup\n",
        "try:\n",
        "    import rapidfireai\n",
        "    import datasets\n",
        "    print(f\"‚úì RapidFire {rapidfireai.__version__} already installed.\")\n",
        "except ImportError:\n",
        "    %pip install rapidfireai==0.14.0  # Takes 1 min\n",
        "    !rapidfireai init --evals # Takes 1 min\n",
        "    print(\"‚ö†Ô∏è  PLEASE RESTART RUNTIME NOW (Runtime > Restart Session) to load new versions.\")\n",
        "\n",
        "# --- 2. Global Imports & Configuration ---\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# RapidFire & LangChain Imports\n",
        "try:\n",
        "    from rapidfireai import Experiment\n",
        "    from rapidfireai.automl import RFLangChainRagSpec, RFvLLMModelConfig, RFGridSearch\n",
        "    from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    from langchain_text_splitters import TextSplitter\n",
        "except ImportError:\n",
        "    print(\"‚ùå Libraries not found. Did you restart the runtime after installation?\")\n",
        "\n",
        "# Set Random Seeds for Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "print(f\"‚úì Environment Ready. Seed set to {SEED}.\")\n",
        "\n",
        "# --- 3. Shared Metric Functions (Used in Phase 1, 2, & 3) ---\n",
        "\n",
        "def compute_metrics_fn(batch: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Compute retrieval metrics (MRR, Recall, NDCG) for a batch.\"\"\"\n",
        "    total_queries = len(batch[\"query\"])\n",
        "    mrrs, recalls_10, ndcgs_10 = [], [], []\n",
        "\n",
        "    # Handle variable key names for ground truth\n",
        "    gt_key = \"ground_truth_documents\" if \"ground_truth_documents\" in batch else \"ground_truth_id\"\n",
        "\n",
        "    for retrieved, gt in zip(batch[\"retrieved_documents\"], batch[gt_key]):\n",
        "        # Normalization: Ensure GT is a set of strings\n",
        "        if isinstance(gt, list):\n",
        "            relevant = set(str(g) for g in gt)\n",
        "        else:\n",
        "            relevant = {str(gt)}\n",
        "\n",
        "        # MRR (Mean Reciprocal Rank)\n",
        "        mrr = 0.0\n",
        "        for i, doc_id in enumerate(retrieved):\n",
        "            if doc_id in relevant:\n",
        "                mrr = 1.0 / (i + 1)\n",
        "                break\n",
        "        mrrs.append(mrr)\n",
        "\n",
        "        # Recall@10\n",
        "        retrieved_set_10 = set(retrieved[:10])\n",
        "        recalls_10.append(len(retrieved_set_10 & relevant) / len(relevant) if relevant else 0.0)\n",
        "\n",
        "        # NDCG@10\n",
        "        dcg = sum(1.0 / math.log2(i + 2) for i, d in enumerate(retrieved[:10]) if d in relevant)\n",
        "        idcg = sum(1.0 / math.log2(i + 2) for i in range(min(10, len(relevant))))\n",
        "        ndcgs_10.append(dcg / idcg if idcg > 0 else 0.0)\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        \"MRR\": {\"value\": sum(mrrs) / total_queries},\n",
        "        \"Recall@10\": {\"value\": sum(recalls_10) / total_queries},\n",
        "        \"NDCG@10\": {\"value\": sum(ndcgs_10) / total_queries},\n",
        "    }\n",
        "\n",
        "def accumulate_metrics_fn(aggregated: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Aggregate batch metrics into a final global score.\"\"\"\n",
        "    # Calculate total queries across all batches\n",
        "    total = sum(m[\"value\"] for m in aggregated[\"Total\"])\n",
        "    if total == 0: return {}\n",
        "\n",
        "    # Weighted average of batch metrics\n",
        "    metrics = [\"MRR\", \"Recall@10\", \"NDCG@10\"]\n",
        "    return {\n",
        "        metric: {\n",
        "            \"value\": sum(x[\"value\"] * n[\"value\"] for x, n in zip(aggregated[metric], aggregated[\"Total\"])) / total,\n",
        "            \"is_algebraic\": True\n",
        "        }\n",
        "        for metric in metrics\n",
        "    }\n",
        "\n",
        "print(\"‚úì Shared metric functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ6nSZ8UuIOm"
      },
      "source": [
        "## 2. Dataset Preparation (QASPER)\n",
        "\n",
        "QASPER contains 1,585 NLP papers with 5,049 expert-annotated questions. The data preprocessing pipeline below performs three critical steps:\n",
        "1.  **Corpus Flattening:** Converts nested JSON (sections/paragraphs) into a flat list of documents.\n",
        "2.  **Evidence Extraction:** Identifies the \"Gold Standard\" evidence spans for evaluation.\n",
        "3.  **RapidFire Serialization:** Exports the corpus to `.jsonl` and relevance judgments (qrels) to `.tsv` for efficient ingestion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzA6T7j_vCee"
      },
      "source": [
        "### I. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir6fm6XZRNr_",
        "outputId": "4802033e-13b8-4a3f-ae6f-64745424048f"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING QASPER DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Add trust_remote_code=True\n",
        "qasper = load_dataset(\"allenai/qasper\", trust_remote_code=True)\n",
        "\n",
        "print(f\"\\n‚úì Dataset loaded:\")\n",
        "print(f\"  Train: {len(qasper['train'])} papers\")\n",
        "print(f\"  Validation: {len(qasper['validation'])} papers\")\n",
        "\n",
        "# Combine train and validation for our corpus\n",
        "all_papers = list(qasper['train']) + list(qasper['validation'])\n",
        "print(f\"  Total: {len(all_papers)} papers\")\n",
        "\n",
        "# Explore structure\n",
        "sample_paper = all_papers[0]\n",
        "print(f\"\\n‚úì Paper structure:\")\n",
        "print(f\"  Keys: {list(sample_paper.keys())}\")\n",
        "print(f\"  Title: {sample_paper['title'][:80]}...\")\n",
        "print(f\"  Sections: {len(sample_paper['full_text']['section_name'])} sections\")\n",
        "print(f\"  Questions: {len(sample_paper['qas']['question'])} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Ll_YMxVp-0"
      },
      "source": [
        "### II. Preprocess QASPER into Corpus + Eval Set\n",
        "\n",
        "Convert QASPER's nested structure into:\n",
        "1. **Corpus**: Flat documents with full paper text\n",
        "2. **Eval Set**: Questions with ground truth paper IDs and evidence spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWbWhf27VsVo",
        "outputId": "2db1b135-c925-453e-e021-c5ed9445b443"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "import tqdm\n",
        "ListType = List\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING QASPER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def extract_full_text(paper: Dict) -> str:\n",
        "    \"\"\"Combine all sections of a paper into full text.\"\"\"\n",
        "    sections = paper['full_text']\n",
        "    full_text_parts = []\n",
        "\n",
        "    for section_name, paragraphs in zip(sections['section_name'], sections['paragraphs']):\n",
        "        if section_name:\n",
        "            full_text_parts.append(f\"\\n## {section_name}\\n\")\n",
        "        full_text_parts.extend(paragraphs)\n",
        "\n",
        "    return '\\n'.join(full_text_parts)\n",
        "\n",
        "\n",
        "def extract_evidence_text(paper: Dict, evidence_list: ListType) -> str:\n",
        "    \"\"\"Extract evidence text from paper given evidence annotations.\"\"\"\n",
        "    evidence_texts = []\n",
        "    for evidence in evidence_list:\n",
        "        if evidence.get('highlighted_evidence'):\n",
        "            evidence_texts.extend(evidence['highlighted_evidence'])\n",
        "    return ' '.join(evidence_texts)\n",
        "\n",
        "\n",
        "# Build corpus\n",
        "corpus = []\n",
        "paper_id_to_idx = {}\n",
        "\n",
        "for idx, paper in enumerate(tqdm.tqdm(all_papers, desc=\"Building corpus\")):\n",
        "    paper_id = paper['id']\n",
        "    full_text = extract_full_text(paper)\n",
        "\n",
        "    # Skip papers with very little text\n",
        "    word_count = len(full_text.split())\n",
        "    if word_count < 100:\n",
        "        continue\n",
        "\n",
        "    corpus.append({\n",
        "        'id': paper_id,\n",
        "        'title': paper['title'],\n",
        "        'abstract': paper['abstract'],\n",
        "        'full_text': full_text,\n",
        "        'word_count': word_count\n",
        "    })\n",
        "    paper_id_to_idx[paper_id] = len(corpus) - 1\n",
        "\n",
        "print(f\"\\n‚úì Corpus built: {len(corpus)} papers\")\n",
        "\n",
        "# Compute corpus statistics\n",
        "word_counts = [doc['word_count'] for doc in corpus]\n",
        "print(f\"\\n  Document length statistics:\")\n",
        "print(f\"    Mean: {np.mean(word_counts):.0f} words\")\n",
        "print(f\"    Median: {np.median(word_counts):.0f} words\")\n",
        "print(f\"    Min: {np.min(word_counts)} words\")\n",
        "print(f\"    Max: {np.max(word_counts)} words\")\n",
        "\n",
        "# Build evaluation set\n",
        "eval_set = []\n",
        "papers_with_questions = set()\n",
        "\n",
        "for paper in tqdm.tqdm(all_papers, desc=\"Building eval set\"):\n",
        "    paper_id = paper['id']\n",
        "\n",
        "    # Skip if paper not in corpus\n",
        "    if paper_id not in paper_id_to_idx:\n",
        "        continue\n",
        "\n",
        "    qas = paper['qas']\n",
        "\n",
        "    for q_idx, question in enumerate(qas['question']):\n",
        "        # Get answers and evidence\n",
        "        answers = qas['answers'][q_idx]\n",
        "\n",
        "        # Extract evidence from all annotators\n",
        "        all_evidence = []\n",
        "        for answer in answers['answer']:\n",
        "            if answer.get('highlighted_evidence'):\n",
        "                all_evidence.extend(answer['highlighted_evidence'])\n",
        "\n",
        "        # Only include questions with evidence (answerable from the paper)\n",
        "        if not all_evidence:\n",
        "            continue\n",
        "\n",
        "        eval_set.append({\n",
        "            'query': question,\n",
        "            'query_id': f\"{paper_id}_q{q_idx}\",\n",
        "            'ground_truth_id': paper_id,\n",
        "            'ground_truth_title': paper['title'],\n",
        "            'evidence_texts': all_evidence[:5],  # Keep top 5 evidence spans\n",
        "        })\n",
        "        papers_with_questions.add(paper_id)\n",
        "\n",
        "print(f\"\\n‚úì Eval set built: {len(eval_set)} questions\")\n",
        "print(f\"  From {len(papers_with_questions)} unique papers\")\n",
        "\n",
        "# Sample questions\n",
        "print(f\"\\n  Sample questions:\")\n",
        "for i, q in enumerate(random.sample(eval_set, min(3, len(eval_set)))):\n",
        "    print(f\"  {i+1}. {q['query'][:80]}...\")\n",
        "    print(f\"     Paper: {q['ground_truth_id']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI0sRTHiV6Je"
      },
      "source": [
        "### III. Save Preprocessed Data for RapidFire\n",
        "\n",
        "Export corpus and eval set in formats compatible with RapidFire's data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSZuj3tGWBdK",
        "outputId": "58723060-8887-4658-9fc2-1c9b9c726e2f"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPORTING DATA FOR RAPIDFIRE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create data directory\n",
        "data_dir = Path(\"qasper_data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save corpus as JSONL (for LangChain JSONLoader)\n",
        "corpus_path = data_dir / \"corpus.jsonl\"\n",
        "with open(corpus_path, 'w') as f:\n",
        "    for doc in corpus:\n",
        "        # Format for JSONLoader\n",
        "        record = {\n",
        "            '_id': doc['id'],\n",
        "            'text': doc['full_text'],\n",
        "            'title': doc['title'],\n",
        "            'abstract': doc['abstract']\n",
        "        }\n",
        "        f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "print(f\"‚úì Corpus saved: {corpus_path}\")\n",
        "\n",
        "# Save eval set\n",
        "eval_path = data_dir / \"eval_set.json\"\n",
        "with open(eval_path, 'w') as f:\n",
        "    json.dump(eval_set, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Eval set saved: {eval_path}\")\n",
        "\n",
        "# Create qrels (relevance judgments) for RapidFire compatibility\n",
        "qrels_data = []\n",
        "for item in eval_set:\n",
        "    qrels_data.append({\n",
        "        'query_id': item['query_id'],\n",
        "        'corpus_id': item['ground_truth_id'],\n",
        "        'relevance': 1\n",
        "    })\n",
        "\n",
        "qrels_df = pd.DataFrame(qrels_data)\n",
        "qrels_path = data_dir / \"qrels.tsv\"\n",
        "qrels_df.to_csv(qrels_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"‚úì Qrels saved: {qrels_path}\")\n",
        "\n",
        "# Save statistics\n",
        "stats = {\n",
        "    'corpus_size': len(corpus),\n",
        "    'eval_queries': len(eval_set),\n",
        "    'unique_papers_with_questions': len(papers_with_questions),\n",
        "    'avg_document_length': float(np.mean(word_counts)),\n",
        "    'median_document_length': float(np.median(word_counts)),\n",
        "}\n",
        "\n",
        "with open(data_dir / \"dataset_stats.json\", 'w') as f:\n",
        "    json.dump(stats, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Dataset statistics:\")\n",
        "for k, v in stats.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJyt6xOWSGd"
      },
      "source": [
        "### IV. Load Data into DataFrames\n",
        "\n",
        "Prepare data structures for experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-hEEsgCWUN3",
        "outputId": "66c384de-7fd6-4755-923d-7a020842ea1d"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING DATA FOR EXPERIMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load corpus\n",
        "corpus_df = pd.read_json(corpus_path, lines=True)\n",
        "corpus_df = corpus_df.rename(columns={'_id': 'id'})\n",
        "print(f\"‚úì Corpus loaded: {len(corpus_df)} documents\")\n",
        "\n",
        "# Load eval set\n",
        "eval_df = pd.DataFrame(eval_set)\n",
        "print(f\"‚úì Eval set loaded: {len(eval_df)} queries\")\n",
        "\n",
        "# Load qrels\n",
        "qrels = pd.read_csv(qrels_path, sep='\\t')\n",
        "qrels['corpus_id'] = qrels['corpus_id'].astype(str)\n",
        "print(f\"‚úì Qrels loaded: {len(qrels)} relevance judgments\")\n",
        "print(f\"Qrels corpus_id type: {qrels['corpus_id'].dtype}\")\n",
        "print(f\"Sample: {qrels['corpus_id'].iloc[0]} (type: {type(qrels['corpus_id'].iloc[0])})\")\n",
        "\n",
        "# Create document lookup\n",
        "doc_lookup = {row['id']: row['text'] for _, row in corpus_df.iterrows()}\n",
        "\n",
        "print(f\"\\n‚úì Data ready for experimentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA8AJPAbWbaW"
      },
      "source": [
        "## Phase 1: Granularity Optimization (Chunking)\n",
        "\n",
        "**Hypothesis:** Scientific literature contains complex logical boundaries. Semantic chunking (based on topic shifts) will yield higher retrieval precision than arbitrary fixed-window chunking.\n",
        "\n",
        "**Methodology:**\n",
        "We implement a factorial design comparing:\n",
        "* **Strategies:** Fixed Token (256/512), Structural (Section-based), and Semantic (BM25-similarity).\n",
        "* **Overlap:** None vs. Fixed (15%) vs. Semantic (context-aware extension).\n",
        "\n",
        "The winning configuration from this phase will be \"locked in\" for Phases 2 and 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5q6Mf_Wvvby"
      },
      "source": [
        "### I. Initialize Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "R7jd23tUWed6",
        "outputId": "575cb1de-b6f0-4fe4-86e9-569b0db9ad2a"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INITIALIZING RAPIDFIRE EXPERIMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "experiment = Experiment(\n",
        "    experiment_name=\"qasper-phase1-chunking\",\n",
        "    mode=\"evals\"\n",
        ")\n",
        "\n",
        "print(\"‚úì Experiment initialized: qasper-phase1-chunking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDyY665HWsjm"
      },
      "source": [
        "### II. Define Custom Chunking & Overlap Strategies\n",
        "\n",
        "**Chunking Schemes:**\n",
        "1. Fixed-size (256, 512 tokens)\n",
        "2. Structural (paragraph/section boundaries)\n",
        "3. Semantic (BM25-guided boundary detection)\n",
        "\n",
        "**Overlap Strategies:**\n",
        "1. No overlap (0%)\n",
        "2. Fixed overlap (15%)\n",
        "3. Semantic overlap (BM25-guided context extension)\n",
        "\n",
        "Full factorial: 4 chunkers √ó 3 overlaps = 12 configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W-pdeXnX-Iy",
        "outputId": "e44306a8-5936-4116-f9b9-fa9cc068ea6d"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import TextSplitter\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List as ListType, Iterable\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. BASE CHUNKING STRATEGIES\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class FixedTokenSplitter(TextSplitter):\n",
        "    \"\"\"Split text into fixed-size token chunks.\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 256, chunk_overlap: int = 0, **kwargs):\n",
        "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap, **kwargs)\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self._name = f\"fixed_{chunk_size}tok_overlap{chunk_overlap}\"\n",
        "\n",
        "    def split_text(self, text: str) -> ListType[str]:\n",
        "        # Import inside method for Ray worker safety\n",
        "        from nltk.tokenize import word_tokenize\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "        chunks = []\n",
        "        step = max(1, self.chunk_size - self.chunk_overlap)\n",
        "\n",
        "        for start in range(0, len(tokens), step):\n",
        "            end = min(start + self.chunk_size, len(tokens))\n",
        "            chunk_tokens = tokens[start:end]\n",
        "            if chunk_tokens:\n",
        "                chunks.append(' '.join(chunk_tokens))\n",
        "            if end >= len(tokens):\n",
        "                break\n",
        "        return chunks\n",
        "\n",
        "\n",
        "class StructuralSplitter(TextSplitter):\n",
        "    \"\"\"Split text at paragraph and section boundaries.\"\"\"\n",
        "\n",
        "    def __init__(self, min_chunk_size: int = 50, max_chunk_size: int = 500,\n",
        "                 overlap_sentences: int = 0, **kwargs):\n",
        "        super().__init__(chunk_size=max_chunk_size, chunk_overlap=overlap_sentences, **kwargs)\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.overlap_sentences = overlap_sentences\n",
        "        self._name = f\"structural_overlap{overlap_sentences}sent\"\n",
        "\n",
        "    def split_text(self, text: str) -> ListType[str]:\n",
        "        # Import inside method for Ray worker safety\n",
        "        import re\n",
        "        from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "        # Split on section headers (## Header) and double newlines\n",
        "        sections = re.split(r'\\n##\\s+|\\n\\n+', text)\n",
        "        sections = [s.strip() for s in sections if s.strip()]\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        previous_sentences = []\n",
        "\n",
        "        for section in sections:\n",
        "            sentences = sent_tokenize(section)\n",
        "\n",
        "            for sent in sentences:\n",
        "                sent_tokens = len(word_tokenize(sent))\n",
        "\n",
        "                if current_length + sent_tokens > self.max_chunk_size and current_chunk:\n",
        "                    chunk_text = ' '.join(current_chunk)\n",
        "                    chunks.append(chunk_text)\n",
        "\n",
        "                    if self.overlap_sentences > 0:\n",
        "                        previous_sentences = current_chunk[-self.overlap_sentences:]\n",
        "                        current_chunk = previous_sentences.copy()\n",
        "                        current_length = sum(len(word_tokenize(s)) for s in current_chunk)\n",
        "                    else:\n",
        "                        current_chunk = []\n",
        "                        current_length = 0\n",
        "\n",
        "                current_chunk.append(sent)\n",
        "                current_length += sent_tokens\n",
        "\n",
        "        if current_chunk and current_length >= self.min_chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "        elif current_chunk and chunks:\n",
        "            chunks[-1] = chunks[-1] + ' ' + ' '.join(current_chunk)\n",
        "        elif current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks if chunks else [text]\n",
        "\n",
        "\n",
        "class SemanticBM25Splitter(TextSplitter):\n",
        "    \"\"\"Split text at semantic boundaries using BM25 similarity.\"\"\"\n",
        "\n",
        "    def __init__(self, similarity_threshold: float = 0.3,\n",
        "                 min_chunk_sentences: int = 3, **kwargs):\n",
        "        super().__init__(chunk_size=500, chunk_overlap=0, **kwargs)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.min_chunk_sentences = min_chunk_sentences\n",
        "        self._name = f\"semantic_bm25_thresh{similarity_threshold}\"\n",
        "\n",
        "    def split_text(self, text: str) -> ListType[str]:\n",
        "        # Import inside method for Ray worker safety\n",
        "        from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        if len(sentences) <= self.min_chunk_sentences:\n",
        "            return [text]\n",
        "\n",
        "        tokenized_sents = [word_tokenize(s.lower()) for s in sentences]\n",
        "        tokenized_sents = [t if t else [''] for t in tokenized_sents]\n",
        "\n",
        "        try:\n",
        "            bm25 = BM25Okapi(tokenized_sents)\n",
        "        except:\n",
        "            return [text]\n",
        "\n",
        "        boundaries = [0]\n",
        "        for i in range(self.min_chunk_sentences, len(sentences) - 1):\n",
        "            prev_context = ' '.join(sentences[max(0, i-3):i])\n",
        "            prev_tokens = word_tokenize(prev_context.lower())\n",
        "\n",
        "            if not prev_tokens: continue\n",
        "\n",
        "            scores = bm25.get_scores(prev_tokens)\n",
        "            max_score = max(scores) if max(scores) > 0 else 1\n",
        "\n",
        "            if scores[i] < self.similarity_threshold * max_score:\n",
        "                if i - boundaries[-1] >= self.min_chunk_sentences:\n",
        "                    boundaries.append(i)\n",
        "\n",
        "        boundaries.append(len(sentences))\n",
        "\n",
        "        chunks = []\n",
        "        for i in range(len(boundaries) - 1):\n",
        "            chunk_sents = sentences[boundaries[i]:boundaries[i+1]]\n",
        "            if chunk_sents:\n",
        "                chunks.append(' '.join(chunk_sents))\n",
        "\n",
        "        return chunks if chunks else [text]\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. OVERLAP WRAPPERS\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class FixedSentenceOverlapWrapper(TextSplitter):\n",
        "    \"\"\"Wraps a base splitter and adds fixed sentence overlap.\"\"\"\n",
        "\n",
        "    def __init__(self, base_splitter: TextSplitter, overlap_sentences: int = 2, **kwargs):\n",
        "        super().__init__(chunk_size=base_splitter._chunk_size,\n",
        "                         chunk_overlap=overlap_sentences * 20, **kwargs)\n",
        "        self.base_splitter = base_splitter\n",
        "        self.overlap_sentences = overlap_sentences\n",
        "        self._name = f\"{getattr(base_splitter, '_name', 'base')}_fixedoverlap{overlap_sentences}\"\n",
        "\n",
        "    def split_text(self, text: str) -> ListType[str]:\n",
        "        # Import inside method for Ray worker safety\n",
        "        from nltk.tokenize import sent_tokenize\n",
        "\n",
        "        base_chunks = self.base_splitter.split_text(text)\n",
        "        if len(base_chunks) <= 1 or self.overlap_sentences == 0:\n",
        "            return base_chunks\n",
        "\n",
        "        extended_chunks = []\n",
        "        for i, chunk in enumerate(base_chunks):\n",
        "            parts = []\n",
        "            if i > 0:\n",
        "                prev_sents = sent_tokenize(base_chunks[i-1])\n",
        "                parts.extend(prev_sents[-self.overlap_sentences:])\n",
        "            parts.append(chunk)\n",
        "            extended_chunks.append(' '.join(parts))\n",
        "        return extended_chunks\n",
        "\n",
        "\n",
        "class SemanticOverlapWrapper(TextSplitter):\n",
        "    \"\"\"Wraps base splitter; adds overlap based on BM25 semantic similarity of boundary sentences.\"\"\"\n",
        "\n",
        "    def __init__(self, base_splitter: TextSplitter, similarity_threshold: float = 0.2, **kwargs):\n",
        "        super().__init__(chunk_size=base_splitter._chunk_size,\n",
        "                         chunk_overlap=50, **kwargs)\n",
        "        self.base_splitter = base_splitter\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        base_name = getattr(base_splitter, '_name', 'base')\n",
        "        self._name = f\"{base_name}_semoverlap\"\n",
        "\n",
        "    def split_text(self, text: str) -> ListType[str]:\n",
        "        # Import inside method for Ray worker safety\n",
        "        from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "        base_chunks = self.base_splitter.split_text(text)\n",
        "        if len(base_chunks) <= 1:\n",
        "            return base_chunks\n",
        "\n",
        "        boundary_sents = []\n",
        "        for chunk in base_chunks:\n",
        "            sents = sent_tokenize(chunk)\n",
        "            boundary_sents.append(sents[0] if sents else \"\")\n",
        "            boundary_sents.append(sents[-1] if sents else \"\")\n",
        "\n",
        "        tokenized = [word_tokenize(s.lower()) for s in boundary_sents]\n",
        "        tokenized = [t if t else [''] for t in tokenized]\n",
        "\n",
        "        try:\n",
        "            bm25 = BM25Okapi(tokenized)\n",
        "        except:\n",
        "            return base_chunks\n",
        "\n",
        "        extended_chunks = []\n",
        "        for i, chunk in enumerate(base_chunks):\n",
        "            parts = []\n",
        "\n",
        "            if i > 0:\n",
        "                prev_last_idx = (i - 1) * 2 + 1\n",
        "                curr_first_idx = i * 2\n",
        "\n",
        "                query = tokenized[curr_first_idx]\n",
        "                if query and query != ['']:\n",
        "                    scores = bm25.get_scores(query)\n",
        "                    max_score = max(scores) if max(scores) > 0 else 1\n",
        "\n",
        "                    if scores[prev_last_idx] > self.similarity_threshold * max_score:\n",
        "                        prev_sents = sent_tokenize(base_chunks[i-1])\n",
        "                        if prev_sents:\n",
        "                            parts.append(prev_sents[-1])\n",
        "\n",
        "            parts.append(chunk)\n",
        "\n",
        "            if i < len(base_chunks) - 1:\n",
        "                curr_last_idx = i * 2 + 1\n",
        "                next_first_idx = (i + 1) * 2\n",
        "\n",
        "                query = tokenized[curr_last_idx]\n",
        "                if query and query != ['']:\n",
        "                    scores = bm25.get_scores(query)\n",
        "                    max_score = max(scores) if max(scores) > 0 else 1\n",
        "\n",
        "                    if scores[next_first_idx] > self.similarity_threshold * max_score:\n",
        "                        next_sents = sent_tokenize(base_chunks[i+1])\n",
        "                        if next_sents:\n",
        "                            parts.append(next_sents[0])\n",
        "\n",
        "            extended_chunks.append(' '.join(parts))\n",
        "\n",
        "        return extended_chunks\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. CREATE ALL 12 CONFIGURATIONS\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURING CHUNKING EXPERIMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Base chunkers (no overlap)\n",
        "fixed_256_base = FixedTokenSplitter(chunk_size=256, chunk_overlap=0)\n",
        "fixed_512_base = FixedTokenSplitter(chunk_size=512, chunk_overlap=0)\n",
        "structural_base = StructuralSplitter(min_chunk_size=50, max_chunk_size=500, overlap_sentences=0)\n",
        "semantic_base = SemanticBM25Splitter(similarity_threshold=0.3, min_chunk_sentences=3)\n",
        "\n",
        "# Fixed overlap versions (15% overlap)\n",
        "fixed_256_fixedoverlap = FixedTokenSplitter(chunk_size=256, chunk_overlap=38)\n",
        "fixed_512_fixedoverlap = FixedTokenSplitter(chunk_size=512, chunk_overlap=77)\n",
        "structural_fixedoverlap = StructuralSplitter(min_chunk_size=50, max_chunk_size=500, overlap_sentences=2)\n",
        "semantic_fixedoverlap = FixedSentenceOverlapWrapper(\n",
        "    SemanticBM25Splitter(similarity_threshold=0.3, min_chunk_sentences=3),\n",
        "    overlap_sentences=2\n",
        ")\n",
        "\n",
        "# Semantic overlap versions (Standardized threshold 0.2)\n",
        "fixed_256_semoverlap = SemanticOverlapWrapper(FixedTokenSplitter(chunk_size=256, chunk_overlap=0), similarity_threshold=0.2)\n",
        "fixed_512_semoverlap = SemanticOverlapWrapper(FixedTokenSplitter(chunk_size=512, chunk_overlap=0), similarity_threshold=0.2)\n",
        "structural_semoverlap = SemanticOverlapWrapper(StructuralSplitter(min_chunk_size=50, max_chunk_size=500), similarity_threshold=0.2)\n",
        "semantic_semoverlap = SemanticOverlapWrapper(SemanticBM25Splitter(similarity_threshold=0.3), similarity_threshold=0.2)\n",
        "\n",
        "ALL_SPLITTERS = [\n",
        "    # No overlap\n",
        "    (\"fixed_256_no_overlap\", fixed_256_base),\n",
        "    (\"fixed_512_no_overlap\", fixed_512_base),\n",
        "    (\"structural_no_overlap\", structural_base),\n",
        "    (\"semantic_no_overlap\", semantic_base),\n",
        "    # Fixed 15% overlap\n",
        "    (\"fixed_256_fixed_overlap\", fixed_256_fixedoverlap),\n",
        "    (\"fixed_512_fixed_overlap\", fixed_512_fixedoverlap),\n",
        "    (\"structural_fixed_overlap\", structural_fixedoverlap),\n",
        "    (\"semantic_fixed_overlap\", semantic_fixedoverlap),\n",
        "    # Semantic BM25 overlap\n",
        "    (\"fixed_256_semantic_overlap\", fixed_256_semoverlap),\n",
        "    (\"fixed_512_semantic_overlap\", fixed_512_semoverlap),\n",
        "    (\"structural_semantic_overlap\", structural_semoverlap),\n",
        "    (\"semantic_semantic_overlap\", semantic_semoverlap),\n",
        "]\n",
        "\n",
        "print(f\"\\n‚úì Created {len(ALL_SPLITTERS)} chunking configurations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8SU0fq-YPHh"
      },
      "source": [
        "### III. Execution Logic (Pre/Post Processing)\n",
        "\n",
        "We define the specific runtime logic required for the RapidFire evaluator:\n",
        "\n",
        "1.  **`preprocess_fn`**: The \"engine\" of the experiment. It accepts a batch of queries, executes the retrieval using the specific chunking configuration being tested, and formats the results.\n",
        "2.  **`postprocess_fn`**: The \"judge\". It looks up the Ground Truth document IDs (from the `qrels` dataframe) and attaches them to the results so we can calculate accuracy.\n",
        "\n",
        "*Note: The actual scoring logic (`compute_metrics_fn`) is inherited from the Global Setup step to ensure consistency across all phases.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwbzTCoFYSK8",
        "outputId": "6bbab780-ccca-4250-a60e-5e116b46f060"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "def preprocess_fn(\n",
        "    batch: Dict[str, List],\n",
        "    rag: RFLangChainRagSpec,\n",
        "    prompt_manager=None\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"\n",
        "    Phase 1 Retrieval Execution:\n",
        "    1. Runs the RAG retrieval using the current chunking config.\n",
        "    2. Extracts retrieved Document IDs.\n",
        "    3. Serializes context for the LLM (minimal prompt).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Perform batched retrieval\n",
        "    # serialize=False lets us access the raw Document objects to get metadata\n",
        "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
        "\n",
        "    # 2. Extract retrieved document IDs (for metric calculation)\n",
        "    retrieved_documents = [\n",
        "        [doc.metadata.get(\"doc_id\", \"\") for doc in docs]\n",
        "        for docs in all_context\n",
        "    ]\n",
        "\n",
        "    # 3. Serialize context for the generator\n",
        "    serialized_context = rag.serialize_documents(all_context)\n",
        "\n",
        "    # 4. Format prompts (Minimal structure, we focus on retrieval metrics here)\n",
        "    prompts = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"Answer briefly.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {ctx[:500]}\\n\\nQuestion: {q}\\n\\nAnswer:\"}\n",
        "        ]\n",
        "        for q, ctx in zip(batch[\"query\"], serialized_context)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"prompts\": prompts,\n",
        "        \"retrieved_documents\": retrieved_documents,\n",
        "        **batch,\n",
        "    }\n",
        "\n",
        "\n",
        "def postprocess_fn(batch: Dict[str, List]) -> Dict[str, List]:\n",
        "    \"\"\"\n",
        "    Phase 1 Ground Truth Attachment:\n",
        "    Looks up the correct 'corpus_id' for each query in the global 'qrels' DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if we have a match in the QRELS (Relevance Judgments) file\n",
        "    batch[\"ground_truth_documents\"] = [\n",
        "        [str(qrels[qrels[\"query_id\"] == qid][\"corpus_id\"].iloc[0])]\n",
        "        if len(qrels[qrels[\"query_id\"] == qid]) > 0 else []\n",
        "        for qid in batch[\"query_id\"]\n",
        "    ]\n",
        "\n",
        "    return batch\n",
        "\n",
        "print(\"‚úì Phase 1 Execution Logic defined (Pre/Post processors).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teHK2T_i6TW7"
      },
      "source": [
        "### IV. Sequential & Mounted Experiment Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pLX6O8g6evL",
        "outputId": "ab992c09-fb13-4c2b-e932-4d5988d67bf9"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "results_dir = '/content/drive/MyDrive/RapidFire_QASPER_Results/phase1'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "print(f\"‚úì Results will be saved to: {results_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RVGDK-A86ahL",
        "outputId": "f4fca252-0eef-430d-9e31-07e8a2a5c260"
      },
      "outputs": [],
      "source": [
        "from rapidfireai.automl import RFvLLMModelConfig\n",
        "\n",
        "# =============================================================================\n",
        "# SERIAL EXECUTION - ONE CONFIG AT A TIME (Colab-safe)\n",
        "# =============================================================================\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Prepare eval dataset (same as before)\n",
        "from datasets import Dataset\n",
        "sample_fraction = 0.1\n",
        "sample_size = int(len(eval_df) * sample_fraction)\n",
        "eval_dataset = Dataset.from_pandas(eval_df.sample(n=sample_size, random_state=42))\n",
        "print(f\"‚úì Eval dataset: {len(eval_dataset)} queries ({sample_fraction*100:.0f}% sample)\")\n",
        "\n",
        "# Store all results\n",
        "all_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING SERIAL EXPERIMENTS (12 configs)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for config_idx, (config_name, splitter) in enumerate(ALL_SPLITTERS):\n",
        "\n",
        "    # import ray\n",
        "    # if ray.is_initialized():\n",
        "    #     ray.shutdown()\n",
        "    #     import time\n",
        "    #     time.sleep(5)\n",
        "\n",
        "    # Check if already completed (resume capability)\n",
        "    save_path = f\"{results_dir}/{config_name}_results.csv\"\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"\\n‚è≠Ô∏è  [{config_idx+1}/12] Skipping {config_name} (already done)\")\n",
        "        all_results.append(pd.read_csv(save_path))\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üöÄ [{config_idx+1}/12] Running: {config_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create RAG config for THIS splitter only (not List)\n",
        "    rag_config = RFLangChainRagSpec(\n",
        "        document_loader=DirectoryLoader(\n",
        "            path=str(data_dir),\n",
        "            glob=\"corpus.jsonl\",\n",
        "            loader_cls=JSONLoader,\n",
        "            loader_kwargs={\n",
        "                \"jq_schema\": \".\",\n",
        "                \"content_key\": \"text\",\n",
        "                \"metadata_func\": lambda record, metadata: {\n",
        "                    \"doc_id\": record.get(\"_id\"),\n",
        "                    \"title\": record.get(\"title\", \"\")\n",
        "                },\n",
        "                \"json_lines\": True,\n",
        "                \"text_content\": False,\n",
        "            },\n",
        "        ),\n",
        "        text_splitter=splitter,  # SINGLE splitter, not List\n",
        "        embedding_cls=HuggingFaceEmbeddings,\n",
        "        embedding_kwargs={\n",
        "            \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "            \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
        "        },\n",
        "        vector_store=None,\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 20},\n",
        "        reranker_cls=None,\n",
        "        reranker_kwargs=None,\n",
        "        enable_gpu_search=True,\n",
        "    )\n",
        "\n",
        "    # vLLM config\n",
        "    vllm_config = RFvLLMModelConfig(\n",
        "        model_config={\n",
        "            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "            \"dtype\": \"half\",\n",
        "            \"gpu_memory_utilization\": 0.2,\n",
        "            \"tensor_parallel_size\": 1,\n",
        "            \"distributed_executor_backend\": \"mp\",\n",
        "            \"enable_chunked_prefill\": False,\n",
        "            \"enable_prefix_caching\": False,\n",
        "            \"max_model_len\": 1024,\n",
        "            \"disable_log_stats\": True,\n",
        "            \"enforce_eager\": True,\n",
        "            \"disable_custom_all_reduce\": True,\n",
        "        },\n",
        "        sampling_params={\n",
        "            \"temperature\": 0.1,\n",
        "            \"top_p\": 0.95,\n",
        "            \"max_tokens\": 16,\n",
        "        },\n",
        "        rag=rag_config,\n",
        "        prompt_manager=None,\n",
        "    )\n",
        "\n",
        "    # Config set\n",
        "    config_set = {\n",
        "        \"vllm_config\": vllm_config,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"preprocess_fn\": preprocess_fn,\n",
        "        \"postprocess_fn\": postprocess_fn,\n",
        "        \"compute_metrics_fn\": compute_metrics_fn,\n",
        "        \"accumulate_metrics_fn\": accumulate_metrics_fn,\n",
        "        \"online_strategy_kwargs\": {\n",
        "            \"strategy_name\": \"normal\",\n",
        "            \"confidence_level\": 0.95,\n",
        "            \"use_fpc\": True,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    config_group = RFGridSearch(config_set)\n",
        "\n",
        "    # Fresh experiment per config\n",
        "    exp = Experiment(\n",
        "        experiment_name=f\"qasper_{config_name}\",\n",
        "        mode=\"evals\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Run evaluation\n",
        "        results = exp.run_evals(\n",
        "            config_group=config_group,\n",
        "            dataset=eval_dataset,\n",
        "            num_actors=1,\n",
        "            num_shards=4,\n",
        "            seed=42,\n",
        "        )\n",
        "\n",
        "        # Extract metrics\n",
        "        for run_id, (config, metrics_dict) in results.items():\n",
        "            row = {\n",
        "                'config_name': config_name,\n",
        "                'run_id': run_id,\n",
        "                'chunk_size': splitter._chunk_size,\n",
        "                'chunk_overlap': splitter._chunk_overlap,\n",
        "            }\n",
        "            for k, v in metrics_dict.items():\n",
        "                if isinstance(v, dict) and 'value' in v:\n",
        "                    row[k] = v['value']\n",
        "                else:\n",
        "                    row[k] = v\n",
        "\n",
        "            df = pd.DataFrame([row])\n",
        "            df.to_csv(save_path, index=False)\n",
        "            all_results.append(df)\n",
        "\n",
        "            print(f\"   ‚úÖ MRR: {row.get('MRR', 'N/A'):.4f}, Recall@10: {row.get('Recall@10', 'N/A'):.4f}\")\n",
        "\n",
        "        # End experiment\n",
        "        exp.end()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        # Save error info\n",
        "        error_df = pd.DataFrame([{'config_name': config_name, 'error': str(e)}])\n",
        "        error_df.to_csv(f\"{results_dir}/{config_name}_ERROR.csv\", index=False)\n",
        "\n",
        "    # Cleanup memory\n",
        "    del exp, rag_config, vllm_config, config_set, config_group\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"   üßπ Memory cleaned\")\n",
        "\n",
        "# Combine all results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMBINING RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_df = pd.concat(all_results, ignore_index=True)\n",
        "results_df.to_csv(f\"{results_dir}/all_results.csv\", index=False)\n",
        "print(f\"‚úì Combined results saved: {results_dir}/all_results.csv\")\n",
        "print(f\"\\n{results_df.to_string(index=False)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYOux0hExO6S"
      },
      "source": [
        "### V. End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow1zTBwCxd8x",
        "outputId": "bcf3d867-b3a2-4410-874a-9ce0560fd117"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    experiment.end()\n",
        "    print(\"‚úì Phase 1 Experiment finalized and closed.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Note: Experiment might have already ended or failed: {e}\")\n",
        "\n",
        "# Trigger garbage collection to prep RAM for Phase 2\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws_-Td05MOXV"
      },
      "source": [
        "## Phase 2: Representation Optimization (Embeddings)\n",
        "\n",
        "**Hypothesis:** Newer embedding models (2023-2024 era) optimized for large-scale retrieval tasks (e.g., MTEB leaderboard toppers) will significantly outperform the standard 2021 baselines (`all-MiniLM`, `specter`) on scientific text.\n",
        "\n",
        "**Experimental Setup:**\n",
        "Using the **Semantic BM25 Chunker** (Phase 1 Winner), we evaluate a spectrum of embedding models:\n",
        "* **Baselines:** `all-MiniLM-L6-v2`, `allenai-specter`, `all-mpnet-base-v2`, `multi-qa-mpnet-base`.\n",
        "* **Modern Dense:** `bge-m3`.\n",
        "* **Modern Hybrid:** `bge-m3` (Dense + Sparse).\n",
        "\n",
        "*Note: For the hybrid experiments, we utilize a weighted fusion of dense and sparse vectors to capture both semantic meaning and exact keyword matches.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zpa4DlMyvvi"
      },
      "source": [
        "### I. Verify Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yy55WjyOdRK",
        "outputId": "b7de1a22-4ccf-41f0-be91-131898c8d6fb"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PHASE 2: EMBEDDING MODEL OPTIMIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Verify data is loaded\n",
        "try:\n",
        "    print(f\"‚úì Corpus loaded: {len(corpus_df)} documents\")\n",
        "    print(f\"‚úì Eval set loaded: {len(eval_df)} queries\")\n",
        "    print(f\"‚úì Qrels loaded: {len(qrels)} relevance judgments\")\n",
        "except NameError as e:\n",
        "    print(\"‚ùå ERROR: Data not loaded!\")\n",
        "    print(\"   Please run Phase 1 Cells 1-5 first.\")\n",
        "    print(\"   (Through 'LOADING DATA FOR EXPERIMENTS')\")\n",
        "    raise e\n",
        "\n",
        "# Verify random seed consistency\n",
        "import random\n",
        "import numpy as np\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "print(f\"‚úì Random seeds set to 42 (ensures same eval sample as Phase 1)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBo2pzJ0zAIP"
      },
      "source": [
        "### II. Define Winning Chunking & Overlap Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZAH6i1WOvtc",
        "outputId": "ca1d1ca6-9c14-4465-e530-3dc91b5d993b"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import TextSplitter\n",
        "from typing import List as ListType\n",
        "\n",
        "# Instantiate the winning chunker\n",
        "PHASE1_WINNER_SPLITTER = SemanticBM25Splitter(\n",
        "    similarity_threshold=0.3,\n",
        "    min_chunk_sentences=3\n",
        ")\n",
        "\n",
        "print(\"‚úì Phase 1 Winner Defined: SemanticBM25Splitter (no overlap)\")\n",
        "print(f\"  - Similarity threshold: 0.3\")\n",
        "print(f\"  - Min chunk sentences: 3\")\n",
        "\n",
        "# Quick validation\n",
        "test_text = corpus_df.iloc[0]['text'][:2000]\n",
        "test_chunks = PHASE1_WINNER_SPLITTER.split_text(test_text)\n",
        "print(f\"  - Validation: {len(test_chunks)} chunks from sample text ‚úì\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZzaA8E0zUO8"
      },
      "source": [
        "### III. Define Embedding Model Configurations\n",
        "| Model                 | Era  | Context | Dim  | Prefixes |\n",
        "|-----------------------|------|---------|------|----------|\n",
        "| all-MiniLM-L6-v2      | 2021 | 512     | 384  | No       |\n",
        "| all-mpnet-base-v2     | 2021 | 512     | 768  | No       |\n",
        "| allenai-specter       | 2020 | 512     | 768  | No       |\n",
        "| multi-qa-mpnet-base   | 2021 | 512     | 768  | No       |\n",
        "| bge-m3                | 2024 | 8192    | 1024 | No       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vVCFOTtJPFDh",
        "outputId": "6d7831aa-dc4d-4d20-e872-05012d16e1f0"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_MODELS = [\n",
        "    {\n",
        "        \"name\": \"all-MiniLM-L6-v2\",\n",
        "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"domain\": \"general\",\n",
        "        \"size\": \"small\",\n",
        "        \"dim\": 384,\n",
        "        \"max_seq\": 512,\n",
        "        \"query_prefix\": \"\",\n",
        "        \"document_prefix\": \"\",\n",
        "        \"trust_remote_code\": False,\n",
        "        \"description\": \"2021 Baseline - General purpose, small & fast\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"all-mpnet-base-v2\",\n",
        "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "        \"domain\": \"general\",\n",
        "        \"size\": \"base\",\n",
        "        \"dim\": 768,\n",
        "        \"max_seq\": 512,\n",
        "        \"query_prefix\": \"\",\n",
        "        \"document_prefix\": \"\",\n",
        "        \"trust_remote_code\": False,\n",
        "        \"description\": \"2021 Baseline - General purpose, larger model\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"allenai-specter\",\n",
        "        \"model_name\": \"sentence-transformers/allenai-specter\",\n",
        "        \"domain\": \"scientific\",\n",
        "        \"size\": \"base\",\n",
        "        \"dim\": 768,\n",
        "        \"max_seq\": 512,\n",
        "        \"query_prefix\": \"\",\n",
        "        \"document_prefix\": \"\",\n",
        "        \"trust_remote_code\": False,\n",
        "        \"description\": \"2020 Baseline - Scientific papers (citation-trained)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"multi-qa-mpnet-base\",\n",
        "        \"model_name\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
        "        \"domain\": \"qa-tuned\",\n",
        "        \"size\": \"base\",\n",
        "        \"dim\": 768,\n",
        "        \"max_seq\": 512,\n",
        "        \"query_prefix\": \"\",\n",
        "        \"document_prefix\": \"\",\n",
        "        \"trust_remote_code\": False,\n",
        "        \"description\": \"2021 Baseline - QA-tuned on 215M pairs\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"bge-m3\",\n",
        "        \"model_name\": \"BAAI/bge-m3\",\n",
        "        \"domain\": \"retrieval-2024\",\n",
        "        \"size\": \"large\",\n",
        "        \"dim\": 1024,\n",
        "        \"max_seq\": 8192,\n",
        "        \"query_prefix\": \"\",  # BGE-M3 doesn't require prefixes for English\n",
        "        \"document_prefix\": \"\",\n",
        "        \"trust_remote_code\": False,\n",
        "        \"description\": \"2024 SOTA - Multi-granularity, 8K context, dense+sparse+colbert\"\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 2: EMBEDDING MODEL CONFIGURATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Separate by status\n",
        "done_models = [m for m in EMBEDDING_MODELS if m['domain'] not in ['retrieval-2024']]\n",
        "new_models = [m for m in EMBEDDING_MODELS if m['domain'] == 'retrieval-2024']\n",
        "\n",
        "print(f\"\\n‚úì {len(EMBEDDING_MODELS)} total models configured:\")\n",
        "print(f\"  ‚Ä¢ {len(done_models)} baseline models (2020-2021) - will skip if results exist\")\n",
        "print(f\"  ‚Ä¢ {len(new_models)} new models (2024) - to be run\")\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*70}\")\n",
        "print(\"2024 MODELS TO TEST:\")\n",
        "print(f\"{'‚îÄ'*70}\")\n",
        "for i, model in enumerate(new_models, 1):\n",
        "    prefix_status = \"‚úì Requires prefixes\" if model['query_prefix'] else \"No prefixes needed\"\n",
        "    print(f\"\\n  {i}. {model['name']}\")\n",
        "    print(f\"     Path: {model['model_name']}\")\n",
        "    print(f\"     Context: {model['max_seq']} | Dim: {model['dim']} | {prefix_status}\")\n",
        "    print(f\"     {model['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbBHGgOi2aVv"
      },
      "source": [
        "### IV. Initialize Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "collapsed": true,
        "id": "A93NZ2t_PIpU",
        "outputId": "f56e5a6c-b0bc-4f83-87f8-2d8bc496fc30"
      },
      "outputs": [],
      "source": [
        "from rapidfireai import Experiment\n",
        "\n",
        "experiment = Experiment(\n",
        "    experiment_name=\"qasper-phase2-embeddings\",\n",
        "    mode=\"evals\"\n",
        ")\n",
        "\n",
        "print(\"‚úì Experiment initialized: qasper-phase2-embeddings\")\n",
        "\n",
        "# Prepare eval dataset (SAME sampling as Phase 1 via seed=42)\n",
        "from datasets import Dataset\n",
        "\n",
        "sample_fraction = 0.1\n",
        "sample_size = int(len(eval_df) * sample_fraction)\n",
        "eval_dataset = Dataset.from_pandas(eval_df.sample(n=sample_size, random_state=42))\n",
        "\n",
        "print(f\"‚úì Eval dataset: {len(eval_dataset)} queries ({sample_fraction*100:.0f}% sample)\")\n",
        "print(f\"  (Same distribution as Phase 1 via random_state=42)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMhkXzQi3dYW"
      },
      "source": [
        "### V. Experiment Execution (Dense Only)\n",
        "\n",
        "This cell executes the embedding benchmark for the configured models.\n",
        "\n",
        "**Configuration:**\n",
        "* **Models:** We iterate through the `EMBEDDING_MODELS` list defined above.\n",
        "* **Architecture:** We use the standard `HuggingFaceEmbeddings` class for stability.\n",
        "* **Resources:** \"Safe Mode\" is enabled with a reduced batch size (8) and shard count (2) to fit within T4 GPU memory limits.\n",
        "* **Resume Capability:** The code checks for existing results in Google Drive and skips models that have already completed, allowing for interruption and resumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iQxKxqy3PPKd",
        "outputId": "0c09302a-af1a-4238-b0c6-7808dc21e1af"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import RFLangChainRagSpec, RFvLLMModelConfig, RFGridSearch\n",
        "\n",
        "# 1. Setup Resources\n",
        "results_dir = '/content/drive/MyDrive/RapidFire_QASPER_Results/phase2'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "data_dir = Path(\"qasper_data\")\n",
        "\n",
        "# Ultra-Safe Params for T4 GPU\n",
        "SAFE_BATCH_SIZE = 8\n",
        "NUM_SHARDS = 2\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"RUNNING PHASE 2 EXECUTION (Safe Mode)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Models to run: {len(EMBEDDING_MODELS)}\")\n",
        "print(f\"Batch Size: {SAFE_BATCH_SIZE} | Shards: {NUM_SHARDS}\")\n",
        "\n",
        "all_results = []\n",
        "gc.collect()\n",
        "\n",
        "# 2. Execution Loop\n",
        "for model_idx, model_config in enumerate(EMBEDDING_MODELS):\n",
        "    model_name = model_config[\"name\"]\n",
        "    model_path = model_config[\"model_name\"]\n",
        "\n",
        "    # Path to save individual run results\n",
        "    save_path = f\"{results_dir}/{model_name}_results.csv\"\n",
        "\n",
        "    # --- Check Resume Capability ---\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"\\n‚è≠Ô∏è  [{model_idx+1}/{len(EMBEDDING_MODELS)}] Skipping {model_name} (already done)\")\n",
        "        try:\n",
        "            all_results.append(pd.read_csv(save_path))\n",
        "        except: pass\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üöÄ [{model_idx+1}/{len(EMBEDDING_MODELS)}] Running: {model_name}\")\n",
        "    print(f\"   {model_config['description']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # --- Configure Embeddings (Standard Mode) ---\n",
        "    # We use standard HuggingFaceEmbeddings which is stable for Ray serialization\n",
        "    model_kwargs = {\"device\": \"cuda:0\"}\n",
        "    if model_config.get(\"trust_remote_code\"):\n",
        "        model_kwargs[\"trust_remote_code\"] = True\n",
        "\n",
        "    rag_config = RFLangChainRagSpec(\n",
        "        document_loader=DirectoryLoader(\n",
        "            path=str(data_dir),\n",
        "            glob=\"corpus.jsonl\",\n",
        "            loader_cls=JSONLoader,\n",
        "            loader_kwargs={\n",
        "                \"jq_schema\": \".\",\n",
        "                \"content_key\": \"text\",\n",
        "                \"metadata_func\": lambda record, metadata: {\n",
        "                    \"doc_id\": record.get(\"_id\"),\n",
        "                    \"title\": record.get(\"title\", \"\")\n",
        "                },\n",
        "                \"json_lines\": True,\n",
        "                \"text_content\": False,\n",
        "            },\n",
        "        ),\n",
        "        # Use the winner from Phase 1\n",
        "        text_splitter=PHASE1_WINNER_SPLITTER,\n",
        "        embedding_cls=HuggingFaceEmbeddings,\n",
        "        embedding_kwargs={\n",
        "            \"model_name\": model_path,\n",
        "            \"model_kwargs\": model_kwargs,\n",
        "            \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": SAFE_BATCH_SIZE},\n",
        "        },\n",
        "        vector_store=None,\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 20},\n",
        "        enable_gpu_search=True,\n",
        "    )\n",
        "\n",
        "    # --- vLLM Config ---\n",
        "    vllm_config = RFvLLMModelConfig(\n",
        "        model_config={\n",
        "            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "            \"dtype\": \"half\",\n",
        "            \"gpu_memory_utilization\": 0.2,\n",
        "            \"max_model_len\": 1024,\n",
        "            \"disable_log_stats\": True,\n",
        "            \"enforce_eager\": True,\n",
        "        },\n",
        "        sampling_params={\"temperature\": 0.1, \"max_tokens\": 16},\n",
        "        rag=rag_config,\n",
        "    )\n",
        "\n",
        "    # --- Run Experiment ---\n",
        "    exp = Experiment(experiment_name=f\"qasper_phase2_{model_name}\", mode=\"evals\")\n",
        "\n",
        "    try:\n",
        "        # Aggressive memory cleanup before run\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        results = exp.run_evals(\n",
        "            config_group=RFGridSearch({\n",
        "                \"vllm_config\": vllm_config,\n",
        "                \"batch_size\": SAFE_BATCH_SIZE,\n",
        "                \"preprocess_fn\": preprocess_fn,\n",
        "                \"postprocess_fn\": postprocess_fn,\n",
        "                \"compute_metrics_fn\": compute_metrics_fn,\n",
        "                \"accumulate_metrics_fn\": accumulate_metrics_fn,\n",
        "            }),\n",
        "            dataset=eval_dataset,\n",
        "            num_actors=1,\n",
        "            num_shards=NUM_SHARDS,\n",
        "            seed=42,\n",
        "        )\n",
        "\n",
        "        # --- Save Results ---\n",
        "        for run_id, (config, metrics_dict) in results.items():\n",
        "            row = {\n",
        "                'phase': 2,\n",
        "                'embedding_model': model_name,\n",
        "                'embedding_domain': model_config['domain'],\n",
        "                'chunking': 'semantic_bm25_no_overlap',\n",
        "                'run_id': run_id,\n",
        "            }\n",
        "            # Extract metrics values safely\n",
        "            for k, v in metrics_dict.items():\n",
        "                if isinstance(v, dict) and 'value' in v:\n",
        "                    row[k] = v['value']\n",
        "                else:\n",
        "                    row[k] = v\n",
        "\n",
        "            df = pd.DataFrame([row])\n",
        "            df.to_csv(save_path, index=False)\n",
        "            all_results.append(df)\n",
        "            print(f\"   ‚úÖ MRR: {row.get('MRR', 0):.4f}\")\n",
        "\n",
        "        exp.end()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        # Log error to file so we know what happened\n",
        "        pd.DataFrame([{'model': model_name, 'error': str(e)}]).to_csv(f\"{results_dir}/{model_name}_ERROR.csv\")\n",
        "\n",
        "    # Cleanup after loop\n",
        "    del exp, rag_config, vllm_config\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 3. Final Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 2 RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if all_results:\n",
        "    phase2_results_df = pd.concat(all_results, ignore_index=True)\n",
        "    phase2_results_df.to_csv(f\"{results_dir}/phase2_all_results.csv\", index=False)\n",
        "\n",
        "    cols = ['embedding_model', 'embedding_domain', 'MRR', 'Recall@10', 'NDCG@10']\n",
        "    available_cols = [c for c in cols if c in phase2_results_df.columns]\n",
        "\n",
        "    print(phase2_results_df[available_cols].sort_values('MRR', ascending=False).to_string(index=False))\n",
        "else:\n",
        "    print(\"‚ùå No results collected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk_nI104FPX"
      },
      "source": [
        "### VI. Hybrid Retrieval (Dense + Sparse)\n",
        "\n",
        "**Objective:**\n",
        "Standard dense embeddings often fail on exact keyword matching (e.g., specific chemical names or citations). This phase tests whether a **Hybrid** approach‚Äîfusing dense semantic vectors with sparse lexical weights‚Äîimproves retrieval on scientific text.\n",
        "\n",
        "**Methodology:**\n",
        "We utilize **BGE-M3**, a model capable of generating both dense embeddings and sparse (ColBERT-style) lexical weights, and **the victor of our dense-only embedding task** from above.\n",
        "* **Mechanism:** We perform a weighted sum of normalized dense and sparse scores: $Score = \\alpha \\cdot S_{dense} + (1-\\alpha) \\cdot S_{sparse}$.\n",
        "* **Variables:** We test $\\alpha$ (Dense Weight) at **1.0** (Baseline), **0.7**, and **0.5**.\n",
        "\n",
        "**Technical Note:**\n",
        "To ensure correct serialization with Ray actors, we define the `_hybrid_preprocess` function to load the `BGEM3FlagModel` on demand within the worker process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MM6JgDnlsN9",
        "outputId": "a9419262-768f-4d18-be01-fc38094d36c6"
      },
      "outputs": [],
      "source": [
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import RFLangChainRagSpec, RFvLLMModelConfig, RFGridSearch\n",
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Setup Phase 2b Directory\n",
        "results_dir_2b = '/content/drive/MyDrive/RapidFire_QASPER_Results/phase2b'\n",
        "os.makedirs(results_dir_2b, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 2b: HYBRID RETRIEVAL (BGE-M3)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 2. Data Preparation (Arrow-Free for Ray Safety)\n",
        "# We re-load the dataset to ensure we have a clean dictionary-based structure\n",
        "eval_set_path = Path(\"qasper_data\") / \"eval_set.json\"\n",
        "with open(eval_set_path, 'r') as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "# Sample 10% for speed\n",
        "eval_df = pd.DataFrame(eval_data)\n",
        "sampled_df = eval_df.sample(n=int(len(eval_df) * 0.1), random_state=42).reset_index(drop=True)\n",
        "eval_dataset = Dataset.from_dict(sampled_df.to_dict(orient='list'))\n",
        "\n",
        "print(f\"‚úì Eval dataset loaded: {len(eval_dataset)} samples\")\n",
        "\n",
        "# 3. Hybrid Logic Functions\n",
        "def _hybrid_preprocess(batch: Dict[str, List], rag: RFLangChainRagSpec, dense_weight: float) -> Dict[str, List]:\n",
        "    \"\"\"\n",
        "    Executes Hybrid Retrieval:\n",
        "    1. Get candidate docs via standard retrieval.\n",
        "    2. Load BGE-M3 model (on worker).\n",
        "    3. Re-score candidates using Dense + Sparse fusion.\n",
        "    \"\"\"\n",
        "    from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "    # Get initial candidates\n",
        "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
        "\n",
        "    # Load Scorer (Lazy load on GPU)\n",
        "    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, device='cuda')\n",
        "\n",
        "    reranked_context = []\n",
        "\n",
        "    for query, docs in zip(batch[\"query\"], all_context):\n",
        "        if not docs:\n",
        "            reranked_context.append([])\n",
        "            continue\n",
        "\n",
        "        doc_texts = [doc.page_content for doc in docs]\n",
        "\n",
        "        # Encode Query & Docs (Dense + Sparse)\n",
        "        q_out = model.encode([query], return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
        "        d_out = model.encode(doc_texts, batch_size=4, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n",
        "\n",
        "        # Calculate Scores\n",
        "        dense_scores = np.dot(d_out['dense_vecs'], q_out['dense_vecs'][0])\n",
        "\n",
        "        # Sparse Dot Product\n",
        "        q_lex = q_out['lexical_weights'][0]\n",
        "        sparse_scores = []\n",
        "        for d_lex in d_out['lexical_weights']:\n",
        "            score = sum(q_lex.get(t, 0) * d_lex.get(t, 0) for t in set(q_lex) & set(d_lex))\n",
        "            sparse_scores.append(score)\n",
        "        sparse_scores = np.array(sparse_scores)\n",
        "\n",
        "        # Normalize & Fuse\n",
        "        if dense_scores.max() > 0: dense_scores /= dense_scores.max()\n",
        "        if sparse_scores.max() > 0: sparse_scores /= sparse_scores.max()\n",
        "\n",
        "        hybrid_scores = (dense_weight * dense_scores) + ((1 - dense_weight) * sparse_scores)\n",
        "\n",
        "        # Sort by Hybrid Score\n",
        "        ranked = sorted(zip(docs, hybrid_scores), key=lambda x: x[1], reverse=True)\n",
        "        reranked_context.append([d for d, s in ranked])\n",
        "\n",
        "    # Serialize\n",
        "    serialized = rag.serialize_documents(reranked_context)\n",
        "    retrieved_ids = [[d.metadata.get(\"doc_id\", \"\") for d in docs] for docs in reranked_context]\n",
        "\n",
        "    prompts = [[{\"role\": \"user\", \"content\": f\"Context: {ctx[:500]}\\nQ: {q}\"}]\n",
        "               for q, ctx in zip(batch[\"query\"], serialized)]\n",
        "\n",
        "    return {\"prompts\": prompts, \"retrieved_documents\": retrieved_ids, **batch}\n",
        "\n",
        "# Wrappers for Grid Search\n",
        "def preprocess_fn_dense(batch, rag, pm=None):\n",
        "    # Standard retrieval (Dense weight = 1.0 implicitly)\n",
        "    return _hybrid_preprocess(batch, rag, dense_weight=1.0)\n",
        "\n",
        "def preprocess_fn_hybrid_07(batch, rag, pm=None): return _hybrid_preprocess(batch, rag, dense_weight=0.7)\n",
        "def preprocess_fn_hybrid_05(batch, rag, pm=None): return _hybrid_preprocess(batch, rag, dense_weight=0.5)\n",
        "\n",
        "def postprocess_fn(batch):\n",
        "    # Ensure ground truth format matches metrics function expectations\n",
        "    batch[\"ground_truth_documents\"] = [[str(gid)] for gid in batch[\"ground_truth_id\"]]\n",
        "    return batch\n",
        "\n",
        "# 4. Configuration & Execution\n",
        "CONFIGS = [\n",
        "    {\"name\": \"dense_only_baseline\", \"fn\": preprocess_fn_dense},\n",
        "    {\"name\": \"hybrid_0.7_dense\", \"fn\": preprocess_fn_hybrid_07},\n",
        "    {\"name\": \"hybrid_0.5_dense\", \"fn\": preprocess_fn_hybrid_05},\n",
        "]\n",
        "\n",
        "# RAG Config (Base)\n",
        "rag_config = RFLangChainRagSpec(\n",
        "    document_loader=DirectoryLoader(\n",
        "        str(Path(\"qasper_data\")), glob=\"corpus.jsonl\", loader_cls=JSONLoader,\n",
        "        loader_kwargs={\"jq_schema\": \".\", \"content_key\": \"text\", \"json_lines\": True,\n",
        "                       \"metadata_func\": lambda r, m: {\"doc_id\": r.get(\"_id\"), \"title\": r.get(\"title\", \"\")}}\n",
        "    ),\n",
        "    text_splitter=PHASE1_WINNER_SPLITTER,\n",
        "    embedding_cls=HuggingFaceEmbeddings,\n",
        "    embedding_kwargs={\"model_name\": \"BAAI/bge-m3\", \"model_kwargs\": {\"device\": \"cuda\"}, \"encode_kwargs\": {\"batch_size\": 4}},\n",
        "    search_kwargs={\"k\": 20},\n",
        "    enable_gpu_search=False # CPU Search for this phase to save GPU for the Model\n",
        ")\n",
        "\n",
        "vllm_config = RFvLLMModelConfig(\n",
        "    model_config={\"model\": \"Qwen/Qwen2.5-0.5B-Instruct\", \"dtype\": \"half\", \"gpu_memory_utilization\": 0.2, \"max_model_len\": 1024, \"disable_log_stats\": True},\n",
        "    sampling_params={\"temperature\": 0.1, \"max_tokens\": 16},\n",
        "    rag=rag_config\n",
        ")\n",
        "\n",
        "# Run Loop\n",
        "for cfg in CONFIGS:\n",
        "    name = cfg[\"name\"]\n",
        "    save_path = f\"{results_dir_2b}/{name}_results.csv\"\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"‚è≠Ô∏è  Skipping {name} (Done)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"üöÄ Running: {name}\")\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    exp = Experiment(experiment_name=f\"qasper_p2b_{name}\", mode=\"evals\")\n",
        "    try:\n",
        "        results = exp.run_evals(\n",
        "            config_group=RFGridSearch({\n",
        "                \"vllm_config\": vllm_config, \"batch_size\": 4,\n",
        "                \"preprocess_fn\": cfg[\"fn\"], \"postprocess_fn\": postprocess_fn,\n",
        "                \"compute_metrics_fn\": compute_metrics_fn, \"accumulate_metrics_fn\": accumulate_metrics_fn\n",
        "            }),\n",
        "            dataset=eval_dataset, num_actors=1, num_shards=1, seed=42\n",
        "        )\n",
        "\n",
        "        # Save Result\n",
        "        for _, metrics in results.values():\n",
        "            mrr = metrics['MRR']['value']\n",
        "            print(f\"   ‚úÖ MRR: {mrr:.4f}\")\n",
        "            pd.DataFrame([{'config': name, 'MRR': mrr}]).to_csv(save_path, index=False)\n",
        "\n",
        "        exp.end()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtk1DTlxU8-_"
      },
      "source": [
        "### VII. End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXjAiIDBU-sc",
        "outputId": "9d6b593a-ce5a-44f7-ca1d-c9891493c016"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    experiment.end()\n",
        "    print(\"‚úì Phase 2 Global Experiment finalized and closed.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Note: Experiment might have already ended: {e}\")\n",
        "\n",
        "# Cleanup\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Tso0OiB_xX"
      },
      "source": [
        "## 3. Results Analysis & Visualization\n",
        "\n",
        "This section analyzes the experimental results from Phases 1 and 2, generating publication-quality visualizations and statistical analyses. All metrics are logged to MLflow for reproducibility and exported as required artifacts.\n",
        "\n",
        "### Phase 1: Chunking Strategy Analysis\n",
        "\n",
        "**Research Question:** Does semantic-aware chunking (BM25-guided boundary detection) outperform fixed-size and structural chunking approaches for scientific document retrieval?\n",
        "\n",
        "**Experimental Design:**\n",
        "- **Independent Variables:**\n",
        "  - Chunking Strategy: Fixed-256, Fixed-512, Structural, Semantic (BM25)\n",
        "  - Overlap Type: None, Fixed (15%), Semantic\n",
        "- **Dependent Variables:** MRR, Recall@10, NDCG@10\n",
        "- **Total Configurations:** 12 (4 strategies √ó 3 overlap types)\n",
        "- **Evaluation Set:** 317 queries (10% stratified sample of QASPER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcQrSRTzCCYj",
        "outputId": "d002295b-f111-46be-a046-1b14f8db9e18"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import mlflow\n",
        "import json\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Initialize MLflow\n",
        "mlflow.set_experiment(\"QASPER_RAG_Optimization\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Phase 1 results\n",
        "results_path = '/content/drive/MyDrive/RapidFire_QASPER_Results/phase1/all_results.csv'\n",
        "phase1_df = pd.read_csv(results_path)\n",
        "\n",
        "# Extract key metrics\n",
        "metrics = ['MRR', 'Recall@10', 'NDCG@10']\n",
        "results = phase1_df[['config_name'] + metrics].copy()\n",
        "\n",
        "# Parse configuration names for analysis\n",
        "def parse_config(name):\n",
        "    \"\"\"Extract chunking strategy and overlap type from config name.\"\"\"\n",
        "    parts = name.split('_')\n",
        "\n",
        "    if parts[0] == 'fixed':\n",
        "        chunking = f\"Fixed-{parts[1]}\"\n",
        "    elif parts[0] == 'structural':\n",
        "        chunking = \"Structural\"\n",
        "    else:\n",
        "        chunking = \"Semantic (BM25)\"\n",
        "\n",
        "    if 'no_overlap' in name:\n",
        "        overlap = \"None\"\n",
        "    elif 'fixed_overlap' in name:\n",
        "        overlap = \"Fixed (15%)\"\n",
        "    elif 'semantic_overlap' in name:\n",
        "        overlap = \"Semantic\"\n",
        "    else:\n",
        "        overlap = \"None\"\n",
        "\n",
        "    return chunking, overlap\n",
        "\n",
        "results[['Chunking Strategy', 'Overlap Type']] = results['config_name'].apply(\n",
        "    lambda x: pd.Series(parse_config(x))\n",
        ")\n",
        "\n",
        "print(\"‚úì Phase 1 Results Loaded\")\n",
        "print(f\"  Configurations tested: {len(results)}\")\n",
        "print(f\"  Metrics evaluated: {metrics}\")\n",
        "\n",
        "# Display summary table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 1 RESULTS SUMMARY (Sorted by MRR)\")\n",
        "print(\"=\"*70)\n",
        "display_cols = ['config_name', 'MRR', 'Recall@10', 'NDCG@10']\n",
        "print(results[display_cols].sort_values('MRR', ascending=False).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4UiPbg8Fh5C"
      },
      "source": [
        "### I. Performance Comparison Across All Configurations\n",
        "\n",
        "The horizontal bar chart below displays all 12 configurations ranked by each metric. Color coding indicates the chunking strategy, revealing clear performance clustering.\n",
        "\n",
        "**Key Observations:**\n",
        "- All **Semantic (BM25)** configurations (green) cluster at the top\n",
        "- **Fixed-256** (blue) shows moderate performance\n",
        "- **Fixed-512** and **Structural** approaches underperform consistently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "FvnoeqHME-RS",
        "outputId": "9954b20c-6446-42e5-ae7f-85c02503bbdd"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
        "\n",
        "# Sort by MRR for consistent ordering\n",
        "sorted_results = results.sort_values('MRR', ascending=True)\n",
        "\n",
        "# Color mapping by chunking strategy\n",
        "colors = {\n",
        "    'Semantic (BM25)': '#2ecc71',  # Green - winner\n",
        "    'Fixed-256': '#3498db',        # Blue\n",
        "    'Fixed-512': '#9b59b6',        # Purple\n",
        "    'Structural': '#e74c3c'        # Red\n",
        "}\n",
        "\n",
        "bar_colors = [colors[cs] for cs in sorted_results['Chunking Strategy']]\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "    bars = ax.barh(range(len(sorted_results)), sorted_results[metric],\n",
        "                   color=bar_colors, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "    ax.set_yticks(range(len(sorted_results)))\n",
        "    ax.set_yticklabels(sorted_results['config_name'].str.replace('_', '\\n'), fontsize=8)\n",
        "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars, sorted_results[metric]):\n",
        "        ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, f'{val:.3f}',\n",
        "                va='center', fontsize=8)\n",
        "\n",
        "axes[0].set_ylabel('Configuration', fontsize=12)\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=c, label=l) for l, c in colors.items()]\n",
        "fig.legend(handles=legend_elements, loc='upper center', ncol=4,\n",
        "           bbox_to_anchor=(0.5, 1.02), fontsize=10)\n",
        "\n",
        "plt.suptitle('Phase 1: Chunking Strategy Performance Comparison',\n",
        "             fontsize=14, fontweight='bold', y=1.06)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save for MLflow\n",
        "fig.savefig('/content/phase1_metrics_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Figure saved: phase1_metrics_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSHJrcDRFlV3"
      },
      "source": [
        "### II. Aggregated Performance by Chunking Strategy\n",
        "\n",
        "To isolate the effect of chunking strategy from overlap type, we average performance across all overlap configurations. This reveals the fundamental impact of how documents are segmented.\n",
        "\n",
        "**Finding:** Semantic (BM25) chunking achieves **14.9% higher MRR** than the average of other strategies, with consistent gains across all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "vrVhMPu_Fmew",
        "outputId": "1a567f99-0e15-456f-d625-5baedea0fc9d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Aggregate by chunking strategy\n",
        "chunking_avg = results.groupby('Chunking Strategy')[metrics].mean()\n",
        "chunking_avg = chunking_avg.sort_values('MRR', ascending=False)\n",
        "\n",
        "x = np.arange(len(chunking_avg))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, chunking_avg['MRR'], width, label='MRR', color='#3498db')\n",
        "bars2 = ax.bar(x, chunking_avg['Recall@10'], width, label='Recall@10', color='#2ecc71')\n",
        "bars3 = ax.bar(x + width, chunking_avg['NDCG@10'], width, label='NDCG@10', color='#e74c3c')\n",
        "\n",
        "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Chunking Strategy', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Average Performance by Chunking Strategy\\n(Averaged Across All Overlap Types)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(chunking_avg.index, fontsize=11)\n",
        "ax.legend(loc='upper right', fontsize=10)\n",
        "ax.set_ylim(0, 0.75)\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('/content/phase1_chunking_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(\"\\nüìä Performance by Chunking Strategy:\")\n",
        "print(chunking_avg.round(4).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiAlNL5eFqAB"
      },
      "source": [
        "### III. Interaction Effects: Chunking √ó Overlap\n",
        "\n",
        "The heatmap below visualizes the interaction between chunking strategy and overlap type. This factorial analysis reveals whether certain combinations produce synergistic effects.\n",
        "\n",
        "**Key Insight:** The Semantic (BM25) row shows uniformly high performance (all cells > 0.21 MRR), indicating robustness to overlap choice. In contrast, other strategies show more variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "QqhLYoMeFnxb",
        "outputId": "95b4a08a-aa06-4947-a1b2-0a5db558f628"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Create pivot table\n",
        "pivot_data = results.pivot_table(\n",
        "    values='MRR',\n",
        "    index='Chunking Strategy',\n",
        "    columns='Overlap Type',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "# Reorder for visualization\n",
        "chunk_order = ['Semantic (BM25)', 'Fixed-256', 'Fixed-512', 'Structural']\n",
        "overlap_order = ['None', 'Fixed (15%)', 'Semantic']\n",
        "pivot_data = pivot_data.reindex(index=chunk_order, columns=overlap_order)\n",
        "\n",
        "sns.heatmap(pivot_data, annot=True, fmt='.4f', cmap='RdYlGn',\n",
        "            linewidths=2, linecolor='white', cbar_kws={'label': 'MRR'},\n",
        "            annot_kws={'fontsize': 12, 'fontweight': 'bold'}, ax=ax)\n",
        "\n",
        "ax.set_title('MRR Heatmap: Chunking Strategy √ó Overlap Type', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Overlap Type', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Chunking Strategy', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('/content/phase1_heatmap.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQIOC90VFuMY"
      },
      "source": [
        "### IV. Statistical Significance Analysis\n",
        "\n",
        "To validate that the observed differences are meaningful (not due to random variation), we compute effect sizes and relative improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UfxqAXDFsJ5",
        "outputId": "639cc98f-ffde-4d2a-b406-f236741ed750"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Effect size: Semantic vs Others\n",
        "semantic_mrr = results[results['Chunking Strategy'] == 'Semantic (BM25)']['MRR'].values\n",
        "other_mrr = results[results['Chunking Strategy'] != 'Semantic (BM25)']['MRR'].values\n",
        "\n",
        "# Cohen's d\n",
        "pooled_std = np.sqrt((semantic_mrr.std()**2 + other_mrr.std()**2) / 2)\n",
        "cohens_d = (semantic_mrr.mean() - other_mrr.mean()) / pooled_std\n",
        "\n",
        "print(f\"\\nüìä Semantic (BM25) vs. All Other Strategies:\")\n",
        "print(f\"   Semantic Mean MRR:     {semantic_mrr.mean():.4f}\")\n",
        "print(f\"   Other Strategies MRR:  {other_mrr.mean():.4f}\")\n",
        "print(f\"   Absolute Difference:   {semantic_mrr.mean() - other_mrr.mean():.4f}\")\n",
        "print(f\"   Relative Improvement:  {(semantic_mrr.mean() - other_mrr.mean()) / other_mrr.mean() * 100:.1f}%\")\n",
        "print(f\"\\nüìä Effect Size (Cohen's d): {cohens_d:.2f}\")\n",
        "\n",
        "if cohens_d > 0.8:\n",
        "    interpretation = \"Large effect (d > 0.8)\"\n",
        "elif cohens_d > 0.5:\n",
        "    interpretation = \"Medium effect (0.5 < d < 0.8)\"\n",
        "else:\n",
        "    interpretation = \"Small effect (d < 0.5)\"\n",
        "print(f\"   Interpretation: {interpretation}\")\n",
        "\n",
        "# Pairwise comparisons\n",
        "print(f\"\\nüìä Pairwise Comparisons (vs. semantic_no_overlap):\")\n",
        "baseline = results[results['config_name'] == 'semantic_no_overlap']['MRR'].values[0]\n",
        "print(f\"   Baseline MRR: {baseline:.4f}\")\n",
        "print()\n",
        "\n",
        "for _, row in results.sort_values('MRR', ascending=False).iterrows():\n",
        "    if row['config_name'] != 'semantic_no_overlap':\n",
        "        diff = baseline - row['MRR']\n",
        "        pct = diff / row['MRR'] * 100 if row['MRR'] > 0 else 0\n",
        "        direction = \"+\" if diff > 0 else \"\"\n",
        "        print(f\"   vs {row['config_name']:35s}: {direction}{pct:.1f}% ({diff:+.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APpZOUNdF0Ga"
      },
      "source": [
        "### V. Winner Selection: Semantic BM25 (No Overlap)\n",
        "\n",
        "Based on the comprehensive analysis above, we select **`semantic_no_overlap`** as the Phase 1 winner for the following reasons:\n",
        "\n",
        "| Criterion | Value | Justification |\n",
        "|-----------|-------|---------------|\n",
        "| **MRR** | 0.2128 | 2nd highest overall; within 5% of best |\n",
        "| **Recall@10** | 0.3060 | **Best** among all configurations |\n",
        "| **NDCG@10** | 0.6253 | 3rd highest; competitive with top |\n",
        "| **Simplicity** | ‚úì | No overlap computation required |\n",
        "| **Efficiency** | ‚úì | Faster processing, smaller index |\n",
        "\n",
        "**Rationale:** While `semantic_fixed_overlap` achieves marginally higher MRR (0.2229 vs 0.2128), the `semantic_no_overlap` configuration:\n",
        "1. Achieves the **best Recall@10**, which is critical for RAG pipelines where missing the relevant document is costly\n",
        "2. Requires **no additional overlap computation**, reducing complexity\n",
        "3. Produces **fewer, more focused chunks**, leading to more efficient indexing and retrieval\n",
        "\n",
        "For Phase 2, we \"lock in\" the Semantic BM25 chunker with no overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsECRGJVFv1d",
        "outputId": "e11644ce-5d49-47dc-ff67-1e8efeaa75d8"
      },
      "outputs": [],
      "source": [
        "# Log Phase 1 results to MLflow\n",
        "with mlflow.start_run(run_name=\"Phase1_Chunking_Analysis\"):\n",
        "\n",
        "    # Log best configuration metrics\n",
        "    best_config = results[results['config_name'] == 'semantic_no_overlap'].iloc[0]\n",
        "    mlflow.log_param(\"winner_config\", \"semantic_no_overlap\")\n",
        "    mlflow.log_param(\"chunking_strategy\", \"Semantic (BM25)\")\n",
        "    mlflow.log_param(\"overlap_type\", \"None\")\n",
        "    mlflow.log_param(\"total_configs_tested\", len(results))\n",
        "\n",
        "    mlflow.log_metric(\"winner_MRR\", best_config['MRR'])\n",
        "    mlflow.log_metric(\"winner_Recall_at10\", best_config['Recall@10'])\n",
        "    mlflow.log_metric(\"winner_NDCG_at10\", best_config['NDCG@10'])\n",
        "    mlflow.log_metric(\"cohens_d_effect_size\", cohens_d)\n",
        "    mlflow.log_metric(\"improvement_over_baseline_pct\",\n",
        "                      (semantic_mrr.mean() - other_mrr.mean()) / other_mrr.mean() * 100)\n",
        "\n",
        "    # Log artifacts (figures)\n",
        "    mlflow.log_artifact('/content/phase1_metrics_comparison.png')\n",
        "    mlflow.log_artifact('/content/phase1_chunking_comparison.png')\n",
        "    mlflow.log_artifact('/content/phase1_heatmap.png')\n",
        "\n",
        "    # Log results CSV\n",
        "    results.to_csv('/content/phase1_results_processed.csv', index=False)\n",
        "    mlflow.log_artifact('/content/phase1_results_processed.csv')\n",
        "\n",
        "    print(\"‚úì Phase 1 results logged to MLflow\")\n",
        "    print(f\"  Run ID: {mlflow.active_run().info.run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvkfq0PGD8v"
      },
      "source": [
        "---\n",
        "\n",
        "## Phase 1 Conclusion\n",
        "\n",
        "**Key Findings:**\n",
        "1. **Semantic (BM25) chunking dominates** across all metrics with a large effect size (Cohen's d = 2.59)\n",
        "2. **Overlap type has minimal impact** when using semantic chunking (all variants score > 0.21 MRR)\n",
        "3. **Fixed-size chunking underperforms**, particularly at larger chunk sizes (512 tokens)\n",
        "4. The **simpler no-overlap configuration** achieves the best Recall@10, making it ideal for RAG\n",
        "\n",
        "**Winner Locked In:** `SemanticBM25Splitter(similarity_threshold=0.3, min_chunk_sentences=3)` with no overlap\n",
        "\n",
        "This configuration will be used as the foundation for Phase 2 embedding optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxokQ80NIUiY"
      },
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Embedding Model Optimization\n",
        "\n",
        "**Research Question:** Do modern retrieval-optimized embedding models (2024) significantly outperform general-purpose 2021 baselines on scientific document retrieval?\n",
        "\n",
        "**Experimental Design:**\n",
        "- **Fixed Variable:** Semantic BM25 chunking (Phase 1 winner)\n",
        "- **Independent Variable:** Embedding model (5 models across 4 domains)\n",
        "- **Dependent Variables:** MRR, Recall@10, NDCG@10\n",
        "\n",
        "| Model | Era | Domain | Dimensions |\n",
        "|-------|-----|--------|------------|\n",
        "| all-MiniLM-L6-v2 | 2021 | General | 384 |\n",
        "| all-mpnet-base-v2 | 2021 | General | 768 |\n",
        "| allenai-specter | 2020 | Scientific | 768 |\n",
        "| multi-qa-mpnet-base | 2021 | QA-Tuned | 768 |\n",
        "| **bge-m3** | **2024** | **Retrieval** | **1024** |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOzHwbFdIU-P",
        "outputId": "7d3a22d8-c7ba-4c1e-f7ad-670e8a3c185a"
      },
      "outputs": [],
      "source": [
        "# Load Phase 2 results\n",
        "phase2_path = '/content/drive/MyDrive/RapidFire_QASPER_Results/phase2/phase2_all_results.csv'\n",
        "phase2_df = pd.read_csv(phase2_path)\n",
        "\n",
        "# Extract key columns\n",
        "phase2 = phase2_df[['embedding_model', 'embedding_domain', 'MRR', 'Recall@10', 'NDCG@10']].copy()\n",
        "\n",
        "# Load Phase 2b (Hybrid) results\n",
        "phase2b_dir = '/content/drive/MyDrive/RapidFire_QASPER_Results/phase2b'\n",
        "phase2b_files = [\n",
        "    f'{phase2b_dir}/dense_only_baseline_results.csv',\n",
        "    f'{phase2b_dir}/hybrid_0.7_dense_results.csv',\n",
        "    f'{phase2b_dir}/hybrid_0.5_dense_results.csv',\n",
        "]\n",
        "phase2b = pd.concat([pd.read_csv(f) for f in phase2b_files], ignore_index=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 2 DATA LOADED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nPhase 2 (Embedding Models): {len(phase2)} configurations\")\n",
        "print(phase2[['embedding_model', 'MRR', 'Recall@10', 'NDCG@10']].sort_values('MRR', ascending=False).to_string(index=False))\n",
        "\n",
        "print(f\"\\nPhase 2b (Hybrid Retrieval): {len(phase2b)} configurations\")\n",
        "print(phase2b[['config', 'MRR', 'Recall@10', 'NDCG@10']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI2s2IDvIcoW"
      },
      "source": [
        "### I. Embedding Model Performance Comparison\n",
        "\n",
        "The chart below compares MRR across all tested embedding models. The dashed line indicates the Phase 1 baseline (MiniLM with semantic chunking).\n",
        "\n",
        "**Key Finding:** BGE-M3 (2024 SOTA) achieves **16.8% higher MRR** than the 2021 baseline, validating the hypothesis that modern retrieval-optimized models significantly outperform general-purpose encoders.\n",
        "\n",
        "**Surprising Result:** The domain-specific `allenai-specter` model, trained on scientific papers, performs *worst* ‚Äî suggesting that retrieval-specific training objectives matter more than domain matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "Sr0KMZW_Ie7D",
        "outputId": "0c59a5a3-9899-4007-e9af-88604221e07a"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Sort by MRR\n",
        "phase2_sorted = phase2.sort_values('MRR', ascending=True)\n",
        "\n",
        "# Color by domain\n",
        "colors_map = {\n",
        "    'retrieval-2024': '#2ecc71',  # Green - modern SOTA\n",
        "    'general': '#3498db',          # Blue - baseline\n",
        "    'qa-tuned': '#9b59b6',         # Purple\n",
        "    'scientific': '#e74c3c',       # Red\n",
        "}\n",
        "bar_colors = [colors_map.get(d, '#95a5a6') for d in phase2_sorted['embedding_domain']]\n",
        "\n",
        "y_pos = np.arange(len(phase2_sorted))\n",
        "bars = ax.barh(y_pos, phase2_sorted['MRR'], color=bar_colors, edgecolor='white', height=0.6)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(phase2_sorted['embedding_model'], fontsize=11)\n",
        "ax.set_xlabel('MRR', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Phase 2: Embedding Model Performance\\n(Using Semantic BM25 Chunking from Phase 1)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "\n",
        "# Value labels\n",
        "for bar, val in zip(bars, phase2_sorted['MRR']):\n",
        "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, f'{val:.4f}',\n",
        "            va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Baseline reference line\n",
        "baseline_mrr = phase2[phase2['embedding_model'] == 'all-MiniLM-L6-v2']['MRR'].values[0]\n",
        "ax.axvline(x=baseline_mrr, color='gray', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2ecc71', label='2024 SOTA'),\n",
        "    Patch(facecolor='#3498db', label='General (2021)'),\n",
        "    Patch(facecolor='#9b59b6', label='QA-Tuned'),\n",
        "    Patch(facecolor='#e74c3c', label='Scientific'),\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('/content/phase2_embedding_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaPsg7AzIi43"
      },
      "source": [
        "### II. Hybrid Retrieval: Dense + Sparse Fusion (Phase 2b)\n",
        "\n",
        "**Hypothesis:** Scientific text contains domain-specific terminology (chemical names, citations, acronyms) that benefits from exact lexical matching. A hybrid approach combining dense semantic vectors with sparse lexical weights should outperform pure dense retrieval.\n",
        "\n",
        "**Method:** Using BGE-M3's multi-vector output, we compute:\n",
        "$$Score = \\alpha \\cdot S_{dense} + (1-\\alpha) \\cdot S_{sparse}$$\n",
        "\n",
        "Where Œ± is the dense weight (1.0 = pure dense, 0.5 = equal weighting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "Yp1F5bF5IgJ4",
        "outputId": "f88e8d12-a1f7-403a-9572-be45f65e2d2f"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "\n",
        "metrics = ['MRR', 'Recall@10', 'NDCG@10']\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
        "\n",
        "# Create readable labels\n",
        "phase2b['label'] = phase2b['config'].map({\n",
        "    'dense_only_baseline': 'Dense Only\\n(Œ±=1.0)',\n",
        "    'hybrid_0.7_dense': 'Hybrid\\n(Œ±=0.7)',\n",
        "    'hybrid_0.5_dense': 'Hybrid\\n(Œ±=0.5)'\n",
        "})\n",
        "\n",
        "# Sort: Dense first, then hybrids\n",
        "order = ['dense_only_baseline', 'hybrid_0.7_dense', 'hybrid_0.5_dense']\n",
        "phase2b_sorted = phase2b.set_index('config').loc[order].reset_index()\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx]\n",
        "    bars = ax.bar(phase2b_sorted['label'], phase2b_sorted[metric],\n",
        "                  color=colors[idx], edgecolor='white', width=0.6)\n",
        "\n",
        "    ax.set_ylabel(metric if idx == 0 else '', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(metric, fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Adjust y-axis\n",
        "    ymin = phase2b_sorted[metric].min() * 0.95\n",
        "    ymax = phase2b_sorted[metric].max() * 1.03\n",
        "    ax.set_ylim(ymin, ymax)\n",
        "\n",
        "plt.suptitle('Phase 2b: Hybrid Retrieval (Dense + Sparse Fusion)\\nScore = Œ±¬∑Dense + (1-Œ±)¬∑Sparse',\n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "fig.savefig('/content/phase2b_hybrid_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# Print improvement\n",
        "dense_mrr = phase2b[phase2b['config'] == 'dense_only_baseline']['MRR'].values[0]\n",
        "hybrid_mrr = phase2b[phase2b['config'] == 'hybrid_0.5_dense']['MRR'].values[0]\n",
        "print(f\"\\nüìä Hybrid Improvement: {(hybrid_mrr - dense_mrr) / dense_mrr * 100:.1f}% MRR gain from adding sparse retrieval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joFrS8AxIocn"
      },
      "source": [
        "### III. Cumulative Improvement: Full Experimental Journey\n",
        "\n",
        "The waterfall chart below shows the progressive MRR improvements from our worst baseline configuration to the final optimized system.\n",
        "\n",
        "| Phase | Configuration | MRR | Cumulative Gain |\n",
        "|-------|--------------|-----|-----------------|\n",
        "| Baseline | Fixed-512, No Overlap | 0.1819 | ‚Äî |\n",
        "| Phase 1 | Semantic BM25 | 0.2128 | +17.0% |\n",
        "| Phase 2 | BGE-M3 Dense | 0.2486 | +36.7% |\n",
        "| Phase 2b | Hybrid 50/50 | **0.2770** | **+52.3%** |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "VfNJ01ZrIku_",
        "outputId": "42dfc696-00a9-4602-ace4-a06e364b97bf"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Define progression\n",
        "stages = [\n",
        "    ('Phase 1 Baseline\\n(Fixed-512, No Overlap)', 0.1819),\n",
        "    ('Phase 1 Winner\\n(Semantic BM25)', 0.2128),\n",
        "    ('Phase 2 Winner\\n(BGE-M3 Dense)', 0.2486),\n",
        "    ('Phase 2b Winner\\n(Hybrid 50/50)', 0.2770),\n",
        "]\n",
        "\n",
        "labels = [s[0] for s in stages]\n",
        "values = [s[1] for s in stages]\n",
        "\n",
        "# Colors\n",
        "colors = ['#95a5a6'] + ['#2ecc71'] * 3\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "bars = ax.bar(x, values, color=colors, edgecolor='white', width=0.6)\n",
        "\n",
        "# Labels\n",
        "for i, (bar, val) in enumerate(zip(bars, values)):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.005, f'{val:.4f}',\n",
        "            ha='center', fontsize=11, fontweight='bold')\n",
        "    if i > 0:\n",
        "        pct_imp = (val - values[0]) / values[0] * 100\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, val - 0.015, f'+{pct_imp:.1f}%',\n",
        "                ha='center', fontsize=9, color='white', fontweight='bold')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontsize=10)\n",
        "ax.set_ylabel('MRR', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Cumulative MRR Improvement Across Experimental Phases', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(0, 0.35)\n",
        "\n",
        "# Total improvement annotation\n",
        "total_imp = (values[-1] - values[0]) / values[0] * 100\n",
        "ax.annotate(f'Total Improvement: +{total_imp:.1f}%',\n",
        "            xy=(3, values[-1]), xytext=(2.2, 0.32),\n",
        "            fontsize=12, fontweight='bold', color='#2ecc71',\n",
        "            arrowprops=dict(arrowstyle='->', color='#2ecc71', lw=2))\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('/content/phase2_cumulative_improvement.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M87zw7axIvpR"
      },
      "source": [
        "### IV. Complete Results Overview\n",
        "\n",
        "The heatmap below consolidates all experimental configurations, sorted by MRR. This provides a single view of the entire optimization landscape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "nbk85IEsIqWC",
        "outputId": "6e056dce-2fd0-4c29-c1c4-9ed6ca4cfb1a"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Build combined dataframe\n",
        "all_results = []\n",
        "\n",
        "# Phase 1 winner\n",
        "all_results.append({\n",
        "    'Configuration': 'P1: Semantic BM25 (No Overlap)',\n",
        "    'MRR': 0.2128, 'Recall@10': 0.3060, 'NDCG@10': 0.6253\n",
        "})\n",
        "\n",
        "# Phase 2 models\n",
        "for _, row in phase2.iterrows():\n",
        "    all_results.append({\n",
        "        'Configuration': f\"P2: {row['embedding_model']}\",\n",
        "        'MRR': row['MRR'], 'Recall@10': row['Recall@10'], 'NDCG@10': row['NDCG@10']\n",
        "    })\n",
        "\n",
        "# Phase 2b hybrid\n",
        "for _, row in phase2b.iterrows():\n",
        "    label = row['config'].replace('_', ' ').title()\n",
        "    all_results.append({\n",
        "        'Configuration': f\"P2b: {label}\",\n",
        "        'MRR': row['MRR'], 'Recall@10': row['Recall@10'], 'NDCG@10': row['NDCG@10']\n",
        "    })\n",
        "\n",
        "combined_df = pd.DataFrame(all_results)\n",
        "heatmap_data = combined_df.set_index('Configuration')[['MRR', 'Recall@10', 'NDCG@10']]\n",
        "heatmap_data = heatmap_data.sort_values('MRR', ascending=False)\n",
        "\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn',\n",
        "            linewidths=1, linecolor='white', cbar_kws={'label': 'Score'},\n",
        "            annot_kws={'fontsize': 9}, ax=ax)\n",
        "\n",
        "ax.set_title('All Experimental Configurations: Performance Heatmap', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Metric', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Configuration', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('/content/phase2_combined_heatmap.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj45QaMfIw84",
        "outputId": "bbd79db9-e49f-4880-d53f-012fd7af8643"
      },
      "outputs": [],
      "source": [
        "# Log Phase 2 results to MLflow\n",
        "with mlflow.start_run(run_name=\"Phase2_Complete_Analysis\"):\n",
        "\n",
        "    # Phase 2 winner\n",
        "    mlflow.log_param(\"phase2_winner\", \"bge-m3\")\n",
        "    mlflow.log_metric(\"phase2_winner_MRR\", 0.2486)\n",
        "\n",
        "    # Phase 2b winner\n",
        "    mlflow.log_param(\"phase2b_winner\", \"hybrid_0.5_dense\")\n",
        "    mlflow.log_metric(\"phase2b_winner_MRR\", 0.2770)\n",
        "    mlflow.log_metric(\"phase2b_winner_Recall_at10\", 0.3754)\n",
        "    mlflow.log_metric(\"phase2b_winner_NDCG_at10\", 0.7913)\n",
        "\n",
        "    # Total improvement\n",
        "    mlflow.log_metric(\"total_improvement_pct\", 52.3)\n",
        "    mlflow.log_metric(\"baseline_MRR\", 0.1819)\n",
        "    mlflow.log_metric(\"final_MRR\", 0.2770)\n",
        "\n",
        "    # Log artifacts\n",
        "    mlflow.log_artifact('/content/phase2_embedding_comparison.png')\n",
        "    mlflow.log_artifact('/content/phase2b_hybrid_comparison.png')\n",
        "    mlflow.log_artifact('/content/phase2_cumulative_improvement.png')\n",
        "    mlflow.log_artifact('/content/phase2_combined_heatmap.png')\n",
        "\n",
        "    print(\"‚úì Phase 2 results logged to MLflow\")\n",
        "    print(f\"  Run ID: {mlflow.active_run().info.run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua3j51x4I7Wl"
      },
      "source": [
        "---\n",
        "\n",
        "## Experimental Conclusions\n",
        "\n",
        "### Summary of Findings\n",
        "\n",
        "| Phase | Key Finding | Impact |\n",
        "|-------|-------------|--------|\n",
        "| **Phase 1** | Semantic BM25 chunking outperforms fixed-size approaches | +17% MRR |\n",
        "| **Phase 2** | Modern 2024 embeddings (BGE-M3) beat 2021 baselines | +17% MRR |\n",
        "| **Phase 2b** | Hybrid dense+sparse retrieval captures lexical matches | +14% MRR |\n",
        "| **Total** | Cumulative optimization from baseline to best | **+52% MRR** |\n",
        "\n",
        "### Final Optimized Configuration\n",
        "\n",
        "**Chunking:**   SemanticBM25Splitter(threshold=0.3, min_sentences=3)\n",
        "\n",
        "**Overlap:**    None\n",
        "\n",
        "**Embedding:**  BAAI/bge-m3 (1024-dim, 8K context)\n",
        "\n",
        "**Retrieval:**  Hybrid (Œ±=0.5 dense + 0.5 sparse)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
