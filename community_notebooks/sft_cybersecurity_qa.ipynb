{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Torno9_U95qC",
    "outputId": "e551573d-633a-44f5-dab8-dbf1756bf758"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import rapidfireai\n",
    "    print(\"\u2705 rapidfireai already installed\")\n",
    "except ImportError:\n",
    "    %pip install rapidfireai  # Takes 1 min\n",
    "    %pip install bert_score\n",
    "    !rapidfireai init # Takes 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7HrnrM895qC",
    "outputId": "2385957a-b49e-4534-b8d3-21780827c2c7"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from time import sleep\n",
    "import socket\n",
    "try:\n",
    "  s = [socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM)]\n",
    "  s[0].connect((\"127.0.0.1\", 8851))\n",
    "  s[1].connect((\"127.0.0.1\", 8852))\n",
    "  s[2].connect((\"127.0.0.1\", 8853))\n",
    "  s[0].close()\n",
    "  s[1].close()\n",
    "  s[2].close()\n",
    "  print(\"RapidFire Services are running\")\n",
    "except OSError as error:\n",
    "  print(\"RapidFire Services are not running, launching now...\")\n",
    "  subprocess.Popen([\"rapidfireai\", \"start\"])\n",
    "  sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbo1EcUmAxJj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# TensorBoard log directory will be auto-created in experiment path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAde0aIfAxJk"
   },
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig\n",
    "\n",
    "# NB: If you get \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\" from Colab, just rerun this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496,
     "referenced_widgets": [
      "235672aae3de4548a7dd02249105f2e7",
      "e3e4f77190b74c338e0740075308db2e",
      "35fdfb5feba748b1b20cb205ab6f8093",
      "680967a7183540d4a71fbd684eb9aed8",
      "80b76d1bd6d247bf9851c5964504cc73",
      "1d3ef2d1e95949d297c124fcecdb06f5",
      "536ed6525e084bcd9bcb3d5e5902a8dd",
      "a7640039910f425e9e9e6e3ab9860b5b",
      "2d36a41dfafa45db84a768297e836000",
      "515c299b990e415e8c6bddbb1dc58f13",
      "7c252ae57117428fbf175af54c4c6f7d",
      "9537e48ec48541d493c75930251eff1b",
      "a5fd74add668419b8fbc2a69bdda74fb",
      "4031fd180eb649949657eb5b5ee07454",
      "4e1ab0f4611441828506bb1c97cb4c3a",
      "46f7289a630e43b79af4e67f47cd3654",
      "66dd493f64d7491781940ec712a4b508",
      "a9438d8c8b64498fa014c2d7bb9db5e9",
      "750fdfd27fbd4eda993702327425f705",
      "91756f3d04a74967b62eb739600e3e21",
      "7f5fd41c8c0943e1b5b9862b010f5324",
      "25e75808db294a1a98c369960107a386",
      "ea92b57f44604d3188a66a44c2f7b76c",
      "a824d07a3a7b43dab3e7a3faaaef2ab6",
      "fe9e2d2889854ba78527d644adade8a6",
      "b74a030ad9a24363ac8228680d927fa7",
      "9add880dd2ce4556abe473a6c01eb3d2",
      "8a09c1df93704c078ada290bbc01c6dd",
      "e85f2daffed34476a7d8d08ccd7cd07b",
      "a51754b96dd940c48fc34f1812ca0f60",
      "68835157a4f74e0b96ca456f83fb0df7",
      "371e637594614081a88d4f971e87beb6",
      "3a90f5344176463d8a6f31456de46d93",
      "de791fc679a04c97a2089d1d8b4b4c3e",
      "d7b73c9293ca450b9fc5d3151bd66ff6",
      "1452317090894b91bf8e1f75f1a905db",
      "21f8653b116148bd872685a82f1cae0d",
      "5f481ba4b4774b26aea3d80bf4ee28ba",
      "df536b325abe4cebba860adbfdd3d92b",
      "07fc8427a25b4754ab53b14d35c13551",
      "028e2622a77f4fb29befe06d071a9a84",
      "8ab9bc24d37942c189f242024ae208f4",
      "6a949f1a34ae481db26427f8ee243cbf",
      "8bb948f39ca743a0be7be2cf9703442f",
      "44a8c109642541fba25b36d2eaf212b7",
      "352f56e4c04a4977bec46bd00e1c0429",
      "3816323a507a4ede9075a4022625606e",
      "b738a73f179342e5b5b58066bad120fd",
      "42c97dee12ce4753bb0aefa2c935280c",
      "4fac0a1008d34e92a0ca38a648cb3dca",
      "69c0d16a759541cdb9946d63eba713f2",
      "3530626dff5f47f68726d07a81c44bb2",
      "309a120027e24d729473ced77efe96f8",
      "50273866be7046e9acd16933e7d35cf0",
      "3a086b78e5904fb89fc552c678a7bd04",
      "24100b3081314c17b602976746657d45",
      "ad5188dd1cca4061960b3d7c2068955d",
      "8b503d7f08884e98b53f8be0abedd8d2",
      "c1978d72834b428e906e7062bcf9d450",
      "f69ec8194c074dd7b762df1672132975",
      "63c98e2fa1ba43ac9fdc3ba281c8a85e",
      "fd585f7fc1b6491bb0bc1a294a5ad553",
      "d358c8205595491ba229e5823c0eb6e6",
      "7f972d05899b479cafb8a330d624ae70",
      "e81a1d461f344ab78b5cf6d953bd1553",
      "5b0ee129834644479cbee186b2097b4a",
      "5459b50074e94085b39192eb18a1e9ad",
      "90b158a8c3b84e449ccec8a95e356d17",
      "3e13d9b5d7cc48058e9c94d50b981e83",
      "e55cbb9ace554e518c7a19f3a60f962b",
      "b7147f298ba943e984709af28d896719",
      "58112c2e723843ed9157c2457978823b",
      "5d203b4ff6e64d22b3386d71549e2855",
      "987b5f82c6a147cba17ae9e755b62b29",
      "3cf67e071da340b796e78a20a9969a26",
      "5d42ae77cbbc41e6a8f5c94db1f84bf8",
      "cab0a2ad68404265b3031a5a30cdfc9b",
      "eff924f333bc4edd83fa954fee9b06f8",
      "8f35e94c430446b18bdd9dbef3a3f0a7",
      "881b8e9e197642539375cfa8a65dbd96",
      "311d59334be345058cc2dc3b7b146172",
      "12752e28bc8f41a5b2463f5709d04a22",
      "2ef10902ae32415a8b08f2aa3d1fabda",
      "469a5752cc1b4223b8f61f7240bd2f45",
      "e5e3d67b7aa6429e83d6e7bd2f6abddd",
      "a27c4acef26444f0968515fde0b1e094",
      "88caf282154c434bbf71f186a9878552",
      "68eec0ab68d94441984b68026e215f0c",
      "699e790ab3c246baba121ff8afed0984",
      "f1034b0bf80842669acd8b89c45ca99a",
      "d5b59e92e67b4fdcb02e30c2b27b66c4",
      "83c909d704ca45d0b4904753be0f53c8",
      "b792236394f1476090faf72a4bff2b0a",
      "b22506aaa54745eea31d85f0ff42cbed",
      "ffb3d779ae2f4b1184565e9b9e3b0d4e",
      "d865c13a93e14c7a848fcda04a8e3363",
      "d4dfa78127a84b6e8f9c79de3c1a2854",
      "2a8e7ac4ac8443e6bc8756f2b8c8d6d6",
      "84a2d52412e34baea1d24802d824fc2b",
      "ab55c745462442c4bc893473f534f6cb",
      "78e17649dfb0486fbad6f31b64c9dbba",
      "b3dc0ae68318452fa0dc4e8a8a6616ef",
      "9dad6206f8dd4a949810e4840e075e51",
      "0b23f8157a844ce79a96d998c16bcd0d",
      "28faadd9bfe34efa88531f612b6ff573",
      "1cea5d41813d425b976b33ff922ce0ad",
      "5ba977ce39b840f6908ae21f64791307",
      "7c505ac8d4ea4aa19a55a56b88857d0a",
      "8906744b980b40748592a65c2a8d82ea",
      "4bfd889db8ab4fedad6cd644a4e3c602",
      "8bd09bed499b4820906fa5ac01dbc258",
      "b724ec16953f4af2809bbd34a1d94a12",
      "41acc6f8321e4f858728286fd1ea3efa",
      "b2e25ac7eaaf4c19a0ead75fdc157856",
      "6f66676c6372465ebe1fc086e9aba8d6",
      "804f865f02ab44d1acffe79e6a005d7a",
      "85f848d113fb4e779886468f5d6b0d98",
      "badab742d9a94ebba1645af5d5576769",
      "a29e8e66ab864038803d0616e10ce88d",
      "55740e8cb95945bab057d38c82a84e97",
      "d87537da843e465ba7acb22b62c0eac5"
     ]
    },
    "collapsed": true,
    "id": "Vil1zbTeAxJk",
    "outputId": "af31bacb-9cd1-46ef-b0a2-9e061514d07d"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "dataset = load_dataset(\"mariiazhiv/cybersecurity_qa\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYgfSlW1R_wQ"
   },
   "outputs": [],
   "source": [
    "class QwenChatFormatter:\n",
    "    \"\"\"\n",
    "    A robust formatter that carries its own tokenizer.\n",
    "    This works safely across worker processes (multiprocessing).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self._tokenizer = None # Load lazily\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        if self._tokenizer is None:\n",
    "            from transformers import AutoTokenizer\n",
    "            # Load locally inside the worker process\n",
    "            self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_id,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            # Ensure pad token exists\n",
    "            if self._tokenizer.pad_token is None:\n",
    "                self._tokenizer.pad_token = self._tokenizer.eos_token\n",
    "        return self._tokenizer\n",
    "\n",
    "    def __call__(self, example):\n",
    "        user_input = example.get('input', \"\") or \"\"\n",
    "        instruction = example.get('instruction', \"\")\n",
    "\n",
    "        full_content = f\"{instruction}\\n{user_input}\".strip()\n",
    "\n",
    "        # Create the message structure\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": full_content},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"user\": full_content,\n",
    "            \"assistant\": example['output']\n",
    "        }\n",
    "\n",
    "# Instantiate the formatter once\n",
    "qwen_formatter = QwenChatFormatter(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gqa6JduAxJk"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Lightweight metrics computation\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    try:\n",
    "        import evaluate\n",
    "\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_output = rouge.compute(\n",
    "            predictions=predictions,\n",
    "            references=labels,\n",
    "            use_stemmer=True,\n",
    "            rouge_types=[\"rougeL\"]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"rougeL\": round(rouge_output[\"rougeL\"], 4),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Metrics computation failed: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zW2g7CJAxJk"
   },
   "source": [
    "## Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQ6mbRK6AxJl",
    "outputId": "374e3c50-3718-484d-ed82-0403c3d24970"
   },
   "outputs": [],
   "source": [
    "# Create experiment with unique name\n",
    "my_experiment = \"harshit_sft_demo\"\n",
    "experiment = Experiment(experiment_name=my_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAYPi61cAxJl"
   },
   "source": [
    "## Get TensorBoard Log Directory\n",
    "\n",
    "The TensorBoard logs are stored in the experiment directory. Let's get the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QPR4y-YAxJl",
    "outputId": "608e1b36-a602-4d51-e7a0-1491e9431244"
   },
   "outputs": [],
   "source": [
    "# Get experiment path\n",
    "from rapidfireai.fit.db.rf_db import RfDb\n",
    "\n",
    "db = RfDb()\n",
    "experiment_path = db.get_experiments_path(my_experiment)\n",
    "tensorboard_log_dir = f\"{experiment_path}/tensorboard_logs/{my_experiment}\"\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pALJJyZcAxJl"
   },
   "source": [
    "## Define Model Configurations\n",
    "\n",
    "This tutorial showcases GPT-2 (124M parameters), which is perfect for Colab's memory constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shEQVD7kAxJl"
   },
   "outputs": [],
   "source": [
    "# Runs:\n",
    "#   1: Fewer adaptors, aggressive learning\n",
    "#   2: More adaptors, aggressive learning\n",
    "#   3: Fewer adaptors, stable learning\n",
    "#   4: More adaptors, stable learning\n",
    "\n",
    "peft_configs_qwen = List([\n",
    "    # Config A: Lightweight - Targets only Query/Value projections\n",
    "    RFLoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    ),\n",
    "    # Config B: Heavy - Targets all linear layers (more parameters to learn)\n",
    "    RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "])\n",
    "\n",
    "config_set_qwen = List([\n",
    "    # Strategy 1: Aggressive (Higher LR, Linear Decay)\n",
    "    RFModelConfig(\n",
    "        model_name=model_id,\n",
    "        peft_config=peft_configs_qwen,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,     # Standard Qwen LoRA rate\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            max_steps=100,           # Short run for the competition demo\n",
    "            logging_steps=1,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=10,\n",
    "            fp16=True,              # Required for T4\n",
    "            gradient_checkpointing=True, # Saves VRAM\n",
    "            report_to=\"none\",\n",
    "            num_train_epochs=10,\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_cache\": False      # Must be False for Gradient Checkpointing\n",
    "        },\n",
    "        formatting_func=qwen_formatter,\n",
    "        compute_metrics=compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.6,     # Lower temp for factual QA\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    # Strategy 2: Stable (Lower LR, Cosine Schedule, Warmup)\n",
    "    RFModelConfig(\n",
    "        model_name=model_id,\n",
    "        peft_config=peft_configs_qwen,\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=5e-5,     # Conservative rate\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_steps=10,        # Gentle start\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            max_steps=100,\n",
    "            logging_steps=1,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=10,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "            num_train_epochs=10,\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs={\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"use_cache\": False\n",
    "        },\n",
    "        formatting_func=qwen_formatter,\n",
    "        compute_metrics=compute_metrics,\n",
    "        generation_config={\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Co8ciI-AvGe7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuo9B8WrAxJl"
   },
   "outputs": [],
   "source": [
    "def create_model(model_config):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    model_kwargs = model_config[\"model_kwargs\"]\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "    trust_remote = model_kwargs.get(\"trust_remote_code\", False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    return (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7NOeq0PAxJl"
   },
   "outputs": [],
   "source": [
    "config_group = RFGridSearch(\n",
    "    configs=config_set_qwen,\n",
    "    trainer_type=\"SFT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJfXimPpAxJl"
   },
   "source": [
    "## Start TensorBoard\n",
    "\n",
    "**IMPORTANT: Make sure to start TensorBoard BEFORE invoking run_fit() below so that you can watch metrics appear in real-time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "qVVWU42vKBTN",
    "outputId": "ca8604b9-fe2b-4b73-b22b-6baa34755066"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGOs_rZYAxJm"
   },
   "source": [
    "## Run Training + Validation\n",
    "\n",
    "Now we get to the main function for running multi-config training and evals. The metrics will appear in TensorBoard above in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "AykcHp33AxJm",
    "outputId": "7a47ac3b-1daf-45f2-9f3a-34778c8d9930"
   },
   "outputs": [],
   "source": [
    "experiment.run_fit(\n",
    "    config_group,\n",
    "    create_model,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    num_chunks=2,\n",
    "    seed=67\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-oYfIc195qE"
   },
   "source": [
    "## Launch Interactive Run Controller\n",
    "\n",
    "RapidFire AI provides an Interactive Controller that lets you manage executing runs dynamically in real-time from the notebook:\n",
    "\n",
    "- \u23f9\ufe0f **Stop**: Gracefully stop a running config\n",
    "- \u25b6\ufe0f **Resume**: Resume a stopped run\n",
    "- \ud83d\uddd1\ufe0f **Delete**: Remove a run from this experiment\n",
    "- \ud83d\udccb **Clone**: Create a new run by editing the config dictionary of a parent run to try new knob values; optional warm start of parameters\n",
    "- \ud83d\udd04 **Refresh**: Update run status and metrics\n",
    "\n",
    "The Controller uses ipywidgets and is compatible with both Colab (ipywidgets 7.x) and Jupyter (ipywidgets 8.x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616,
     "referenced_widgets": [
      "2063a7e7f0bc4529a57b068842db2622",
      "937d65e1cad446938e974a7cd785271a",
      "7fe5388d166d429f941ed0d6533446b8",
      "3f1331e8ffcd40d6a1a3e6efe04e40c3",
      "20780ad6381e4bb6bc375aea66ee8047",
      "7b462a55cf774e748602702bed53dc32",
      "c459acbbf4f64ce78ceba780ea8aea77",
      "1203af45bd374dfa876fa69e4e9fc44d",
      "7812939895ec414a868d7ea9730e7c6d",
      "a0cb15a640354342aa43c8ed7819acb2",
      "d1b8454200b54771bdc7bd66c5d662be",
      "ad08cc1ceff44a0986e80cb0feaf39be",
      "59c9292b2ac84ad9abf7a276fb007701",
      "c25df4d030d64b0cafaff3edfab8c8c1",
      "8a622bef531847bda3c947ed4c8fa651",
      "23e1e8cc27e64ff3a57b33c7c4570b94",
      "607521c35a5a4ad5a056a3edbc674813",
      "5193c016dacd49c1b004fd330e04334d",
      "3fcb0ba0b9e94ab5a5848dee2025a3b0",
      "da40ffc231ac4db3830c024a4e8c7af4",
      "008f555d0a48419f87a268daf1da7917",
      "90f2c73f9a9c474b8863c8af9cf95b16",
      "65eeca15e4ab472cb119fe7d917b834b",
      "5ddfb9782c98417ca291dda30baf53a9",
      "95bdef6d3c2e4ffe93fca616cf8e685e",
      "83b16983ed934540b61504ae90390a5b",
      "ed2fa9dd398547678fda5a56ede1e0c4",
      "e46a5668c1f8453394b520ca07ff90dc",
      "3f66d46c9d66477e81332404d6369467",
      "74b7a403d74e432b9c4534b4a7611ee5",
      "44b822d35d2e4892871e939e417e108f",
      "b9f9150aef9c47c58b2cda2b734642fc",
      "6c72f683d26a40c78b75dc5bd8f8d8ed",
      "6894c291da564c65a48709b54fb64dcf",
      "009f3b8f1bc74c8d8d45f8d45eb9474d",
      "6424553bfad4474d9247bd3a3a308dfe",
      "bfba0aa7973748bf9d25ae3666e91791",
      "abdcdfe2578647f681958e85710083b5",
      "945d94ef58fc481a9358cb7b8d36ec92",
      "6d01d17458e945a2b12acf595b0b83e4",
      "cc7efb0b8d39419b8df003509e4f87db",
      "924ea4f0b3d94f20b26fe0ca559109cd",
      "375fb8b1e6d340cdbe117d0577e73e20",
      "624e9747393f497796b1ef1873a4440e",
      "8eb85d1926b343348556b3fc0e945494",
      "c26e1986d05e481fb062397beb05931f",
      "f8a94db9e89045d28e0400df17cf0baf",
      "4cb15d4a7acb4e5ebb2c55c41f13e8cd",
      "cba8f318399b45418fed3e0b9c2994d5",
      "ceca06127ced43edb3dbddc717cd3d23",
      "1ab99d96e2a24b6685a19f5650609145",
      "372a18c507cb43df89cdb33e9756604a",
      "e6f3b89b5f49470da6b88e6b4e15ed64",
      "569bee43fc96422cbe42465e5a196ac1",
      "09e74257e1f74e97b47af5fbef2ba9b1",
      "17004d915cb94bc98159560dd6385616",
      "2339b201e8af4446b85335a6097bca14",
      "60b09da0bf54427494eef0a77d049680",
      "eb0c6a1d1e47422ebeec2963457ef930",
      "37a364b58c234b018c976568feeb4b7d",
      "168a90ffa3fd4290affa227847ea17a8",
      "edb116e3d10047b9a79d932def6a216a",
      "41b9e76ad9194050ab36e49f6081571b",
      "aa291dd8308c42f1b5c177e1e497d087",
      "0dc5384622894b49b5600fb9ef03bd16",
      "b0221eac338049568a62227367393b23",
      "11ebc912f14c4082a10b17be0578c14e",
      "13c8eccd3ce84677810e5461effd03ba",
      "8a5683d3425042488b87a770d2e0b243",
      "acb0689543744fee8784e081f1d8387a",
      "c01f462472b042528d9ab5c453a2338f",
      "d077f6e44eb347dab1a9b456ca39f455",
      "ca800c74da414b04999d5c74a3dd92c2"
     ]
    },
    "id": "piK8okxv95qE",
    "outputId": "3e6573ed-3d7e-4c6e-ea1f-843ae5cf0f62"
   },
   "outputs": [],
   "source": [
    "# Create Interactive Controller\n",
    "sleep(15)\n",
    "from rapidfireai.fit.utils.interactive_controller import InteractiveController\n",
    "\n",
    "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8851\")\n",
    "controller.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujpATTaRAxJm"
   },
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "uOTTI-rVAxJm",
    "outputId": "aac8bb25-274c-4f80-fa53-cca1f4154a5b"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n",
    "'''))\n",
    "\n",
    "# eval_js blocks until the Promise resolves\n",
    "output.eval_js('''\n",
    "new Promise((resolve) => {\n",
    "    document.getElementById(\"continue-btn\").onclick = () => {\n",
    "        document.getElementById(\"continue-btn\").disabled = true;\n",
    "        document.getElementById(\"continue-btn\").innerText = \"Continuing...\";\n",
    "        resolve(\"clicked\");\n",
    "    };\n",
    "})\n",
    "''')\n",
    "\n",
    "# Actually end the experiment after the button is clicked\n",
    "experiment.end()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuiwNvldAxJm"
   },
   "source": [
    "## View TensorBoard Plots and Logs\n",
    "\n",
    "After your experiment is ended, you can still view the full logs in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "id": "rvbsKwE2AxJm",
    "outputId": "48cb68d5-ff3a-42fc-ba35-26fabf99ec81"
   },
   "outputs": [],
   "source": [
    "# View final logs\n",
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXeBD1QjWeLv"
   },
   "source": [
    "## View RapidFire AI Log Files\n",
    "\n",
    "You can track the work being done by the system via the RapidFire AI-produced log files in logs/experiments/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "gx4F_u9O95qE",
    "outputId": "0dc12a70-f13b-47ac-8475-4c15c7f015a2"
   },
   "outputs": [],
   "source": [
    "# Get the experiment-specific log file\n",
    "from IPython.display import display, Pretty\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "display(Pretty(f\"\ud83d\udcc4 Experiment Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"\u274c Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1sJ43RPu95qE",
    "outputId": "f18c78aa-8151-44fe-8b08-3bb01cbaa2fa"
   },
   "outputs": [],
   "source": [
    "# Get the training-specific log file\n",
    "log_file = experiment.get_log_file_path(\"training\")\n",
    "\n",
    "display(Pretty(f\"\ud83d\udcc4 Training Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"\u274c Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0480f12859d648679046b293805b74c0",
      "f618b86dd5ee45929a9e383d51608df8",
      "d921650c2f5849febdeb3c3efe26414a",
      "3dda0e40fc2a469ead5953811c9ff10a",
      "96d10911cf224697a21c271992ab0235",
      "5033f4ea580f486f84f2efe6e13fd0d3",
      "8c9dc09b35ae41808d5f25b576b18139",
      "c6b575973a914fafba6d088c50e90a4b",
      "76308c9bb51b400f81337bdd938d2d9d",
      "19a4fab0afa24533a48e23885ebeaab9",
      "139fee0c00a94f438d307c14ec07eb92",
      "d923a8d6e91e47a9ab6d6d5ec601f05e",
      "986449e027164d5c90299c4e63bb3fa0",
      "8baeff17cef3499aa1bfc9798ee01307",
      "85580369378c49bb94b749e5ba9d31ef",
      "12039d0c10c94a61bfd650b09c0c3430",
      "11180148699f4e8ba7ea7b2f6dcbae21",
      "644c3ad145fd4add94917cde179a545f",
      "d6dc5a7bafd64c2486cdfdbbef714e56",
      "c51a30929020403ca40552781bc17514",
      "24af2f30982c4c7c92dae07d84c64288",
      "e302a7351a7b4b32b5815bd8ad2bd6fb",
      "fac52d7ad3da4aab9675203c8facdae6",
      "f27fd76e3ba843d1a74c15f88cdc62b3",
      "8c4e6a02becc4a5e9e044f628560e6a1",
      "bcb255872b554fae8be6274c1d5f617f",
      "c1fb9527def245ce9522c1871c7d9d75",
      "29bed552c28142d19fd3f4f434fe38aa",
      "b8170f43c9f74dfa95071b5072140321",
      "fac4160a930d48c5b32563edd14e1ccb",
      "f0a11d10eb7f482198a92f55b42d788d",
      "8034468a4b814d9086b1112c06acc834",
      "9d72dd02cf224d4ab8f17a560a60e0be",
      "626df174446240cba6ddd97c30fb36c6",
      "a509ac9d31164a2791913bb050bce1f0",
      "fce7c191a7584273a61790b018e0d225",
      "6753a0f2e8a94f3ab10d13bd08a632b8",
      "4f65a8eab28a467da85d187c10f47d55",
      "fdf655dd6b824a11ba5f5b1efe55c9c6",
      "4369bc7b28c44fcc995ce8d30e15c43d",
      "352d233f631847f19b111add9d6d349c",
      "ad52d7a9ddf44a438c27f9022d64fd93",
      "9a73769c34ac4e22b9cd2a41e81bf797",
      "06c7b1ca4eca44fcbc26f3e379ef6734",
      "d4209a5995294d9e98e7a3dc50103617",
      "bcac96d696234b259bec4d98c2550204",
      "0fea62b722b54797bca66321a58a9abf",
      "dc449f3a5af6415887d40e6631266b27",
      "18dd3e02c2d74eb2aed8cd03d0529849",
      "6a6eba658708450fa5a97dbcb3e9c094",
      "5dfa25264b08469f8469e4bc8baaef07",
      "ee870c6c9f27495b89289aa5aeee96c3",
      "faac741308374d5795e033a0f2cf83dc",
      "07e2356cc5834517a374f16de0140c74",
      "97e9116106984d8298e4c0813abe87f4",
      "aed6080c46c54f94ad345046fabd1f0f",
      "f824a2f7ae4f4aca8a460799dee387ad",
      "7770f4362615422ea32886bce26b16c3",
      "1a7413fae57b4189aad255e4c7d521b3",
      "317032695c664eda8a27e115d17e5252",
      "2f2478e6621f47eab7f0afadb391c6fd",
      "84dd4cfeb33f46079cf4787c0ce09d6c",
      "95006d7e73294ca488c04cedc068825b",
      "5ec42573ca2f43d6b7b34bec9d587b31",
      "6de3deba929346af9269dcf1fbeb8086",
      "46b07b4fb40e4e0cafcd9ca63dbbae16",
      "31aeea55c55142d792d576e4e09a6aa2",
      "519b02954ae24195a743a283344dfc1d",
      "ac47f5b94ba44ecda3920be2bdf0540a",
      "e4df5c5551da4619be6186ee9997b8f3",
      "0873a9a35021402f8fc0bec589d9db44",
      "2ce9e019c2604c04bcf0b1fa21ebc880",
      "c85ba403baa54c34a4757a009ab4b338",
      "9d53936afd234dc8a880310d7decda4d",
      "6c10e718e68849f5856d661f1a005ade",
      "fdae170f9aad4a0f9a201877b70431c5",
      "3b161a5fb7384fc7ac63761a135449a6"
     ]
    },
    "id": "l5BZJClvwoe-",
    "outputId": "4fc6bf84-b7f6-4014-b975-67fdf4373477"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SETUP\n",
    "# ---------------------------------------------------------\n",
    "base_exp_path = f\"{experiment_path}/{my_experiment}\"\n",
    "runs_to_eval = [\"Baseline\", \"1\", \"2\", \"3\", \"4\"]\n",
    "final_results = []\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Load Tokenizer once\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # <--- CRITICAL for batch generation\n",
    "\n",
    "eval_subset = eval_dataset\n",
    "references = [ex['output'] for ex in eval_subset]\n",
    "\n",
    "# Pre-format all prompts to save time inside the loop\n",
    "formatted_prompts = []\n",
    "for example in eval_subset:\n",
    "    user_input = example.get('input', \"\") or \"\"\n",
    "    full_content = f\"{example['instruction']}\\n{user_input}\".strip()\n",
    "    prompt = f\"<|im_start|>user\\n{full_content}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    formatted_prompts.append(prompt)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# EVALUATION LOOP (With BERTScore)\n",
    "# ---------------------------------------------------------\n",
    "# Load metrics *once* before the loop to save time\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "for run_id in runs_to_eval:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"PROCESSING RUN: {run_id}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Clean Memory\n",
    "    if 'model' in globals(): del model\n",
    "    if 'base_model' in globals(): del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # 2. Load Base Model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # 3. Apply Adapter\n",
    "    if run_id == \"Baseline\":\n",
    "        model = base_model\n",
    "    else:\n",
    "        # Construct path (Adjust based on your folder structure)\n",
    "        adapter_path = f\"{base_exp_path}/runs/{run_id}/checkpoints/final_checkpoint\"\n",
    "\n",
    "        # Fallback check\n",
    "        if not os.path.exists(adapter_path):\n",
    "             chk_dir = f\"{base_exp_path}/runs/{run_id}/checkpoints\"\n",
    "             if os.path.exists(chk_dir):\n",
    "                 subdirs = [d for d in os.listdir(chk_dir) if d.startswith(\"checkpoint\")]\n",
    "                 if subdirs:\n",
    "                     latest = sorted(subdirs, key=lambda x: int(x.split('-')[-1]))[-1]\n",
    "                     adapter_path = f\"{chk_dir}/{latest}\"\n",
    "\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"\u26a0\ufe0f  Adapter not found at {adapter_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"   Loading Adapter from: {adapter_path}\")\n",
    "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 4. Batched Generation\n",
    "    print(f\"   Generating answers (Batch Size: {BATCH_SIZE})...\")\n",
    "    predictions = []\n",
    "\n",
    "    for i in tqdm(range(0, len(formatted_prompts), BATCH_SIZE)):\n",
    "        batch_prompts = formatted_prompts[i : i + BATCH_SIZE]\n",
    "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, max_new_tokens=128, temperature=0.6, do_sample=True, pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        batch_responses = tokenizer.batch_decode(outputs[:, input_len:], skip_special_tokens=True)\n",
    "        predictions.extend(batch_responses)\n",
    "\n",
    "    # 5. Metrics Calculation\n",
    "    print(\"Calculating Metrics (ROUGE + BERTScore)...\")\n",
    "\n",
    "    # ROUGE\n",
    "    r_scores = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])\n",
    "\n",
    "    # BERTScore (Runs on CPU to avoid OOM)\n",
    "    b_scores = bertscore.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        lang=\"en\",\n",
    "        model_type=\"distilbert-base-uncased\",\n",
    "        device=\"cpu\"  # Keep this CPU!\n",
    "    )\n",
    "    import numpy as np\n",
    "    avg_bert = np.mean(b_scores['f1'])\n",
    "\n",
    "    stats = {\n",
    "        \"Run\": run_id,\n",
    "        \"ROUGE-L\": round(r_scores['rougeL'], 4),\n",
    "        \"BERTScore\": round(avg_bert, 4),\n",
    "        \"Sample Answer\": predictions[0][:150] + \"...\"\n",
    "    }\n",
    "    final_results.append(stats)\n",
    "    print(f\"  ROUGE: {stats['ROUGE-L']} | BERT: {stats['BERTScore']}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FINAL SUMMARY TABLE\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83c\udfc6 FINAL COMPARISON REPORT\")\n",
    "print(\"=\"*60)\n",
    "df = pd.DataFrame(final_results)\n",
    "cols = [\"Run\", \"ROUGE-L\", \"BERTScore\", \"Sample Answer\"]\n",
    "print(df[cols].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_BdstK5wNEv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkW5NU0jUjsz"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}