{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6c633e",
   "metadata": {
    "id": "dd6c633e"
   },
   "source": [
    "# PII Masking with GPT-2 - RapidFire AI Competition Submission\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/suraj-ranganath/pii-redaction/blob/main/rf_pii_masking_experiments.ipynb)\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Do not let the Colab notebook tab stay idle for more than 5min; Colab will disconnect otherwise. Refresh the TensorBoard screen or interact with the cells to avoid disconnection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5381945",
   "metadata": {
    "id": "c5381945"
   },
   "source": [
    "# PII Masking with GPT-2 and RapidFire AI\n",
    "\n",
    "## RapidFire AI Winter Competition Submission\n",
    "\n",
    "This notebook demonstrates **Supervised Fine-Tuning (SFT)** of GPT-2 for PII (Personally Identifiable Information) masking using [RapidFire AI](https://github.com/RapidFireAI/rapidfireai).\n",
    "\n",
    "**Task:** Given text containing PII, generate text with PII replaced by appropriate mask tokens.\n",
    "\n",
    "**Key Features:**\n",
    "- üöÄ Hyperparallel execution of 8 experiment configurations using `run_fit()`\n",
    "- üìä Real-time TensorBoard metrics visualization\n",
    "- üéõÔ∏è Interactive controls: Stop, Clone-Modify underperforming runs\n",
    "- üî¨ Structured experimentation across prompt schemes, LoRA ranks, and learning rates\n",
    "- üìà Exact Match (EM) metric for generation quality\n",
    "\n",
    "**References:**\n",
    "- [RapidFire Docs](http://oss-docs.rapidfire.ai/en/latest/difference.html)\n",
    "- [RapidFire Colab Tutorial](https://colab.research.google.com/github/RapidFireAI/rapidfireai/blob/main/tutorial_notebooks/fine-tuning/rf-colab-tensorboard-tutorial.ipynb)\n",
    "- [TRL RapidFire Integration](https://huggingface.co/docs/trl/en/rapidfire_integration)\n",
    "- [RapidFire Blog](https://huggingface.co/blog/rapidfireai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39750014",
   "metadata": {},
   "source": [
    "## üìå Note: Using Pre-Existing Training Results\n",
    "\n",
    "**This notebook can be used in two ways:**\n",
    "\n",
    "1. **Run full training in Colab** (sections 1-29): Trains all 8 configurations from scratch\n",
    "2. **Analyze existing results only** (sections 30+): Skip training, jump directly to results extraction and visualization\n",
    "\n",
    "If you have already completed training (or are reviewing this submission), you can **skip directly to the \"Extract Results from Training Logs\" section** (around cell 35). All analysis cells read from saved checkpoints in `rapidfireai/rapidfire_experiments/pii-masking-gpt2-v1-all/` and work independently of runtime variables.\n",
    "\n",
    "**For submission review:** The training has already been completed. All metrics, plots, and analysis below are generated from the saved training artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66029f",
   "metadata": {
    "id": "4d66029f"
   },
   "source": [
    "## Install RapidFire AI Package and Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179e737",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f179e737",
    "outputId": "488b8793-b941-4daf-80d2-7ca611eef29b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import rapidfireai, mlflow\n",
    "    print(\"‚úÖ rapidfireai and mlflow already installed\")\n",
    "except ImportError:\n",
    "    %pip install rapidfireai mlflow  # Install both rapidfireai and mlflow\n",
    "    !rapidfireai init # Takes 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2e12a",
   "metadata": {
    "id": "f4f2e12a"
   },
   "source": [
    "## Start RapidFire Services\n",
    "\n",
    "- If any issues arise, check status using `rapidfireai status` or `rapidfireai doctor`\n",
    "- Services run on ports 8851, 8852, 8853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921ee3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6921ee3c",
    "outputId": "4c97b348-260b-4aa8-eb1b-d3e42f280790"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from time import sleep\n",
    "import socket\n",
    "try:\n",
    "  s = [socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM), socket.socket(socket.AF_INET, socket.SOCK_STREAM)]\n",
    "  s[0].connect((\"127.0.0.1\", 8851))\n",
    "  s[1].connect((\"127.0.0.1\", 8852))\n",
    "  s[2].connect((\"127.0.0.1\", 8853))\n",
    "  s[0].close()\n",
    "  s[1].close()\n",
    "  s[2].close()\n",
    "  print(\"RapidFire Services are running\")\n",
    "except OSError as error:\n",
    "  print(\"RapidFire Services are not running, launching now...\")\n",
    "  subprocess.Popen([\"rapidfireai\", \"start\"])\n",
    "  sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217153f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a217153f",
    "outputId": "d7bea5af-c4ce-40e4-9489-2898537d6edf"
   },
   "outputs": [],
   "source": [
    "!rapidfireai status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2f6fa",
   "metadata": {
    "id": "45b2f6fa"
   },
   "source": [
    "## Configure RapidFire to Use TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5f835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bc5f835",
    "outputId": "d29eb1f9-3a76-43fe-f18a-75e55656c540"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Configure RapidFire to use TensorBoard\n",
    "os.environ['RF_TRACKING_BACKEND'] = 'tensorboard'  # Options: 'mlflow', 'tensorboard', 'both'\n",
    "# TensorBoard log directory will be auto-created in experiment path\n",
    "\n",
    "print(\"‚úÖ TensorBoard configured as tracking backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe8668",
   "metadata": {
    "id": "e4fe8668"
   },
   "source": [
    "## Import RapidFire Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b094ff4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b094ff4",
    "outputId": "e638a1cd-4e50-46dd-d022-fab041c8cd9f"
   },
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFSFTConfig\n",
    "\n",
    "# NB: If you get \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\" from Colab, just rerun this cell\n",
    "print(\"‚úÖ RapidFire components imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a6c36",
   "metadata": {
    "id": "3d9a6c36"
   },
   "source": [
    "## Load PII Masking Dataset\n",
    "\n",
    "We use the `ai4privacy/open-pii-masking-500k-ai4privacy` dataset.\n",
    "\n",
    "**Dataset Details:**\n",
    "- Source: AI4Privacy open PII masking dataset (500k examples)\n",
    "- Task: Text-to-text, replace PII with mask tokens\n",
    "- Fields: `source_text` (input), `masked_text` (target)\n",
    "- Train subset: 10,000 examples\n",
    "- Eval subset: 1,000 examples\n",
    "- Generation eval subset: 500 examples (for EM calculation)\n",
    "\n",
    "We filter for English examples and use a manageable subset for Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94b257",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465,
     "referenced_widgets": [
      "66f4028199f244e4b07610c4f7cce379",
      "371b52449b374ee19faf87dc84292a3e",
      "e7925d322daa45c3ad067f6bba213e12",
      "16ebbfeae08b4b9ba2a6a2233dca09e2",
      "6ec6c71da7d7435a98a949a17417a5aa",
      "c7bf3062a528492ba526342887e17293",
      "d85fa354dd8a4fef84a6332ba1f0e5ae",
      "9e8260d04bcc4dee8b1a185200ba2ce4",
      "8eb7ea8230ff47d7a4a2ebe511ea0a74",
      "272592db468f4f61870d9f67268f3323",
      "2e7db72ae8ee44ec9fb9cec0a373e2d1",
      "6c513051c56e4260980c854c4d93338a",
      "341fbea374b04264aa7ac089f888973f",
      "3fd6cbc4443f4c748c3123470a6d5ce6",
      "e2bfdd1b978f4491b4a73c3d2d7a9c0d",
      "b8b99b102b6b4e5dae1a7b527b977de9",
      "7f789c58aa23445ea606c9b7851c2bc8",
      "6a324726def84729a032e650ac76c1fe",
      "499a330ed95248408492cb5cd9fed517",
      "97a794a08e4b42fdb3e1dbaee999222e",
      "c2f847f7d3d84c899b9dbdb7a7004ca0",
      "7db5bf2377a04dbb8c24560db25b3668",
      "eaeafed01d1847d98aabbece2d8ce889",
      "754b63e6ab8f4a51ab1cfddff3933670",
      "8212884db7634f8d862ec793899d97cc",
      "38611fa3a23d4e16bbf1f6f1b87d941b",
      "f898286f6e2e49db9219145370fea8fe",
      "0bd589110d3b46e092be168a324512fb",
      "2c14b612a7cc4a988abe0e36d8400c8d",
      "beb60be7091f4446b89c4f7eb3974763",
      "6a67795523a14cd2b6b0cd21e8c75721",
      "6428c394f77d43548d64bee1da313199",
      "e0f628a1638f40e7bc6dd83d7059e372",
      "784fd0009a844bb8ac2293a45fb80b68",
      "da5b49605d51442a9f7ef2cebd9869f0",
      "e312de1632fc49178ee22d884caad8b9",
      "5d58f2a8afba4e8386344b8f8a8cbb62",
      "0a4400d5e4194be4aaef43ea401c7466",
      "5e26055fb8fd4329be35c56fbf922151",
      "0a50f0c8c6bd442ca24b8c03f3865c5e",
      "f2d480db2aa0409e80f9be7037ad8c96",
      "63e7719dad0a4bba9c6bc1e0bc7f4579",
      "5411df6aeb954edebd6efc88456b30cc",
      "17c4cf2469e045f3b9b4f899d46cd98f",
      "8f0330f9098b4d85b985ceebe57577b1",
      "861cde3a3380400c8c7a8ff5374c7306",
      "49c7281fd7694b6b9725c481dcd43ad2",
      "090dc2f3c8ca41d89b0c2e361dc9d9db",
      "2753121c375f4ff28d3ac657b62120ae",
      "37017ed5c384454ebd9d4b4b4ec61d52",
      "2fd4b99cf0a5469b89f0458eb244e673",
      "69ff7f5f5edf4d44b557db1c50902bec",
      "5805b2feb5c14f4e828d10fd1d5c6667",
      "f0d1072af0b94bc4bc9efbae3fc5dac2",
      "3c1d70f2805a4274b2791e11a41671f2"
     ]
    },
    "id": "2e94b257",
    "outputId": "d4ecfc2f-0fa9-4ff9-f76b-36a5d3901f98"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load full dataset\n",
    "print(\"Loading PII masking dataset...\")\n",
    "dataset = load_dataset(\"ai4privacy/open-pii-masking-500k-ai4privacy\")\n",
    "\n",
    "# Get train split\n",
    "full_train = dataset[\"train\"]\n",
    "\n",
    "# Filter for English examples (optional - dataset may already be English)\n",
    "# For simplicity, we'll use the data as-is\n",
    "\n",
    "# Create subsets for Colab memory constraints\n",
    "train_dataset = full_train.select(range(64))  # 64 training examples\n",
    "eval_dataset = full_train.select(range(64, 74))  # 10 eval examples\n",
    "gen_eval_dataset = full_train.select(range(74, 84))  # 10 for generation eval\n",
    "\n",
    "# Shuffle for better diversity\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42)\n",
    "gen_eval_dataset = gen_eval_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train: {len(train_dataset)} examples\")\n",
    "print(f\"   Eval: {len(eval_dataset)} examples\")\n",
    "print(f\"   Generation Eval: {len(gen_eval_dataset)} examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(f\"Source: {train_dataset[0]['source_text'][:100]}...\")\n",
    "print(f\"Masked: {train_dataset[0]['masked_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278aa7a",
   "metadata": {
    "id": "e278aa7a"
   },
   "source": [
    "## Define Two Prompt Formatting Schemes\n",
    "\n",
    "We experiment with two different prompt formats (Knob Type #1):\n",
    "\n",
    "### Prompt A: Minimal Instruction\n",
    "Simple task instruction without examples.\n",
    "\n",
    "### Prompt B: One-Shot Example\n",
    "Includes one hardcoded example before the actual task.\n",
    "\n",
    "Both prompts ensure the model outputs only the masked text (no explanations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb49a9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eb49a9c",
    "outputId": "074a0e0b-327b-4e13-e8ee-ebe70971989c"
   },
   "outputs": [],
   "source": [
    "def formatting_function_prompt_a(example):\n",
    "    \"\"\"Prompt A: Minimal instruction-based format\"\"\"\n",
    "    prompt = f\"\"\"Instruction: Mask all PII in the text.\n",
    "Text:\n",
    "{example['source_text']}\n",
    "Masked:\n",
    "\"\"\"\n",
    "    # For training: full sequence is prompt + target\n",
    "    full_text = prompt + example['masked_text']\n",
    "\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"source_text\": example['source_text'],  # Keep original\n",
    "        \"masked_text\": example['masked_text']  # Keep original\n",
    "    }\n",
    "\n",
    "\n",
    "def formatting_function_prompt_b(example):\n",
    "    \"\"\"Prompt B: One-shot example format\"\"\"\n",
    "    # Hardcoded one-shot example\n",
    "    one_shot_example = \"\"\"Example:\n",
    "Text:\n",
    "My name is John Smith and my email is john.smith@email.com.\n",
    "Masked:\n",
    "My name is [NAME] and my email is [EMAIL].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Instruction: Mask all PII in the text.\n",
    "\n",
    "{one_shot_example}Now mask this text:\n",
    "Text:\n",
    "{example['source_text']}\n",
    "Masked:\n",
    "\"\"\"\n",
    "    # For training: full sequence is prompt + target\n",
    "    full_text = prompt + example['masked_text']\n",
    "\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"source_text\": example['source_text'],  # Keep original\n",
    "        \"masked_text\": example['masked_text']  # Keep original\n",
    "    }\n",
    "\n",
    "\n",
    "# Test both formatting functions\n",
    "print(\"=\" * 80)\n",
    "print(\"PROMPT A (Minimal):\")\n",
    "print(\"=\" * 80)\n",
    "sample_a = formatting_function_prompt_a(train_dataset[0])\n",
    "print(sample_a['text'][:300])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROMPT B (One-Shot):\")\n",
    "print(\"=\" * 80)\n",
    "sample_b = formatting_function_prompt_b(train_dataset[0])\n",
    "print(sample_b['text'][:400])\n",
    "print(\"\\n‚úÖ Prompt formatting functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c3bef",
   "metadata": {
    "id": "111c3bef"
   },
   "source": [
    "## Define Model Creation Function with GPT-2 Setup\n",
    "\n",
    "GPT-2 requires special tokenizer configuration:\n",
    "- Set `pad_token = eos_token` (GPT-2 has no default pad token)\n",
    "- Set `model.config.pad_token_id = tokenizer.eos_token_id`\n",
    "- Use left padding for decoder-only models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6dfd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def6dfd1",
    "outputId": "9f44d8c3-37c1-4321-d5a0-08a1f82e6e92"
   },
   "outputs": [],
   "source": [
    "def create_model_gpt2(model_config):\n",
    "    \"\"\"Create GPT-2 model with proper tokenizer setup\"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    model_type = model_config[\"model_type\"]\n",
    "    model_kwargs = model_config[\"model_kwargs\"]\n",
    "\n",
    "    # Load model\n",
    "    if model_type == \"causal_lm\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # GPT-2 specific setup (CRITICAL)\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"  # GPT-2 works better with left padding\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        print(f\"‚úÖ GPT-2 tokenizer configured: pad_token={tokenizer.pad_token}, pad_token_id={model.config.pad_token_id}\")\n",
    "\n",
    "    return (model, tokenizer)\n",
    "\n",
    "print(\"‚úÖ Model creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd5568",
   "metadata": {
    "id": "cfcd5568"
   },
   "source": [
    "## Define Compute Metrics Function\n",
    "\n",
    "We compute Exact Match (EM) on generated outputs during evaluation.\n",
    "Exact Match measures how many generated masked texts exactly match the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67593265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67593265",
    "outputId": "4b8b9bbc-fe4e-4489-9bab-3113af5a1acd"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_pii(eval_preds):\n",
    "    \"\"\"Compute Exact Match (EM) for PII masking\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Normalize predictions and labels (strip whitespace, lowercase)\n",
    "    def normalize(text):\n",
    "        return text.strip().lower()\n",
    "\n",
    "    # Calculate Exact Match\n",
    "    exact_matches = sum(1 for pred, label in zip(predictions, labels)\n",
    "                       if normalize(pred) == normalize(label))\n",
    "    em = exact_matches / len(predictions) if predictions else 0.0\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": round(em, 4),\n",
    "        \"num_exact_matches\": exact_matches,\n",
    "        \"total_examples\": len(predictions)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metrics function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ee6f4",
   "metadata": {
    "id": "968ee6f4"
   },
   "source": [
    "## Define 8-Run Experiment Grid (Split into 4 Batches)\n",
    "\n",
    "**Experiment Dimensions (Knobs):**\n",
    "\n",
    "1. **Prompt Scheme** (2 values): Prompt A (minimal) vs Prompt B (one-shot)\n",
    "2. **LoRA Rank** (2 values): r=8 vs r=32\n",
    "3. **Learning Rate** (2 values): 1e-4 vs 5e-4\n",
    "\n",
    "**Total Configurations:** 2 √ó 2 √ó 2 = **8 runs**\n",
    "\n",
    "**Execution Strategy:** To handle Google Colab memory and compute limits, we run in **4 batches of 2 runs each**:\n",
    "- **Batch 1:** Prompt A, lr=5e-4 (2 runs: r=8, r=32)\n",
    "- **Batch 2:** Prompt A, lr=2e-4 (2 runs: r=8, r=32)\n",
    "- **Batch 3:** Prompt B, lr=5e-4 (2 runs: r=8, r=32)\n",
    "- **Batch 4:** Prompt B, lr=2e-4 (2 runs: r=8, r=32)\n",
    "\n",
    "**Fixed Parameters:**\n",
    "- LoRA target modules: `c_attn`, `c_proj` (GPT-2 attention and projection layers)\n",
    "- Max steps: 400 (or 1 epoch, whichever comes first)\n",
    "- Batch size: 4 (per device) with gradient accumulation\n",
    "- Max length: 512 tokens\n",
    "- Evaluation: Every 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efd01e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89efd01e",
    "outputId": "d1cda781-09af-45f1-a4fb-824fc758051f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Base model kwargs (shared across all configs)\n",
    "base_model_kwargs = {\n",
    "    \"device_map\": \"auto\",\n",
    "    \"torch_dtype\": \"float16\",\n",
    "    \"use_cache\": False\n",
    "}\n",
    "\n",
    "# Base generation config (shared across all configs)\n",
    "base_generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"pad_token_id\": 50256,  # GPT-2's EOS token\n",
    "}\n",
    "\n",
    "# GPT-2 specific LoRA configs - shared across all RFModelConfigs\n",
    "# RapidFire will expand this List to create variations\n",
    "peft_configs = List([\n",
    "    RFLoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\"],  # GPT-2 attention modules\n",
    "        bias=\"none\"\n",
    "    ),\n",
    "    RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# BATCH 1: Prompt A, lr=5e-4 (1 RFModelConfig √ó 2 peft_configs = 2 runs)\n",
    "configs_batch1 = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",\n",
    "        peft_config=peft_configs,  # Shared List - RapidFire expands this\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=5e-4,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=64,\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs=base_model_kwargs,\n",
    "        formatting_func=formatting_function_prompt_a,\n",
    "        compute_metrics=compute_metrics_pii,\n",
    "        generation_config=base_generation_config,\n",
    "    ),\n",
    "])\n",
    "\n",
    "# BATCH 2: Prompt A, lr=2e-4 (1 RFModelConfig √ó 2 peft_configs = 2 runs)\n",
    "configs_batch2 = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",\n",
    "        peft_config=peft_configs,  # Shared List - RapidFire expands this\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=64,\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "            warmup_steps=10\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs=base_model_kwargs,\n",
    "        formatting_func=formatting_function_prompt_a,\n",
    "        compute_metrics=compute_metrics_pii,\n",
    "        generation_config=base_generation_config,\n",
    "    ),\n",
    "])\n",
    "\n",
    "# BATCH 3: Prompt B, lr=5e-4 (1 RFModelConfig √ó 2 peft_configs = 2 runs)\n",
    "configs_batch3 = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",\n",
    "        peft_config=peft_configs,  # Shared List - RapidFire expands this\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=5e-4,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=64,\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs=base_model_kwargs,\n",
    "        formatting_func=formatting_function_prompt_b,\n",
    "        compute_metrics=compute_metrics_pii,\n",
    "        generation_config=base_generation_config,\n",
    "    ),\n",
    "])\n",
    "\n",
    "# BATCH 4: Prompt B, lr=2e-4 (1 RFModelConfig √ó 2 peft_configs = 2 runs)\n",
    "configs_batch4 = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"gpt2\",\n",
    "        peft_config=peft_configs,  # Shared List - RapidFire expands this\n",
    "        training_args=RFSFTConfig(\n",
    "            learning_rate=2e-4,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=64,\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=4,\n",
    "            per_device_eval_batch_size=2,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "            warmup_steps=10\n",
    "        ),\n",
    "        model_type=\"causal_lm\",\n",
    "        model_kwargs=base_model_kwargs,\n",
    "        formatting_func=formatting_function_prompt_b,\n",
    "        compute_metrics=compute_metrics_pii,\n",
    "        generation_config=base_generation_config,\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Print experiment grid explanation\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT GRID (8 Configurations in 4 Batches)\")\n",
    "print(\"=\"*80)\n",
    "print(\"To handle Colab memory limits, we split into 4 batches:\")\n",
    "print(\"  - Batch 1: Prompt A, lr=5e-4 ‚Üí 2 runs (r=8, r=32)\")\n",
    "print(\"  - Batch 2: Prompt A, lr=2e-4 ‚Üí 2 runs (r=8, r=32)\")\n",
    "print(\"  - Batch 3: Prompt B, lr=5e-4 ‚Üí 2 runs (r=8, r=32)\")\n",
    "print(\"  - Batch 4: Prompt B, lr=2e-4 ‚Üí 2 runs (r=8, r=32)\")\n",
    "print(\"\")\n",
    "print(\"Knob #1 - Prompt Schemes (2 values):\")\n",
    "print(\"  - Prompt A: Minimal instruction-based format\")\n",
    "print(\"  - Prompt B: One-shot example format\")\n",
    "print(\"\")\n",
    "print(\"Knob #2 - LoRA Rank (2 values):\")\n",
    "print(\"  - r=8 (lora_alpha=16)\")\n",
    "print(\"  - r=32 (lora_alpha=64)\")\n",
    "print(\"\")\n",
    "print(\"Knob #3 - Learning Rate (2 values):\")\n",
    "print(\"  - 5e-4\")\n",
    "print(\"  - 2e-4\")\n",
    "print(\"\")\n",
    "print(\"Total combinations: 2 prompts √ó 2 ranks √ó 2 LRs = 8 total runs\")\n",
    "print(\"Execution: 4 batches √ó 2 runs each = 8 runs total\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a config map for reference (manual tracking)\n",
    "config_map = {\n",
    "    \"promptA_r8_lr5e-04\": {\"id\": 1, \"prompt_variant\": \"A\", \"lora_rank\": 8, \"learning_rate\": 5e-4},\n",
    "    \"promptA_r32_lr5e-04\": {\"id\": 2, \"prompt_variant\": \"A\", \"lora_rank\": 32, \"learning_rate\": 5e-4},\n",
    "    \"promptA_r8_lr2e-04\": {\"id\": 3, \"prompt_variant\": \"A\", \"lora_rank\": 8, \"learning_rate\": 2e-4},\n",
    "    \"promptA_r32_lr2e-04\": {\"id\": 4, \"prompt_variant\": \"A\", \"lora_rank\": 32, \"learning_rate\": 2e-4},\n",
    "    \"promptB_r8_lr5e-04\": {\"id\": 5, \"prompt_variant\": \"B\", \"lora_rank\": 8, \"learning_rate\": 5e-4},\n",
    "    \"promptB_r32_lr5e-04\": {\"id\": 6, \"prompt_variant\": \"B\", \"lora_rank\": 32, \"learning_rate\": 5e-4},\n",
    "    \"promptB_r8_lr2e-04\": {\"id\": 7, \"prompt_variant\": \"B\", \"lora_rank\": 8, \"learning_rate\": 2e-4},\n",
    "    \"promptB_r32_lr2e-04\": {\"id\": 8, \"prompt_variant\": \"B\", \"lora_rank\": 32, \"learning_rate\": 2e-4},\n",
    "}\n",
    "\n",
    "# Save config map to file\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "with open(\"outputs/run_config_map.json\", \"w\") as f:\n",
    "    json.dump(config_map, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Configuration batches created:\")\n",
    "print(\"   - 4 batches, each with 1 RFModelConfig ‚Üí 2 runs\")\n",
    "print(\"   - Total: 8 runs across 4 batches\")\n",
    "print(\"   Config map saved to outputs/run_config_map.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0660c",
   "metadata": {
    "id": "e4c0660c"
   },
   "source": [
    "## Initialize Experiment and Get TensorBoard Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a46b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba2a46b5",
    "outputId": "fe9bb4a7-b8d8-4405-caba-07adca5b3c50"
   },
   "outputs": [],
   "source": [
    "# Create experiment with unique name\n",
    "my_experiment = \"pii-masking-gpt2-v1\"\n",
    "experiment = Experiment(experiment_name=my_experiment)\n",
    "\n",
    "# Get TensorBoard log directory\n",
    "from rapidfireai.fit.db.rf_db import RfDb\n",
    "\n",
    "db = RfDb()\n",
    "experiment_path = db.get_experiments_path(my_experiment)\n",
    "tensorboard_log_dir = f\"{experiment_path}/{my_experiment}/tensorboard_logs\"\n",
    "\n",
    "print(f\"‚úÖ Experiment initialized: {my_experiment}\")\n",
    "print(f\"üìä TensorBoard logs will be saved to: {tensorboard_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff40f05",
   "metadata": {
    "id": "eff40f05"
   },
   "source": [
    "## Create RFGridSearch Configuration Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd9962",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56dd9962",
    "outputId": "6e0c8f3d-84c4-4a21-b89e-a7a394b3e87f"
   },
   "outputs": [],
   "source": [
    "# Create four separate grid searches for batched execution\n",
    "config_group_batch1 = RFGridSearch(\n",
    "    configs=configs_batch1,  # 1 RFModelConfig (Prompt A, lr=5e-4)\n",
    "    trainer_type=\"SFT\"\n",
    ")\n",
    "\n",
    "config_group_batch2 = RFGridSearch(\n",
    "    configs=configs_batch2,  # 1 RFModelConfig (Prompt A, lr=2e-4)\n",
    "    trainer_type=\"SFT\"\n",
    ")\n",
    "\n",
    "config_group_batch3 = RFGridSearch(\n",
    "    configs=configs_batch3,  # 1 RFModelConfig (Prompt B, lr=5e-4)\n",
    "    trainer_type=\"SFT\"\n",
    ")\n",
    "\n",
    "config_group_batch4 = RFGridSearch(\n",
    "    configs=configs_batch4,  # 1 RFModelConfig (Prompt B, lr=2e-4)\n",
    "    trainer_type=\"SFT\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ RFGridSearch batches created\")\n",
    "print(f\"   Batch 1: Prompt A, lr=5e-4 ‚Üí 2 parallel runs\")\n",
    "print(f\"   Batch 2: Prompt A, lr=2e-4 ‚Üí 2 parallel runs\")\n",
    "print(f\"   Batch 3: Prompt B, lr=5e-4 ‚Üí 2 parallel runs\")\n",
    "print(f\"   Batch 4: Prompt B, lr=2e-4 ‚Üí 2 parallel runs\")\n",
    "print(f\"   Trainer type: SFT (Supervised Fine-Tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3772bb",
   "metadata": {
    "id": "0c3772bb"
   },
   "source": [
    "## Start TensorBoard (BEFORE run_fit)\n",
    "\n",
    "**IMPORTANT:** Start TensorBoard BEFORE invoking `run_fit()` to watch metrics appear in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb82472",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "3fb82472",
    "outputId": "cbcb708e-fe93-49df-c3d2-330442b2908a"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc7f31",
   "metadata": {
    "id": "8edc7f31"
   },
   "source": [
    "## Run Hyperparallel Training with run_fit() - Choose Execution Mode\n",
    "\n",
    "We run experiments in **four batches of 2 runs each** to minimize memory usage on Colab.\n",
    "\n",
    "**Batch 1:** Prompt A, lr=5e-4 (2 runs)\n",
    "**Batch 2:** Prompt A, lr=2e-4 (2 runs)\n",
    "**Batch 3:** Prompt B, lr=5e-4 (2 runs)\n",
    "**Batch 4:** Prompt B, lr=2e-4 (2 runs)\n",
    "\n",
    "Each batch runs 2 configurations in parallel (different LoRA ranks), then moves to the next batch.\n",
    "Each batch also gets its **own experiment name + TensorBoard log directory** for clean tracking.\n",
    "\n",
    "**Alternative:** You can also run all 8 configs in a single training run with `num_chunks=2` for maximum parallelism (but higher memory usage). Set `RUN_IN_BATCHES = False` in the next cell.\n",
    "\n",
    "**Expected runtime:** ~30-60 minutes total (7-15 min per batch) on free Colab GPU\n",
    "\n",
    "\n",
    "**Expected runtime:** ~30-60 minutes total (7-15 min per batch) on free Colab GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c7e22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "df4c7e22",
    "outputId": "4da07932-4df7-4379-ecf8-0f90abe2f3e3"
   },
   "outputs": [],
   "source": [
    "# Choose execution mode\n",
    "RUN_IN_BATCHES = False  # Set to False to run all 8 configs in one go with num_chunks=2\n",
    "\n",
    "if RUN_IN_BATCHES:\n",
    "    # Launch hyperparallel training in 4 batches\n",
    "    print(\"üöÄ Starting hyperparallel training in 4 batches...\")\n",
    "    print(f\"   Training dataset: {len(train_dataset)} examples\")\n",
    "    print(f\"   Eval dataset: {len(eval_dataset)} examples\")\n",
    "    print(f\"   Chunk-based scheduling with num_chunks=4\")\n",
    "\n",
    "    batch_specs = [\n",
    "        (\"BATCH 1/4\", \"Prompt A, lr=5e-4\", config_group_batch1),\n",
    "        (\"BATCH 2/4\", \"Prompt A, lr=2e-4\", config_group_batch2),\n",
    "        (\"BATCH 3/4\", \"Prompt B, lr=5e-4\", config_group_batch3),\n",
    "        (\"BATCH 4/4\", \"Prompt B, lr=2e-4\", config_group_batch4),\n",
    "    ]\n",
    "\n",
    "    total_runs = len(batch_specs) * 2\n",
    "\n",
    "    for idx, (batch_label, batch_desc, config_group) in enumerate(batch_specs, start=1):\n",
    "        exp_name = f\"{my_experiment}-batch{idx}\"\n",
    "        experiment = Experiment(experiment_name=exp_name)\n",
    "        experiment_path = db.get_experiments_path(exp_name)\n",
    "        tensorboard_log_dir = f\"{experiment_path}/{exp_name}/tensorboard_logs\"\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{batch_label}: {batch_desc} (2 parallel runs: r=8, r=32)\")\n",
    "        print(f\"Experiment: {exp_name}\")\n",
    "        print(f\"TensorBoard log dir: {tensorboard_log_dir}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        experiment.run_fit(\n",
    "            config_group,\n",
    "            create_model_gpt2,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            num_chunks=4,  # Chunk-based scheduling for hyperparallelism\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # ROBUST POLLING LOOP: Ensuring sequential batch execution for Colab\n",
    "        print(f\"‚è≥ Waiting for {batch_label} to complete...\")\n",
    "        import time\n",
    "\n",
    "        # Initial wait to allow runs to register in the background\n",
    "        time.sleep(15)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                runs_info = experiment.get_runs_info()\n",
    "                if runs_info is not None and not runs_info.empty:\n",
    "                    active_statuses = ['RUNNING', 'QUEUED', 'STARTING']\n",
    "                    active_runs = runs_info[runs_info['status'].isin(active_statuses)]\n",
    "\n",
    "                    # Only exit when zero active runs remain AND we actually fetched some info\n",
    "                    if len(active_runs) == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    # If runs_info is empty, it might still be registering.\n",
    "                    # Do NOT break yet.\n",
    "                    print(\"   ...registering runs...\")\n",
    "            except Exception:\n",
    "                # Ignore transient errors and retry\n",
    "                pass\n",
    "\n",
    "            time.sleep(30)  # Poll every 30 seconds to keep Colab active\n",
    "\n",
    "        completed_runs = idx * 2\n",
    "        print(f\"\\n‚úÖ {batch_label} completed ({completed_runs}/{total_runs} configurations done)\\n\")\n",
    "\n",
    "    print(\"üéâ All 8 configurations completed training!\")\n",
    "\n",
    "else:\n",
    "    # Run all 8 configs in one training run with num_chunks=2\n",
    "    print(\"üöÄ Starting hyperparallel training for all 8 configs in one run...\")\n",
    "    print(f\"   Training dataset: {len(train_dataset)} examples\")\n",
    "    print(f\"   Eval dataset: {len(eval_dataset)} examples\")\n",
    "    print(f\"   Chunk-based scheduling with num_chunks=2\")\n",
    "    print(\"\\n‚è≥ This will take approximately 30-60 minutes. Watch TensorBoard above for real-time metrics!\\n\")\n",
    "\n",
    "    # # Combine all configs into one list (each RFModelConfig expands to 2 runs via peft_configs)\n",
    "    # # RF List objects may not be subscriptable - convert to Python lists safely\n",
    "    # py_configs = []\n",
    "    # for batch in (configs_batch1, configs_batch2, configs_batch3, configs_batch4):\n",
    "    #     try:\n",
    "    #         # Try converting directly\n",
    "    #         py_configs.extend(list(batch))\n",
    "    #     except Exception:\n",
    "    #         # Fallback: iterate and append\n",
    "    #         for item in batch:\n",
    "    #             py_configs.append(item)\n",
    "\n",
    "    # # Wrap back into a RapidFire List for RFGridSearch\n",
    "    # all_configs = List(py_configs)\n",
    "\n",
    "    # # Create single grid search for all configs\n",
    "    # config_group_all = RFGridSearch(\n",
    "    #     configs=all_configs,\n",
    "    #     trainer_type=\"SFT\"\n",
    "    # )\n",
    "\n",
    "    # Explicitly define the four RFModelConfig objects for the combined run\n",
    "    all_configs = List([\n",
    "        RFModelConfig(\n",
    "            model_name=\"gpt2\",\n",
    "            peft_config=peft_configs,\n",
    "            training_args=RFSFTConfig(\n",
    "                learning_rate=5e-4,\n",
    "                lr_scheduler_type=\"linear\",\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=2,\n",
    "                max_steps=64,\n",
    "                logging_steps=2,\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=4,\n",
    "                per_device_eval_batch_size=4,\n",
    "                fp16=True,\n",
    "                gradient_checkpointing=True,\n",
    "                report_to=\"none\",\n",
    "            ),\n",
    "            model_type=\"causal_lm\",\n",
    "            model_kwargs=base_model_kwargs,\n",
    "            formatting_func=formatting_function_prompt_a,\n",
    "            compute_metrics=compute_metrics_pii,\n",
    "            generation_config=base_generation_config,\n",
    "        ),\n",
    "        RFModelConfig(\n",
    "            model_name=\"gpt2\",\n",
    "            peft_config=peft_configs,\n",
    "            training_args=RFSFTConfig(\n",
    "                learning_rate=2e-4,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=2,\n",
    "                max_steps=64,\n",
    "                logging_steps=2,\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=4,\n",
    "                per_device_eval_batch_size=2,\n",
    "                fp16=True,\n",
    "                gradient_checkpointing=True,\n",
    "                report_to=\"none\",\n",
    "                warmup_steps=10,\n",
    "            ),\n",
    "            model_type=\"causal_lm\",\n",
    "            model_kwargs=base_model_kwargs,\n",
    "            formatting_func=formatting_function_prompt_a,\n",
    "            compute_metrics=compute_metrics_pii,\n",
    "            generation_config=base_generation_config,\n",
    "        ),\n",
    "        RFModelConfig(\n",
    "            model_name=\"gpt2\",\n",
    "            peft_config=peft_configs,\n",
    "            training_args=RFSFTConfig(\n",
    "                learning_rate=5e-4,\n",
    "                lr_scheduler_type=\"linear\",\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=2,\n",
    "                max_steps=64,\n",
    "                logging_steps=2,\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=4,\n",
    "                per_device_eval_batch_size=2,\n",
    "                fp16=True,\n",
    "                gradient_checkpointing=True,\n",
    "                report_to=\"none\",\n",
    "            ),\n",
    "            model_type=\"causal_lm\",\n",
    "            model_kwargs=base_model_kwargs,\n",
    "            formatting_func=formatting_function_prompt_b,\n",
    "            compute_metrics=compute_metrics_pii,\n",
    "            generation_config=base_generation_config,\n",
    "        ),\n",
    "        RFModelConfig(\n",
    "            model_name=\"gpt2\",\n",
    "            peft_config=peft_configs,\n",
    "            training_args=RFSFTConfig(\n",
    "                learning_rate=2e-4,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=2,\n",
    "                max_steps=64,\n",
    "                logging_steps=2,\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=4,\n",
    "                per_device_eval_batch_size=2,\n",
    "                fp16=True,\n",
    "                gradient_checkpointing=True,\n",
    "                report_to=\"none\",\n",
    "                warmup_steps=10,\n",
    "            ),\n",
    "            model_type=\"causal_lm\",\n",
    "            model_kwargs=base_model_kwargs,\n",
    "            formatting_func=formatting_function_prompt_b,\n",
    "            compute_metrics=compute_metrics_pii,\n",
    "            generation_config=base_generation_config,\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    config_group_all = RFGridSearch(\n",
    "        configs=all_configs,\n",
    "        trainer_type=\"SFT\"\n",
    "    )\n",
    "\n",
    "    # Use single experiment for all runs\n",
    "    exp_name = f\"{my_experiment}-all\"\n",
    "    experiment = Experiment(experiment_name=exp_name)\n",
    "    experiment_path = db.get_experiments_path(exp_name)\n",
    "    tensorboard_log_dir = f\"{experiment_path}/{exp_name}/tensorboard_logs\"\n",
    "\n",
    "    print(f\"Experiment: {exp_name}\")\n",
    "    print(f\"TensorBoard log dir: {tensorboard_log_dir}\")\n",
    "\n",
    "    experiment.run_fit(\n",
    "        config_group_all,\n",
    "        create_model_gpt2,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        num_chunks=2,  # Lower chunks for higher parallelism\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ All 8 configurations completed training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5149d",
   "metadata": {
    "id": "9cf5149d"
   },
   "source": [
    "## Using RapidFire Interactive Controls (Stop, Clone-Modify)\n",
    "\n",
    "RapidFire AI provides an **Interactive Controller** for managing experiments dynamically:\n",
    "\n",
    "### Key Operations:\n",
    "\n",
    "#### 1. ‚èπÔ∏è Stop Underperforming Runs\n",
    "**When to use:** After reviewing TensorBoard curves, you notice some configurations are clearly underperforming (high loss, not converging).\n",
    "\n",
    "**How to do it:**\n",
    "1. Launch the Interactive Controller (see cell below)\n",
    "2. Identify the run by its config name or run ID\n",
    "3. Click the **Stop** button next to that run\n",
    "4. The run will gracefully stop, freeing GPU resources for other runs\n",
    "\n",
    "**Example:** If `promptA_r8_lr1e-04` shows high eval loss after 100 steps, you can stop it early.\n",
    "\n",
    "#### 2. üìã Clone-Modify to Explore New Hyperparameters\n",
    "**When to use:** You find a promising configuration and want to try a variation (e.g., slightly different learning rate or LoRA rank).\n",
    "\n",
    "**How to do it:**\n",
    "1. In the Interactive Controller, find the best-performing run\n",
    "2. Click the **Clone** button\n",
    "3. A form appears showing the run's configuration as a JSON dict\n",
    "4. Modify the desired parameter (e.g., change `\"learning_rate\": 5e-4` to `\"learning_rate\": 2e-4`)\n",
    "5. Optionally enable warm start to initialize from the parent run's checkpoint\n",
    "6. Click **Submit** to launch the new run\n",
    "\n",
    "**Example:** If `promptB_r32_lr5e-04` performs best, clone it and try `lr=3e-4` to see if it improves further.\n",
    "\n",
    "#### 3. ‚ñ∂Ô∏è Resume Stopped Runs\n",
    "**When to use:** You stopped a run but later decide to continue it.\n",
    "\n",
    "**How to do it:**\n",
    "1. Find the stopped run in the Interactive Controller\n",
    "2. Click the **Resume** button\n",
    "3. The run continues from its last checkpoint\n",
    "\n",
    "#### 4. üóëÔ∏è Delete Failed Runs\n",
    "**When to use:** A run failed due to errors or you want to remove it from the experiment.\n",
    "\n",
    "**How to do it:**\n",
    "1. Find the run in the Interactive Controller\n",
    "2. Click the **Delete** button\n",
    "3. Confirm deletion\n",
    "\n",
    "### Launching the Controller:\n",
    "Run the cell below to display the Interactive Controller. You can click **Refresh** to update run statuses and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dba28d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616,
     "referenced_widgets": [
      "6ea5feb7172046feba31d3c72d1e16d3",
      "f6b76a614f914d7689cc5cafb81b3c68",
      "61922134af6f4e558d6982cc2034790d",
      "efbfd58593e5429a9a3d8e1d805f1fef",
      "d818bc00b7784a6eb5c95fdd4e22dc1a",
      "f8c13b9aa3ed47edb6a461de345cd0f5",
      "ed9c09ea6eec40aa938e3591842c8f68",
      "1219a478d34148f4938f99330c9839e8",
      "e14ca92f0c2a4b9bb63e520eba544ab5",
      "ae107f8259c645fdaaa28e96e4c2c082",
      "258d015f61ba4db6acdbabd9a4feaf7a",
      "b65ebcbb36254d97a472c0361d860ff9",
      "a0b29dd5a6fd4d41b64f2c363f3c1e52",
      "2f3775ca2a65498dacc6686bfed5ffab",
      "ff6ad0355e1f41bead1f6f125b9c40ad",
      "93db72f9ca56439a9a632a1fd90974d3",
      "e9043c34589b4249ac3d024efa79063e",
      "3e5f9e11429d462ba41d43a7f09752b4",
      "329ccb3afc4e43fa9f33f7d9bd6fb32a",
      "768306a4769b4d66b7b59b3636d02bcb",
      "6c80c071d09543a29c19e72e74cdedd7",
      "128fb4e6d8c649018b1d3a6945c77112",
      "85e27e7c590b48539bbbca9de0500c93",
      "83333be6b0bd40ea891fc8e3a8492327",
      "4ba677f2ddfb414fa5bece47b6a56f79",
      "4a43381a9ddb4e7fa8bafdbe74685e48",
      "165380f74cb443f3a28647ef0a65e39c",
      "cbef5eda247948249e61ca235b0268d8",
      "afd955bf1093485db291883b57122b12",
      "e514c711444946f5ac53ea29a38ed290",
      "3886f00a8a4b479fa55ad99af4fe973e",
      "27db915793c4416d9d3a8a3f9fd2e7d3",
      "a32e441b7e0b41dda6aec2cdf2b9e926",
      "22676d4469eb4d22ab27f799be94e05b",
      "e9e1637747af41169a43f117d762e58d",
      "2a0b7aaa26e74e389b1f6ba1cdfc345f",
      "df457169a9544e748314e35a3f122992",
      "1e8c03f340c9433fb320593a485bf27f",
      "46209c65db1c4caa8c9f9a4dc7ccc763",
      "f1e3e032e9b643a888f1aaaca377b75e",
      "1685537e5b2d476592a4b86422ed3846",
      "58b549ea7eff4e85bd5cda80773b0c16",
      "3d99dc03c27d49a39ba85a04e51f8bad",
      "c190fc85154141f6b7e051d700b52a9d",
      "ce97ab592a1844be838015f7bcd61848",
      "f400dfc303bd42cf944e8adc3a4ccf98",
      "236e91d3e73f4cb0b9f89258151d249a",
      "6e786483e4e847fc8b168c8a60b4547e",
      "3965336a43c64f51b3c8632ec10d9c64",
      "066b790d6a9b4d1c9edd9b5c99265714",
      "0d05054ae2aa434fbff4a43a52df1338",
      "9c359d189fa24af9ab19af1eae9b23d2",
      "49bcd312970e43e6ab28e90a717ec0ca",
      "d54773a212a044829a0f6a074887d970",
      "d14e6d06e42e49659939ab54eb86225f",
      "a851f65393dc47f9915115a4905a3cb7",
      "39c40dd45a864242ae0a04ad8b6d2e2a",
      "9d9b61400e744eb3a53f2ce7569d515a",
      "28dcdc8565e6421bb40b9cde97565021",
      "2e45225d9a6b4338bd126112b755b860",
      "76a7246fb4214c34b57c1befff1bcd54",
      "0ecdde63ea574fea9c233ffe3e16913e",
      "381b5c0bb4034df8882f6132c755ce90",
      "bc3f1bde570d4639bb2959e3d6adcc0a",
      "39a261f5d667422bb4af38317573f8a6",
      "b520d3d6bae74cf1bd6ea3a957314bfd",
      "96033ba43acb4bce8f37d1a3445a0b8d",
      "f51dc4573bb14ac184a82fa7ecf877b6",
      "bf69f999f0fb40fca3a1a89c051e0c3e",
      "35b17400764c4e398be6e2ac5d8dae8a",
      "7bf0fc0701f44d388d54fd40396e936c",
      "c66ed8692cb24860a56587c63f08508a",
      "0c68215c510f4d478d5d9c1143b471fb"
     ]
    },
    "id": "38dba28d",
    "outputId": "f902f3fc-6acf-4331-9e46-84d9fc024dce"
   },
   "outputs": [],
   "source": [
    "# Launch Interactive Controller\n",
    "sleep(15)  # Wait for runs to initialize\n",
    "\n",
    "from rapidfireai.fit.utils.interactive_controller import InteractiveController\n",
    "\n",
    "controller = InteractiveController(dispatcher_url=\"http://127.0.0.1:8851\")\n",
    "controller.display()\n",
    "\n",
    "print(\"\\n‚úÖ Interactive Controller loaded. Use Stop, Clone, Resume, Delete buttons to manage runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324804b1",
   "metadata": {
    "id": "324804b1"
   },
   "source": [
    "## Extract Results from Training Logs\n",
    "\n",
    "We extract final metrics from `trainer_state.json` files saved during training. Each run's checkpoint contains complete training history with loss curves, eval metrics, and runtime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4b3fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bd4b3fd",
    "outputId": "bf05f499-3d9b-43c2-dd01-795098ce4e62"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the config map\n",
    "with open(\"outputs/run_config_map.json\", \"r\") as f:\n",
    "    config_map = json.load(f)\n",
    "\n",
    "# Extract metrics from trainer_state.json files\n",
    "results_data = []\n",
    "\n",
    "base_path = Path(\"rapidfireai/rapidfire_experiments/pii-masking-gpt2-v1-all/runs\")\n",
    "\n",
    "for config_name, details in config_map.items():\n",
    "    run_id = details[\"id\"]\n",
    "    trainer_state_path = base_path / str(run_id) / \"checkpoints\" / \"final_checkpoint\" / \"trainer_state.json\"\n",
    "    \n",
    "    if trainer_state_path.exists():\n",
    "        with open(trainer_state_path, \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        # Extract final metrics from log_history\n",
    "        log_history = trainer_state.get(\"log_history\", [])\n",
    "        \n",
    "        # Get final eval metrics (last eval entry)\n",
    "        final_eval_loss = None\n",
    "        exact_match = None\n",
    "        eval_mean_token_accuracy = None\n",
    "        eval_num_tokens = None\n",
    "        \n",
    "        for entry in reversed(log_history):\n",
    "            if \"eval_loss\" in entry:\n",
    "                final_eval_loss = entry[\"eval_loss\"]\n",
    "                exact_match = entry.get(\"exact_match\", None)\n",
    "                eval_mean_token_accuracy = entry.get(\"eval_mean_token_accuracy\", None)\n",
    "                eval_num_tokens = entry.get(\"eval_num_tokens\", None)\n",
    "                break\n",
    "        \n",
    "        # Get final train metrics (last train entry)\n",
    "        final_train_loss = None\n",
    "        train_runtime = None\n",
    "        \n",
    "        for entry in reversed(log_history):\n",
    "            if \"train_loss\" in entry:\n",
    "                final_train_loss = entry[\"train_loss\"]\n",
    "                train_runtime = entry.get(\"train_runtime\", None)\n",
    "                break\n",
    "        \n",
    "        results_data.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"config_name\": config_name,\n",
    "            \"prompt\": details[\"prompt_variant\"],\n",
    "            \"lora_rank\": details[\"lora_rank\"],\n",
    "            \"learning_rate\": details[\"learning_rate\"],\n",
    "            \"final_train_loss\": round(final_train_loss, 6) if final_train_loss else None,\n",
    "            \"final_eval_loss\": round(final_eval_loss, 6) if final_eval_loss else None,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"eval_mean_token_accuracy\": round(eval_mean_token_accuracy, 6) if eval_mean_token_accuracy else None,\n",
    "            \"train_runtime_sec\": round(train_runtime, 2) if train_runtime else None,\n",
    "        })\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: trainer_state.json not found for {config_name}\")\n",
    "\n",
    "# Create DataFrame and sort by eval_loss (best first)\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.sort_values(\"final_eval_loss\", ascending=True)\n",
    "\n",
    "# Display results table\n",
    "print(\"=\" * 100)\n",
    "print(\"EXPERIMENT RESULTS (Sorted by Eval Loss - Best First)\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"outputs/results.csv\", index=False)\n",
    "results_df.to_json(\"outputs/results.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Results saved to outputs/results.csv and outputs/results.json\")\n",
    "print(f\"\\nüèÜ Best configuration: {results_df.iloc[0]['config_name']}\")\n",
    "print(f\"   Final Eval Loss: {results_df.iloc[0]['final_eval_loss']}\")\n",
    "print(f\"   Mean Token Accuracy: {results_df.iloc[0]['eval_mean_token_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773df02",
   "metadata": {
    "id": "e773df02"
   },
   "source": [
    "## Identify Best Configuration\n",
    "\n",
    "Based on TensorBoard metrics, identify the best configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaaf825",
   "metadata": {},
   "source": [
    "## Extract and Plot Metrics from TensorBoard Logs\n",
    "\n",
    "We'll extract training and evaluation metrics from TensorBoard event files and create publication-ready plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce395fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorboard if needed\n",
    "try:\n",
    "    from tensorboard.backend.event_processing import event_accumulator\n",
    "    print(\"‚úÖ TensorBoard already available\")\n",
    "except ImportError:\n",
    "    print(\"Installing tensorboard...\")\n",
    "    %pip install -q tensorboard\n",
    "    from tensorboard.backend.event_processing import event_accumulator\n",
    "    print(\"‚úÖ TensorBoard installed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to read TensorBoard event files\n",
    "def read_tensorboard_scalars(log_dir, tags):\n",
    "    \"\"\"Read scalar data from TensorBoard event files.\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(str(log_dir))\n",
    "    ea.Reload()\n",
    "    \n",
    "    scalars = {}\n",
    "    for tag in tags:\n",
    "        if tag in ea.Tags()['scalars']:\n",
    "            events = ea.Scalars(tag)\n",
    "            scalars[tag] = [(e.step, e.value) for e in events]\n",
    "        else:\n",
    "            scalars[tag] = []\n",
    "    \n",
    "    return scalars\n",
    "\n",
    "# Read metrics from all runs\n",
    "tensorboard_base = Path(\"rapidfireai/rapidfire_experiments/pii-masking-gpt2-v1-all/tensorboard_logs\")\n",
    "all_metrics = {}\n",
    "\n",
    "tags_to_extract = ['loss', 'eval_loss', 'eval_mean_token_accuracy', 'exact_match']\n",
    "\n",
    "for config_name, details in config_map.items():\n",
    "    run_id = details[\"id\"]\n",
    "    log_dir = tensorboard_base / str(run_id)\n",
    "    \n",
    "    if log_dir.exists():\n",
    "        print(f\"Reading metrics for {config_name} (run {run_id})...\")\n",
    "        scalars = read_tensorboard_scalars(log_dir, tags_to_extract)\n",
    "        all_metrics[config_name] = scalars\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: TensorBoard logs not found for {config_name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded metrics for {len(all_metrics)} configurations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for plots\n",
    "from pathlib import Path\n",
    "Path(\"outputs/plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 8))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for idx, (config_name, metrics) in enumerate(all_metrics.items()):\n",
    "    if 'loss' in metrics and metrics['loss']:\n",
    "        steps, values = zip(*metrics['loss'])\n",
    "        ax.plot(steps, values, label=config_name, linewidth=2, color=colors[idx], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Training Step', fontsize=12)\n",
    "ax.set_ylabel('Training Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Across All Configurations', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/plots/training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved outputs/plots/training_loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Evaluation Loss (Most Important)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for idx, (config_name, metrics) in enumerate(all_metrics.items()):\n",
    "    if 'eval_loss' in metrics and metrics['eval_loss']:\n",
    "        steps, values = zip(*metrics['eval_loss'])\n",
    "        linestyle = '-' if config_name == best_config_name else '--'\n",
    "        linewidth = 3 if config_name == best_config_name else 2\n",
    "        alpha = 1.0 if config_name == best_config_name else 0.7\n",
    "        ax.plot(steps, values, label=config_name, linewidth=linewidth, \n",
    "                linestyle=linestyle, color=colors[idx], alpha=alpha)\n",
    "\n",
    "ax.set_xlabel('Training Step', fontsize=12)\n",
    "ax.set_ylabel('Evaluation Loss', fontsize=12)\n",
    "ax.set_title('Evaluation Loss Across All Configurations (Best Config Highlighted)', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/plots/eval_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved outputs/plots/eval_loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Mean Token Accuracy\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for idx, (config_name, metrics) in enumerate(all_metrics.items()):\n",
    "    if 'eval_mean_token_accuracy' in metrics and metrics['eval_mean_token_accuracy']:\n",
    "        steps, values = zip(*metrics['eval_mean_token_accuracy'])\n",
    "        linestyle = '-' if config_name == best_config_name else '--'\n",
    "        linewidth = 3 if config_name == best_config_name else 2\n",
    "        alpha = 1.0 if config_name == best_config_name else 0.7\n",
    "        ax.plot(steps, values, label=config_name, linewidth=linewidth,\n",
    "                linestyle=linestyle, color=colors[idx], alpha=alpha)\n",
    "\n",
    "ax.set_xlabel('Training Step', fontsize=12)\n",
    "ax.set_ylabel('Mean Token Accuracy', fontsize=12)\n",
    "ax.set_title('Mean Token Accuracy Across All Configurations', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/plots/token_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved outputs/plots/token_accuracy.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Comparison by Hyperparameter Groups\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Group by prompt variant\n",
    "ax = axes[0, 0]\n",
    "for prompt_var in ['A', 'B']:\n",
    "    configs_for_prompt = [cn for cn, d in config_map.items() if d['prompt_variant'] == prompt_var]\n",
    "    eval_losses = []\n",
    "    for cn in configs_for_prompt:\n",
    "        if cn in all_metrics and 'eval_loss' in all_metrics[cn] and all_metrics[cn]['eval_loss']:\n",
    "            final_loss = all_metrics[cn]['eval_loss'][-1][1]\n",
    "            eval_losses.append(final_loss)\n",
    "    if eval_losses:\n",
    "        ax.bar(prompt_var, np.mean(eval_losses), yerr=np.std(eval_losses), capsize=5, alpha=0.7)\n",
    "ax.set_xlabel('Prompt Variant', fontsize=11)\n",
    "ax.set_ylabel('Avg Final Eval Loss', fontsize=11)\n",
    "ax.set_title('Eval Loss by Prompt Variant', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Group by LoRA rank\n",
    "ax = axes[0, 1]\n",
    "for rank in [8, 32]:\n",
    "    configs_for_rank = [cn for cn, d in config_map.items() if d['lora_rank'] == rank]\n",
    "    eval_losses = []\n",
    "    for cn in configs_for_rank:\n",
    "        if cn in all_metrics and 'eval_loss' in all_metrics[cn] and all_metrics[cn]['eval_loss']:\n",
    "            final_loss = all_metrics[cn]['eval_loss'][-1][1]\n",
    "            eval_losses.append(final_loss)\n",
    "    if eval_losses:\n",
    "        ax.bar(f\"r={rank}\", np.mean(eval_losses), yerr=np.std(eval_losses), capsize=5, alpha=0.7)\n",
    "ax.set_xlabel('LoRA Rank', fontsize=11)\n",
    "ax.set_ylabel('Avg Final Eval Loss', fontsize=11)\n",
    "ax.set_title('Eval Loss by LoRA Rank', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Group by learning rate\n",
    "ax = axes[1, 0]\n",
    "for lr in [0.0002, 0.0005]:\n",
    "    configs_for_lr = [cn for cn, d in config_map.items() if d['learning_rate'] == lr]\n",
    "    eval_losses = []\n",
    "    for cn in configs_for_lr:\n",
    "        if cn in all_metrics and 'eval_loss' in all_metrics[cn] and all_metrics[cn]['eval_loss']:\n",
    "            final_loss = all_metrics[cn]['eval_loss'][-1][1]\n",
    "            eval_losses.append(final_loss)\n",
    "    if eval_losses:\n",
    "        lr_label = f\"{lr:.0e}\"\n",
    "        ax.bar(lr_label, np.mean(eval_losses), yerr=np.std(eval_losses), capsize=5, alpha=0.7)\n",
    "ax.set_xlabel('Learning Rate', fontsize=11)\n",
    "ax.set_ylabel('Avg Final Eval Loss', fontsize=11)\n",
    "ax.set_title('Eval Loss by Learning Rate', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# All configs comparison (bar chart)\n",
    "ax = axes[1, 1]\n",
    "config_names_sorted = results_df.sort_values('final_eval_loss')['config_name'].tolist()\n",
    "eval_losses_sorted = results_df.sort_values('final_eval_loss')['final_eval_loss'].tolist()\n",
    "bars = ax.bar(range(len(config_names_sorted)), eval_losses_sorted, alpha=0.7)\n",
    "bars[0].set_color('gold')  # Highlight best\n",
    "bars[0].set_edgecolor('black')\n",
    "bars[0].set_linewidth(2)\n",
    "ax.set_xticks(range(len(config_names_sorted)))\n",
    "ax.set_xticklabels(config_names_sorted, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Final Eval Loss', fontsize=11)\n",
    "ax.set_title('All Configurations Ranked by Eval Loss', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/plots/hyperparameter_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved outputs/plots/hyperparameter_comparison.png\")\n",
    "print(\"\\nüìä All plots saved in outputs/plots/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef1913",
   "metadata": {},
   "source": [
    "## Load Best Checkpoint and Run Inference (Standalone)\n",
    "\n",
    "This section works independently - it reads directly from saved checkpoints and doesn't require running previous cells.\n",
    "\n",
    "It will:\n",
    "1. Load the config-to-run mapping from `outputs/run_config_map.json`\n",
    "2. Scan all 8 checkpoints to find the best one (lowest eval_loss)\n",
    "3. Load that checkpoint and run inference on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "    from datasets import load_dataset\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    print(\"‚úÖ Required packages already available\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    %pip install -q torch transformers peft accelerate datasets\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "    from datasets import load_dataset\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    print(\"‚úÖ Packages installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STANDALONE INFERENCE - Finding Best Checkpoint from Filesystem\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Load config map to get run IDs\n",
    "config_map_path = \"outputs/run_config_map.json\"\n",
    "with open(config_map_path, \"r\") as f:\n",
    "    config_map = json.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded config map with {len(config_map)} configurations\")\n",
    "\n",
    "# Step 2: Scan all checkpoints to find best one (lowest eval_loss)\n",
    "base_path = Path(\"rapidfireai/rapidfire_experiments/pii-masking-gpt2-v1-all/runs\")\n",
    "best_eval_loss = float('inf')\n",
    "best_run_id = None\n",
    "best_config_name = None\n",
    "best_config_details = None\n",
    "\n",
    "print(\"\\nüìä Scanning checkpoints for best eval_loss:\")\n",
    "for config_name, details in config_map.items():\n",
    "    run_id = details[\"id\"]\n",
    "    trainer_state_path = base_path / str(run_id) / \"checkpoints\" / \"final_checkpoint\" / \"trainer_state.json\"\n",
    "    \n",
    "    if trainer_state_path.exists():\n",
    "        with open(trainer_state_path, \"r\") as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        # Get final eval loss from log history\n",
    "        log_history = trainer_state.get(\"log_history\", [])\n",
    "        final_eval_loss = None\n",
    "        for entry in reversed(log_history):\n",
    "            if \"eval_loss\" in entry:\n",
    "                final_eval_loss = entry[\"eval_loss\"]\n",
    "                break\n",
    "        \n",
    "        if final_eval_loss is not None:\n",
    "            print(f\"  Run {run_id} ({config_name}): eval_loss = {final_eval_loss:.4f}\")\n",
    "            \n",
    "            if final_eval_loss < best_eval_loss:\n",
    "                best_eval_loss = final_eval_loss\n",
    "                best_run_id = run_id\n",
    "                best_config_name = config_name\n",
    "                best_config_details = details\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üèÜ BEST CHECKPOINT FOUND:\")\n",
    "print(f\"   Run ID: {best_run_id}\")\n",
    "print(f\"   Config: {best_config_name}\")\n",
    "print(f\"   Eval Loss: {best_eval_loss:.4f}\")\n",
    "print(f\"   Prompt: {best_config_details['prompt_variant']}\")\n",
    "print(f\"   LoRA Rank: {best_config_details['lora_rank']}\")\n",
    "print(f\"   Learning Rate: {best_config_details['learning_rate']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 3: Load the best checkpoint\n",
    "best_checkpoint_path = f\"rapidfireai/rapidfire_experiments/pii-masking-gpt2-v1-all/runs/{best_run_id}/checkpoints/final_checkpoint\"\n",
    "\n",
    "print(f\"\\nüì• Loading model from: {best_checkpoint_path}\")\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Load fine-tuned LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, best_checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Step 4: Define prompt formatting based on best config\n",
    "if best_config_details['prompt_variant'] == 'A':\n",
    "    # Prompt A: Minimal instruction\n",
    "    def format_prompt_for_inference(source_text):\n",
    "        return f\"\"\"Mask all PII in the following text. Output only the masked text without explanations.\n",
    "\n",
    "Text: {source_text}\n",
    "\n",
    "Masked text:\"\"\"\n",
    "    print(\"üìù Using Prompt A (minimal instruction)\")\n",
    "else:\n",
    "    # Prompt B: One-shot example\n",
    "    def format_prompt_for_inference(source_text):\n",
    "        return f\"\"\"Mask all PII in the text. Replace names with [NAME], emails with [EMAIL], etc. Output only the masked text.\n",
    "\n",
    "Example:\n",
    "Text: John Smith's email is john@example.com\n",
    "Masked: [NAME]'s email is [EMAIL]\n",
    "\n",
    "Text: {source_text}\n",
    "\n",
    "Masked text:\"\"\"\n",
    "    print(\"üìù Using Prompt B (one-shot with example)\")\n",
    "\n",
    "# Step 5: Load dataset for inference\n",
    "print(\"\\nüìö Loading dataset for inference examples...\")\n",
    "dataset = load_dataset(\"ai4privacy/open-pii-masking-500k-ai4privacy\", split=\"train\")\n",
    "gen_eval_dataset = dataset.select(range(74, 84))  # 10 examples for inference demo\n",
    "print(f\"‚úÖ Loaded {len(gen_eval_dataset)} examples for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on evaluation examples\n",
    "def generate_masked_text(source_text, max_new_tokens=150):\n",
    "    \"\"\"Generate masked text for given source text.\"\"\"\n",
    "    prompt = format_prompt_for_inference(source_text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part (after the prompt)\n",
    "    if \"Masked text:\" in full_output:\n",
    "        generated = full_output.split(\"Masked text:\")[-1].strip()\n",
    "        # Take only up to first newline to avoid extra generation\n",
    "        generated = generated.split('\\n')[0].strip()\n",
    "    else:\n",
    "        generated = full_output[len(prompt):].strip()\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test on examples\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"INFERENCE DEMONSTRATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "num_examples_to_show = 5\n",
    "exact_matches = 0\n",
    "\n",
    "for i in range(min(num_examples_to_show, len(gen_eval_dataset))):\n",
    "    example = gen_eval_dataset[i]\n",
    "    source = example['source_text']\n",
    "    reference = example['masked_text']\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"üìÑ Source Text:\\n{source}\\n\")\n",
    "    print(f\"‚úÖ Reference (Ground Truth):\\n{reference}\\n\")\n",
    "    \n",
    "    generated = generate_masked_text(source)\n",
    "    print(f\"ü§ñ Generated (Our Model):\\n{generated}\\n\")\n",
    "    \n",
    "    # Simple exact match check\n",
    "    is_exact_match = generated.strip() == reference.strip()\n",
    "    if is_exact_match:\n",
    "        exact_matches += 1\n",
    "    \n",
    "    print(f\"Match: {'‚úÖ EXACT MATCH' if is_exact_match else '‚ùå Different (model may have variations)'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"üìä Summary: {exact_matches}/{num_examples_to_show} exact matches ({exact_matches/num_examples_to_show*100:.1f}%)\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nüí° Note: Exact match is very strict. The model may correctly mask PII but use different\")\n",
    "print(\"   token formats or word boundaries, which counts as 'no match' in this metric. This was just trained on 64 examples and is a GPT-2 model. Expect this section to work very well when running longer training or using better models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b66bfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80b66bfe",
    "outputId": "063530b7-f965-4597-d78b-0665dda3419d"
   },
   "outputs": [],
   "source": [
    "# Identify best configuration based on eval_loss\n",
    "best_config = results_df.iloc[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BEST CONFIGURATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "print(f\"üèÜ Best Configuration: {best_config['config_name']}\")\n",
    "print(f\"   Prompt Variant: {best_config['prompt']}\")\n",
    "print(f\"   LoRA Rank: {best_config['lora_rank']}\")\n",
    "print(f\"   Learning Rate: {best_config['learning_rate']}\")\n",
    "print(\"\")\n",
    "print(\"üìä Performance Metrics:\")\n",
    "print(f\"   Final Eval Loss: {best_config['final_eval_loss']:.6f}\")\n",
    "print(f\"   Final Train Loss: {best_config['final_train_loss']:.6f}\")\n",
    "print(f\"   Mean Token Accuracy: {best_config['eval_mean_token_accuracy']:.4f} ({best_config['eval_mean_token_accuracy']*100:.2f}%)\")\n",
    "print(f\"   Exact Match: {best_config['exact_match']}\")\n",
    "print(f\"   Training Runtime: {best_config['train_runtime_sec']:.2f} seconds\")\n",
    "print(\"\")\n",
    "print(\"üí° Key Insights:\")\n",
    "\n",
    "# Compare with other configs\n",
    "prompt_a_best = results_df[results_df['prompt'] == 'A'].iloc[0] if len(results_df[results_df['prompt'] == 'A']) > 0 else None\n",
    "prompt_b_best = results_df[results_df['prompt'] == 'B'].iloc[0] if len(results_df[results_df['prompt'] == 'B']) > 0 else None\n",
    "\n",
    "if prompt_a_best is not None and prompt_b_best is not None:\n",
    "    prompt_improvement = ((prompt_a_best['final_eval_loss'] - prompt_b_best['final_eval_loss']) / prompt_a_best['final_eval_loss']) * 100\n",
    "    print(f\"   - Prompt B outperforms Prompt A by {prompt_improvement:.1f}% in eval loss\")\n",
    "    print(f\"   - One-shot examples help the model learn PII masking patterns better\")\n",
    "\n",
    "# Compare LoRA ranks\n",
    "r8_avg = results_df[results_df['lora_rank'] == 8]['final_eval_loss'].mean()\n",
    "r32_avg = results_df[results_df['lora_rank'] == 32]['final_eval_loss'].mean()\n",
    "print(f\"   - Higher LoRA rank (r=32) avg eval loss: {r32_avg:.4f}\")\n",
    "print(f\"   - Lower LoRA rank (r=8) avg eval loss: {r8_avg:.4f}\")\n",
    "print(f\"   - Rank 32 captures more complexity, reducing loss by {((r8_avg - r32_avg)/r8_avg)*100:.1f}%\")\n",
    "\n",
    "# Compare learning rates\n",
    "lr_high_avg = results_df[results_df['learning_rate'] == 0.0005]['final_eval_loss'].mean()\n",
    "lr_low_avg = results_df[results_df['learning_rate'] == 0.0002]['final_eval_loss'].mean()\n",
    "print(f\"   - Higher LR (5e-4) avg eval loss: {lr_high_avg:.4f}\")\n",
    "print(f\"   - Lower LR (2e-4) avg eval loss: {lr_low_avg:.4f}\")\n",
    "if lr_high_avg < lr_low_avg:\n",
    "    print(f\"   - Higher LR trains faster and achieves better loss (improvement: {((lr_low_avg - lr_high_avg)/lr_low_avg)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   - Lower LR is more stable (better loss by {((lr_high_avg - lr_low_avg)/lr_high_avg)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store best config info for later use\n",
    "best_run_id = best_config['run_id']\n",
    "best_config_name = best_config['config_name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d288236",
   "metadata": {
    "id": "6d288236"
   },
   "source": [
    "## One-Page Experiment Summary\n",
    "\n",
    "This summary follows the competition template and contains all key information for the submission document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6fae75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc6fae75",
    "outputId": "25927e4b-4d40-4ef8-fc7b-bfc30d3259d6"
   },
   "outputs": [],
   "source": [
    "# Generate experiment summary with real metrics\n",
    "best_row = results_df.iloc[0]\n",
    "worst_row = results_df.iloc[-1]\n",
    "\n",
    "# Calculate key statistics\n",
    "prompt_a_configs = results_df[results_df['prompt'] == 'A']\n",
    "prompt_b_configs = results_df[results_df['prompt'] == 'B']\n",
    "prompt_improvement = ((prompt_a_configs['final_eval_loss'].mean() - prompt_b_configs['final_eval_loss'].mean()) / prompt_a_configs['final_eval_loss'].mean()) * 100\n",
    "\n",
    "r8_configs = results_df[results_df['lora_rank'] == 8]\n",
    "r32_configs = results_df[results_df['lora_rank'] == 32]\n",
    "rank_improvement = ((r8_configs['final_eval_loss'].mean() - r32_configs['final_eval_loss'].mean()) / r8_configs['final_eval_loss'].mean()) * 100\n",
    "\n",
    "lr_low_configs = results_df[results_df['learning_rate'] == 0.0002]\n",
    "lr_high_configs = results_df[results_df['learning_rate'] == 0.0005]\n",
    "\n",
    "pdf_content = f\"\"\"\n",
    "================================================================================\n",
    "PII MASKING EXPERIMENT SUMMARY ‚Äî RapidFire AI Winter Competition\n",
    "================================================================================\n",
    "\n",
    "WHAT WE TRIED:\n",
    "--------------\n",
    "We fine-tuned GPT-2 for PII (Personally Identifiable Information) masking‚Äîa \n",
    "text-to-text task where the model replaces PII entities (names, emails, phone \n",
    "numbers, etc.) with appropriate mask tokens like [NAME], [EMAIL], etc.\n",
    "\n",
    "Good performance means the model correctly identifies and masks all PII while \n",
    "preserving non-PII text structure. We measure this with:\n",
    "- Eval Loss (lower is better): How well the model predicts masked tokens\n",
    "- Mean Token Accuracy: Percentage of correctly predicted tokens\n",
    "- Exact Match: Percentage of perfectly masked examples (0% expected for tiny dataset)\n",
    "\n",
    "SETUP:\n",
    "------\n",
    "‚Ä¢ Base model: GPT-2 (124M parameters)\n",
    "‚Ä¢ Dataset: ai4privacy/open-pii-masking-500k-ai4privacy\n",
    "  - Train: 64 examples (small for Colab speed)\n",
    "  - Eval: 10 examples\n",
    "‚Ä¢ Training method: LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
    "‚Ä¢ Compute: Google Colab T4 GPU, ~45-55 seconds per run, ~7 minutes total\n",
    "\n",
    "EXPERIMENTS (WHAT CHANGED):\n",
    "----------------------------\n",
    "We varied THREE dimensions (2√ó2√ó2 = 8 total configurations):\n",
    "\n",
    "1. Prompt Scheme (Knob #1):\n",
    "   ‚Ä¢ Prompt A: Minimal instruction (\"Mask all PII...\")\n",
    "   ‚Ä¢ Prompt B: One-shot with example\n",
    "\n",
    "2. LoRA Rank (Knob #2):\n",
    "   ‚Ä¢ r=8: Fewer parameters, faster, may underfit\n",
    "   ‚Ä¢ r=32: More capacity, better patterns, risk of overfitting\n",
    "\n",
    "3. Learning Rate (Knob #3):\n",
    "   ‚Ä¢ 2e-4: Conservative, stable\n",
    "   ‚Ä¢ 5e-4: Aggressive, faster convergence\n",
    "\n",
    "All runs used: 1 epoch, batch size 8 (effective 16 with grad accum)\n",
    "\n",
    "RESULTS:\n",
    "--------\n",
    "Config                    Prompt  Rank    LR      Eval Loss   Token Acc   Runtime\n",
    "{best_row['config_name']:<24} {best_row['prompt']:<7} {best_row['lora_rank']:<7} {best_row['learning_rate']:<7.0e} {best_row['final_eval_loss']:<11.4f} {best_row['eval_mean_token_accuracy']:<11.2%} {best_row['train_runtime_sec']:<7.1f}s\n",
    "{results_df.iloc[1]['config_name']:<24} {results_df.iloc[1]['prompt']:<7} {results_df.iloc[1]['lora_rank']:<7} {results_df.iloc[1]['learning_rate']:<7.0e} {results_df.iloc[1]['final_eval_loss']:<11.4f} {results_df.iloc[1]['eval_mean_token_accuracy']:<11.2%} {results_df.iloc[1]['train_runtime_sec']:<7.1f}s\n",
    "{results_df.iloc[2]['config_name']:<24} {results_df.iloc[2]['prompt']:<7} {results_df.iloc[2]['lora_rank']:<7} {results_df.iloc[2]['learning_rate']:<7.0e} {results_df.iloc[2]['final_eval_loss']:<11.4f} {results_df.iloc[2]['eval_mean_token_accuracy']:<11.2%} {results_df.iloc[2]['train_runtime_sec']:<7.1f}s\n",
    "{worst_row['config_name']:<24} {worst_row['prompt']:<7} {worst_row['lora_rank']:<7} {worst_row['learning_rate']:<7.0e} {worst_row['final_eval_loss']:<11.4f} {worst_row['eval_mean_token_accuracy']:<11.2%} {worst_row['train_runtime_sec']:<7.1f}s  ‚Üê worst\n",
    "\n",
    "üèÜ BEST: {best_row['config_name']} \n",
    "   Final Eval Loss: {best_row['final_eval_loss']:.4f}\n",
    "   Mean Token Accuracy: {best_row['eval_mean_token_accuracy']:.2%}\n",
    "\n",
    "TAKEAWAYS:\n",
    "----------\n",
    "‚úÖ What helped most:\n",
    "   ‚Ä¢ Prompt B (one-shot) reduced eval loss by {prompt_improvement:.1f}% vs Prompt A\n",
    "     ‚Üí Providing an example helps the model learn PII masking patterns\n",
    "   ‚Ä¢ Higher LoRA rank (r=32) improved loss by {rank_improvement:.1f}% vs r=8\n",
    "     ‚Üí More capacity captures complex PII entity patterns better\n",
    "   ‚Ä¢ Higher LR (5e-4) converged faster with better final loss\n",
    "     ‚Üí Our small dataset benefits from aggressive learning\n",
    "\n",
    "‚ùå What didn't help:\n",
    "   ‚Ä¢ Prompt A (minimal) struggled: worst 4 configs all used Prompt A\n",
    "   ‚Ä¢ r=8 with Prompt A: severe underfitting (eval loss >1.7)\n",
    "\n",
    "‚ö†Ô∏è Failure modes observed:\n",
    "   ‚Ä¢ Exact Match = 0% for all configs (expected: dataset is tiny, 10 eval examples)\n",
    "   ‚Ä¢ Model sometimes generates extra text beyond the masked output\n",
    "   ‚Ä¢ Some PII entities missed (needs more training data or longer training)\n",
    "\n",
    "HOW RAPIDFIRE AI HELPED:\n",
    "-------------------------\n",
    "1. Hyperparallel Execution:\n",
    "   ‚úì Ran all 8 configs in ~7 minutes (vs ~6 minutes sequential)\n",
    "   ‚úì Used run_fit(num_chunks=2) for efficient parallel scheduling\n",
    "   ‚úì Each run tracked independently with real-time TensorBoard metrics\n",
    "\n",
    "2. Reproducibility:\n",
    "   ‚úì Every run logged to separate TensorBoard directory\n",
    "   ‚úì All checkpoints preserved in runs/1-8/checkpoints/\n",
    "   ‚úì Config-to-run mapping saved for traceability\n",
    "\n",
    "3. Interactive Control (demonstrated but not used):\n",
    "   ‚úì Could stop underperforming runs (e.g., promptA_r8_lr2e-04 at step 2)\n",
    "   ‚úì Could clone best config and try lr=4e-4 for refinement\n",
    "   ‚úì Could resume training if needed\n",
    "\n",
    "Result: Completed structured 8-config experiment in <10 minutes end-to-end,\n",
    "with full metrics, plots, and checkpoints‚Äîready for production use.\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(pdf_content)\n",
    "\n",
    "# Save to file\n",
    "with open(\"outputs/experiment_summary_1page.txt\", \"w\") as f:\n",
    "    f.write(pdf_content)\n",
    "\n",
    "print(\"\\n‚úÖ Summary saved to outputs/experiment_summary_1page.txt\")\n",
    "print(\"   Use this content to create your 1-page PDF submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27f3ac",
   "metadata": {
    "id": "5a27f3ac"
   },
   "source": [
    "## End Experiment\n",
    "\n",
    "Click the button below to gracefully end the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4979076",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "b4979076",
    "outputId": "f39c7d71-f54a-41f0-a30a-f11bcf736e8d"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;\">Click to End Experiment</button>\n",
    "'''))\n",
    "\n",
    "# eval_js blocks until the Promise resolves\n",
    "output.eval_js('''\n",
    "new Promise((resolve) => {\n",
    "    document.getElementById(\"continue-btn\").onclick = () => {\n",
    "        document.getElementById(\"continue-btn\").disabled = true;\n",
    "        document.getElementById(\"continue-btn\").innerText = \"Ending experiment...\";\n",
    "        resolve(\"clicked\");\n",
    "    };\n",
    "})\n",
    "''')\n",
    "\n",
    "# Actually end the experiment after the button is clicked\n",
    "experiment.end()\n",
    "print(\"‚úÖ Experiment ended successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fd18b",
   "metadata": {
    "id": "e59fd18b"
   },
   "source": [
    "## View Final TensorBoard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768bde1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "5768bde1",
    "outputId": "e5410204-bd19-4cf4-ee93-c70448fc52ef"
   },
   "outputs": [],
   "source": [
    "# View final TensorBoard logs\n",
    "%tensorboard --logdir {tensorboard_log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc047fc",
   "metadata": {
    "id": "2dc047fc"
   },
   "source": [
    "## View RapidFire AI Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586af74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7586af74",
    "outputId": "6fc011fe-cb45-4dd0-9333-4538d9c88d4b"
   },
   "outputs": [],
   "source": [
    "# Get the experiment-specific log file\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "display(Pretty(f\"üìÑ Experiment Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"‚ùå Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf0f2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9ecf0f2c",
    "outputId": "d065f665-8a73-42c2-d360-0e2c9d0bfdee"
   },
   "outputs": [],
   "source": [
    "# Get the training-specific log file\n",
    "log_file = experiment.get_log_file_path(\"training\")\n",
    "\n",
    "display(Pretty(f\"üìÑ Training Log File: {log_file}\"))\n",
    "\n",
    "if log_file.exists():\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    display(Pretty(f\"Last 30 lines of {log_file.name}:\"))\n",
    "    display(Pretty(\"=\" * 80))\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            display(Pretty(line.rstrip()))\n",
    "else:\n",
    "    display(Pretty(f\"‚ùå Log file not found: {log_file}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6678b26",
   "metadata": {
    "id": "e6678b26"
   },
   "source": [
    "## Output Files\n",
    "\n",
    "All outputs are saved in the `outputs/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z4Gpo0Mnh3dm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Z4Gpo0Mnh3dm",
    "outputId": "b7a80d7a-d0be-445c-ed0f-6cea80ff24d0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "src = \"/content\"\n",
    "zip_base = \"content_backup\"  # will create content_backup.zip\n",
    "\n",
    "if not os.path.exists(src):\n",
    "    raise FileNotFoundError(f\"{src} not found\")\n",
    "\n",
    "zip_path = shutil.make_archive(zip_base, \"zip\", src)\n",
    "print(f\"Created: {zip_path}\")\n",
    "\n",
    "files.download(zip_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "runtime_attributes": {
    "runtime_version": "2025.10"
   }
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
